<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ElasticSearch08-内核原理]]></title>
    <url>%2F2018%2F09%2F28%2FelasticSearch08%2F</url>
    <content type="text"><![CDATA[倒排索引如何使文本被搜索到是搜索引擎很重要的一部分，倒排索引是很适合搜索的。 因为它的结构： 包含这个关键词的document list 包含这个关键词的所有document的数量：IDFinverse document frequency. 这个关键词在每个document中出现的次数：TFterm frequency. 这个关键词在这个document中的次序 每个document的长度：length norm 包含这个关键词的所有document的平均长度 123456Term | Doc 1 | Doc 2 | Doc 3 | ...------------------------------------brown | X | | X | ...fox | X | X | X | ...quick | X | X | | ...the | X | | X | ... 倒排索引是不可变的，它的好处如下： 不需要锁，提升并发能力，避免锁的问题 数据不变，一直保存在os cache中，只要cache内存足够 filter cache一直驻留在内存，因为数据不变 可以压缩，节省cpu和io开销 当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。 文档写入原理文档在写入的时候，ES的流程如下： 新文档写入buffer commit point buffer中的数据写入新的index segment 等待在os cache中的index segment被fsync强制刷到磁盘上 新的index sgement被打开，供search使用 buffer被清空 删除和更新 每次commit point时，会有一个.del文件，标记了哪些segment中的哪些document被标记为deleted了。搜索的时候，会依次查询所有的segment，从旧的到新的，比如被修改过的document，在旧的segment中，会标记为deleted，在新的segment中会有其新的数据。 优化写入流程现有流程的问题，每次都必须等待fsync将segment刷入磁盘，才能将segment打开供search使用，这样的话从一个document写入，到它可以被搜索，可能会超过1分钟！这就不是近实时的搜索了！主要瓶颈在于fsync实际发生磁盘IO写数据进磁盘，是很耗时的。 写入流程被改进如下： 数据写入buffer 每隔一定时间，buffer中的数据被写入segment文件，但是先写入os cache 只要segment写入os cache，那就直接打开供search使用，不立即执行commit 数据写入os cache，并被打开供搜索的过程，叫做refresh，默认是每隔1秒refresh一次。也就是说，每隔一秒就会将buffer中的数据写入一个新的index segment file，先写入os cache中。所以es是近实时的，数据写入到可以被搜索，默认是1秒。 POST /my_index/_refresh，可以手动refresh，一般不需要手动执行，没必要这样，让es自己搞就可以了。 比如说，我们现在的时效性要求比较低，只要求一条数据写入es，一分钟以后才让我们搜索到就可以了，那么就可以调整refresh interval。如果写的并发量和数据量比较大的话，refresh设置长一点可以优化写入速度，因为频繁的写入index segment file会比较占用资源。 123456PUT /my_index&#123; "settings": &#123; "refresh_interval": "60s" &#125;&#125; 可靠存储实现再次优化的写入流程 数据写入buffer缓冲和translog日志文件 每隔一秒钟，buffer中的数据被写入新的segment file，并进入os cache，此时segment被打开并供search使用 buffer被清空 重复1~3，新的segment不断添加，buffer不断被清空，而translog中的数据不断累加 当translog长度达到一定程度的时候，commit操作发生 buffer中的所有数据写入一个新的segment，并写入os cache，打开供使用 buffer被清空 一个commit ponit被写入磁盘，标明了所有的index segment filesystem cache中的所有index segment file缓存数据，被fsync强行刷到磁盘上 现有的translog被清空，创建一个新的translog 新的文档被添加到内存缓冲区并且被追加到了事务日志 刷新（refresh）完成后, 缓存被清空但是事务日志不会 事务日志不断积累文档 在刷新（flush）之后，段被全量提交，并且事务日志被清空 数据恢复OS cache中积累了不少数据，这个时候如果机器宕机了，虽然os cache的数据丢失了，但是translog的数据是存在的，可以基于translog和commit point进行数据恢复。 fsync+清空translog，就是flush，默认每隔30分钟flush一次，或者当translog过大的时候，也会flush。 POST /my_index/_flush，一般来说别手动flush，让它自动执行就可以了。 translog translog本身，每隔5秒被fsync一次到磁盘上。在一次增删改操作之后，当fsync在primary shard和replica shard都成功之后，那次增删改操作才会成功。 但是这种在一次增删改时强行fsync translog可能会导致部分操作比较耗时，也可以允许部分数据丢失，设置异步fsync translog。 12345PUT /my_index/_settings&#123; "index.translog.durability": "async", "index.translog.sync_interval": "5s"&#125; 如果你不确定这个行为的后果，最好是使用默认的参数（ “index.translog.durability”: “request” ）来避免数据丢失。 段合并前面说了，refresh操作默认是每秒一次，每秒生成一个新的segment file，这样文件太多了，而且每次search都要搜索所有的segment，很耗时。 ES默认会在后台执行segment merge操作，在merge的时候，被标记为deleted的document也会被彻底物理删除。 每次merge操作的执行流程： 选择一些有相似大小的segment，merge成一个大的segment 将新的segment flush到磁盘上去 写一个新的commit point，包括了新的segment，并且排除旧的那些segment 将新的segment打开供搜索 将旧的segment删除 两个提交了的段和一个未提交的段正在被合并到一个更大的段 一旦合并结束，老的段被删除 optimize APIoptimize API大可看做是 强制合并 API 。它会将一个分片强制合并到 max_num_segments 参数指定大小的段数目。 这样做的意图是减少段的数量（通常减少到一个），来提升搜索性能。 POST /my_index/_optimize?max_num_segments=1，尽量不要手动执行，让它自动默认执行就可以了。]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch07-索引管理]]></title>
    <url>%2F2018%2F09%2F27%2FelasticSearch07%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 索引管理手动创建索引之前都是直接插入document，ES为我们自动创建索引。其实在大部分情况下，我们是需要自己先手动创建mapping的，就像数据库建表一样，你可能需要设置primary shard的数量，字段的分词器，具体的数据类型等等。 创建索引的语法 12345678910111213141516PUT /my_index&#123; "settings": &#123; "number_of_shards": 1, "number_of_replicas": 0 &#125;, "mappings": &#123; "my_type": &#123; "properties": &#123; "my_field": &#123; "type": "text" &#125; &#125; &#125; &#125;&#125; 修改索引number_of_shards是不能修改的，修改一下number_of_replicas试试。 1234PUT /my_index/_settings&#123; "number_of_replicas": 1&#125; 删除索引1234DELETE /my_indexDELETE /index_one,index_twoDELETE /index_*DELETE /_all DELETE /_all是很危险的操作，可以在ES的配置文件里禁用掉。 elasticsearch.yml 1action.destructive_requires_name: true 配置分词器分词器是用于将全文字符串转换为适合搜索的倒排索引。 standard 分析器是用于全文字段的默认分析器， 对于大部分西方语系来说是一个不错的选择。 它包括了以下几点： standard tokenizer：以单词边界进行切分 standard token filter：什么都不做 lowercase token filter：将所有字母转换为小写 stop token filter（默认被禁用）：移除停用词，比如a the it等等 修改分词器启用english停用词token filter 在下面的例子中，我们创建了一个新的分析器，叫做 es_std ， 并使用预定义的英语停用词列表： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192PUT /my_index&#123; "settings": &#123; "analysis": &#123; "analyzer": &#123; "es_std": &#123; "type": "standard", "stopwords": "_english_" &#125; &#125; &#125; &#125;&#125;GET /my_index/_analyze&#123; "analyzer": "standard", "text": "a dog is in the house"&#125;&#123; "tokens": [ &#123; "token": "a", "start_offset": 0, "end_offset": 1, "type": "&lt;ALPHANUM&gt;", "position": 0 &#125;, &#123; "token": "dog", "start_offset": 2, "end_offset": 5, "type": "&lt;ALPHANUM&gt;", "position": 1 &#125;, &#123; "token": "is", "start_offset": 6, "end_offset": 8, "type": "&lt;ALPHANUM&gt;", "position": 2 &#125;, &#123; "token": "in", "start_offset": 9, "end_offset": 11, "type": "&lt;ALPHANUM&gt;", "position": 3 &#125;, &#123; "token": "the", "start_offset": 12, "end_offset": 15, "type": "&lt;ALPHANUM&gt;", "position": 4 &#125;, &#123; "token": "house", "start_offset": 16, "end_offset": 21, "type": "&lt;ALPHANUM&gt;", "position": 5 &#125; ]&#125;GET /my_index/_analyze&#123; "analyzer": "es_std", "text":"a dog is in the house"&#125;&#123; "tokens": [ &#123; "token": "dog", "start_offset": 2, "end_offset": 5, "type": "&lt;ALPHANUM&gt;", "position": 1 &#125;, &#123; "token": "house", "start_offset": 16, "end_offset": 21, "type": "&lt;ALPHANUM&gt;", "position": 5 &#125; ]&#125; 自定义分词器虽然Elasticsearch带有一些现成的分析器，然而在分析器上Elasticsearch真正的强大之处在于，你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器来创建自定义的分析器。 一个分词器就是在一个包里面组合了三种函数的一个包装器， 三种函数按照顺序被执行: 字符过滤器 字符过滤器 用来 整理 一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像 &lt;p&gt; 或者 &lt;div&gt; 这样的HTML标签，这些标签是我们不想索引的。我们可以使用 html清除 字符过滤器 来移除掉所有的HTML标签，并且像把 &amp;Aacute; 转换为相对应的Unicode字符 Á 这样，转换HTML实体。 一个分析器可能有0个或者多个字符过滤器。 分词器 一个分析器 必须 有一个唯一的分词器。 分词器把字符串分解成单个词条或者词汇单元。 standard 分析器里使用的 standard 分词器 把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。 词单元过滤器 经过分词，作为结果的 词单元流 会按照指定的顺序通过指定的词单元过滤器 。举例： 123456789101112131415161718192021222324252627PUT /my_index&#123; "settings": &#123; "analysis": &#123; "char_filter": &#123; "&amp;_to_and": &#123; "type": "mapping", "mappings": ["&amp;=&gt; and"] &#125; &#125;, "filter": &#123; "my_stopwords": &#123; "type": "stop", "stopwords": ["the", "a"] &#125; &#125;, "analyzer": &#123; "my_analyzer": &#123; "type": "custom", "char_filter": ["html_strip", "&amp;_to_and"], "tokenizer": "standard", "filter": ["lowercase", "my_stopwords"] &#125; &#125; &#125; &#125;&#125; 这里创建了一个my_analyzer分析器，分词用的standard，字符过滤器用了html_strip和&amp;_to_and，词过滤器用了lowercase和my_stopwords。 测试一下分词器的效果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152GET /my_index/_analyze&#123; "text": "tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!", "analyzer": "my_analyzer"&#125;&#123; "tokens": [ &#123; "token": "tomandjerry", "start_offset": 0, "end_offset": 9, "type": "&lt;ALPHANUM&gt;", "position": 0 &#125;, &#123; "token": "are", "start_offset": 10, "end_offset": 13, "type": "&lt;ALPHANUM&gt;", "position": 1 &#125;, &#123; "token": "friend", "start_offset": 16, "end_offset": 22, "type": "&lt;ALPHANUM&gt;", "position": 3 &#125;, &#123; "token": "in", "start_offset": 23, "end_offset": 25, "type": "&lt;ALPHANUM&gt;", "position": 4 &#125;, &#123; "token": "house", "start_offset": 30, "end_offset": 35, "type": "&lt;ALPHANUM&gt;", "position": 6 &#125;, &#123; "token": "haha", "start_offset": 42, "end_offset": 46, "type": "&lt;ALPHANUM&gt;", "position": 7 &#125; ]&#125; 如何在索引中使用我们自定义的分词器？ 创建一个mapping，为content设置我们自定义的分词器 123456789PUT /my_index/_mapping/my_type&#123; "properties": &#123; "content": &#123; "type": "text", "analyzer": "my_analyzer" &#125; &#125;&#125; 类型type，是一个index中用来区分类似的文档。类似的文档可能有不同的fields，而且有不同的属性来控制索引建立、分词器。就像数据库中的 schema ，描述了文档可能具有的字段或 属性 、 每个字段的数据类型—比如 string, integer 或 date —以及Lucene是如何索引和存储这些字段的。 field的value，在底层的lucene中建立索引的时候，全部是opaque bytes类型，是不区分类型的。 Lucene 没有文档类型的概念，每个文档的类型名被存储在一个叫 _type 的元数据字段上。 当我们要检索某个类型的文档时, Elasticsearch 通过在 _type 字段上使用过滤器限制只返回这个类型的文档。 Lucene 也没有映射的概念。 映射是 Elasticsearch 将复杂 JSON 文档 映射 成 Lucene 需要的扁平化数据的方式。 一个index中的多个type，实际上是放在一起存储的，因此一个index下，不能有多个type重名，而类型或者其他设置不同的，因为那样是无法处理的。 比如有个映射和文档： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; "ecommerce": &#123; "mappings": &#123; "elactronic_goods": &#123; "properties": &#123; "name": &#123; "type": "string", &#125;, "price": &#123; "type": "double" &#125;, "service_period": &#123; "type": "string" &#125; &#125; &#125;, "fresh_goods": &#123; "properties": &#123; "name": &#123; "type": "string", &#125;, "price": &#123; "type": "double" &#125;, "eat_period": &#123; "type": "string" &#125; &#125; &#125; &#125; &#125;&#125;&#123; "name": "geli kongtiao", "price": 1999.0, "service_period": "one year"&#125;&#123; "name": "aozhou dalongxia", "price": 199.0, "eat_period": "one week"&#125; 实际在Lucene里的数据可能是： 1234567891011121314151617181920212223242526272829303132333435363738&#123; "ecommerce": &#123; "mappings": &#123; "_type": &#123; "type": "string", "index": "not_analyzed" &#125;, "name": &#123; "type": "string" &#125; "price": &#123; "type": "double" &#125; "service_period": &#123; "type": "string" &#125; "eat_period": &#123; "type": "string" &#125; &#125; &#125;&#125;&#123; "_type": "elactronic_goods", "name": "geli kongtiao", "price": 1999.0, "service_period": "one year", "eat_period": ""&#125;&#123; "_type": "fresh_goods", "name": "aozhou dalongxia", "price": 199.0, "service_period": "", "eat_period": "one week"&#125; 所以最佳实践是，将类似结构的type放在一个index下，这些type应该有多个field是相同的。假如你将两个type的field完全不同，放在一个index下，那么就每条数据都至少有一半的field在底层的lucene中是空值，会有严重的性能问题。 根对象映射的最高一层被称为 根对象 ，它可能包含下面几项： 一个 properties 节点，列出了文档中可能包含的每个字段的映射 各种元数据字段，它们都以一个下划线开头，例如 _type 、 _id 和 _source 设置项，控制如何动态处理新的字段，例如 analyzer 、 dynamic_date_formats 和 dynamic_templates 其他设置，可以同时应用在根对象和其他 object 类型的字段上，例如 enabled 、 dynamic 和 include_in_all 12345678PUT /my_index&#123; "mappings": &#123; "my_type": &#123; "properties": &#123;&#125; &#125; &#125;&#125; properties文档的字段描述，包含type，index，analyzer 12345678PUT /my_index/_mapping/my_type&#123; "properties": &#123; "title": &#123; "type": "text" &#125; &#125;&#125; _sourceElasticsearch 在 _source 字段存储代表文档体的JSON字符串。和所有被存储的字段一样， _source 字段在被写入磁盘之前先会被压缩。 好处： 查询的时候，直接可以拿到完整的document，不需要先拿document id，再发送一次请求拿document partial update基于_source实现 reindex时，直接基于_source实现，不需要从数据库（或者其他外部存储）查询数据再修改 可以基于_source定制返回field debug query更容易，因为可以直接看到_source 不需要的话，也可以禁用_source，不保存原始对象 1234PUT /my_index/_mapping/my_type2&#123; "_source": &#123;"enabled": false&#125;&#125; _all前面说过，将所有field打包在一起，作为一个_all field，建立索引。没指定任何field进行搜索时，就是使用_all field在搜索。 1234PUT /my_index/_mapping/my_type3&#123; "_all": &#123;"enabled": false&#125;&#125; 也可以在field级别设置include_in_all field，设置是否要将field的值包含在_all field中&gt; 123456789PUT /my_index/_mapping/my_type4&#123; "properties": &#123; "my_field": &#123; "type": "text", "include_in_all": false &#125; &#125;&#125; 标识性metadata _id 文档的 ID 字符串 _type 文档的类型名 _index 文档所在的索引 _uid _type 和 _id 连接在一起构造成 type#id 定制dynamic策略ES在数据遇到新的字段时候，会为我们自动mapping，但是我们也可以定制化这个策略： true：遇到陌生字段，就进行dynamic mapping false：遇到陌生字段，就忽略 strict：遇到陌生字段，就报错 修改策略为strict： 1234567891011121314151617PUT /my_index&#123; "mappings": &#123; "my_type": &#123; "dynamic": "strict", "properties": &#123; "title": &#123; "type": "text" &#125;, "address": &#123; "type": "object", "dynamic": "true" &#125; &#125; &#125; &#125;&#125; 测试添加数据，这里多了一个mapping里没有的content字段，因为是strict策略，所以ES报错了： 1234567891011121314151617181920212223PUT /my_index/my_type/1&#123; "title": "my article", "content": "this is my article", "address": &#123; "province": "guangdong", "city": "guangzhou" &#125;&#125;&#123; "error": &#123; "root_cause": [ &#123; "type": "strict_dynamic_mapping_exception", "reason": "mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed" &#125; ], "type": "strict_dynamic_mapping_exception", "reason": "mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed" &#125;, "status": 400&#125; date_detection默认会按照一定格式识别date，比如yyyy-MM-dd。但是如果某个field先过来一个2017-01-01的值，就会被自动dynamic mapping成date，后面如果再来一个”hello world”之类的值，就会报错。可以手动关闭某个type的date_detection，如果有需要，自己手动指定某个field为date类型。 1234PUT /my_index/_mapping/my_type&#123; "date_detection": false&#125; 自定义dynamic mapping template（type level）1234567891011121314151617181920212223242526272829PUT /my_index&#123; "mappings": &#123; "my_type": &#123; "dynamic_templates": [ &#123; "en": &#123; "match": "*_en", "match_mapping_type": "string", "mapping": &#123; "type": "string", "analyzer": "english" &#125; &#125; &#125;, &#123; "long_to_date": &#123; "match": "*time", "match_mapping_type": "long", "mapping": &#123; "type": "date", "index": "not_analyzed" &#125; &#125; &#125; ] &#125; &#125;&#125; 测试一下： 123456PUT /my_index/my_type/1&#123; "title": "this is my first article", "title_en": "this is my first article", "create_time": 1538031023000&#125; 看看ES为我们创建的mapping，create_time成功被映射成了date类型，title_en设置了english为分词器。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950GET /my_index/my_type/_mapping&#123; "my_index": &#123; "mappings": &#123; "my_type": &#123; "dynamic_templates": [ &#123; "en": &#123; "match": "*_en", "match_mapping_type": "string", "mapping": &#123; "analyzer": "english", "type": "string" &#125; &#125; &#125;, &#123; "long_to_date": &#123; "match": "*time", "match_mapping_type": "long", "mapping": &#123; "index": "not_analyzed", "type": "date" &#125; &#125; &#125; ], "properties": &#123; "create_time": &#123; "type": "date" &#125;, "title": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "title_en": &#123; "type": "text", "analyzer": "english" &#125; &#125; &#125; &#125; &#125;&#125; 自定义dynamic mapping template（index level）_default_就是设置这个index下所有的模板，type自己的配置可以覆盖_default_的配置。 1234567891011PUT /my_index&#123; "mappings": &#123; "_default_": &#123; "_all": &#123; "enabled": false &#125; &#125;, "blog": &#123; "_all": &#123; "enabled": true &#125; &#125; &#125;&#125; 重建索引一个field的设置是不能被修改的，如果要修改一个Field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入index中。 批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scoll就查询指定日期的一段数据，交给一个线程即可。 场景一开始，依靠dynamic mapping，插入数据，但是不小心有些数据是2017-01-01这种日期格式的，所以title这种field被自动映射为了date类型，实际上它应该是string类型的。 123456789101112131415161718PUT /my_index/my_type/3&#123; "title": "2017-01-03"&#125;&#123; "my_index": &#123; "mappings": &#123; "my_type": &#123; "properties": &#123; "title": &#123; "type": "date" &#125; &#125; &#125; &#125; &#125;&#125; 当后期向索引中加入string类型的title值的时候，就会报错 12345678910111213141516171819202122PUT /my_index/my_type/4&#123; "title": "my first article"&#125;&#123; "error": &#123; "root_cause": [ &#123; "type": "mapper_parsing_exception", "reason": "failed to parse [title]" &#125; ], "type": "mapper_parsing_exception", "reason": "failed to parse [title]", "caused_by": &#123; "type": "illegal_argument_exception", "reason": "Invalid format: \"my first article\"" &#125; &#125;, "status": 400&#125; 如果此时想修改title的类型，是不可能的 12345678910111213141516171819202122PUT /my_index/_mapping/my_type&#123; "properties": &#123; "title": &#123; "type": "text" &#125; &#125;&#125;&#123; "error": &#123; "root_cause": [ &#123; "type": "illegal_argument_exception", "reason": "mapper [title] of different type, current_type [date], merged_type [text]" &#125; ], "type": "illegal_argument_exception", "reason": "mapper [title] of different type, current_type [date], merged_type [text]" &#125;, "status": 400&#125; 此时，唯一的办法就是进行reindex。重新建立一个索引，将旧索引的数据查询出来，再导入新索引。 比如旧索引的名字，是old_index，新索引的名字是new_index，终端是java应用，已经在使用old_index在操作了，难道还要去停止java应用，修改使用的index为new_index，才重新启动java应用吗？这个过程中，就会导致java应用停机，可用性降低，这样肯定不好。 那么在我们的例子中先给旧索引一个别名，客户端先用goods_index这个别名来操作，此时实际指向的是旧的my_index。 1PUT /my_index/_alias/goods_index 然后新建一个index，调整其title的类型为string 123456789101112PUT /my_index_new&#123; "mappings": &#123; "my_type": &#123; "properties": &#123; "title": &#123; "type": "text" &#125; &#125; &#125; &#125;&#125; 使用scroll api将数据批量查询出来 12345678910111213141516171819202122232425262728293031323334353637GET /my_index/_search?scroll=1m&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": ["_doc"], "size": 1&#125;&#123; "_scroll_id": "DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAwhlFkxyT3g1eWVVUVFhWWJGNEwxM3o1WncAAAAAAAMIZhZMck94NXllVVFRYVliRjRMMTN6NVp3AAAAAAADCGQWTHJPeDV5ZVVRUWFZYkY0TDEzejVadwAAAAAAAwhnFkxyT3g1eWVVUVFhWWJGNEwxM3o1WncAAAAAAAMIaBZMck94NXllVVFRYVliRjRMMTN6NVp3", "took": 38, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": null, "hits": [ &#123; "_index": "my_index", "_type": "my_type", "_id": "2", "_score": null, "_source": &#123; "title": "2017-01-02" &#125;, "sort": [ 0 ] &#125; ] &#125;&#125; 采用bulk api将scoll查出来的一批数据，批量写入新索引。 123POST /_bulk&#123; "index":&#123; "_index": "my_index_new", "_type": "my_type", "_id": "2" &#125;&#125;&#123; "title":"2017-01-02" &#125; 反复循环，查询一批又一批的数据出来，采取bulk api将每一批数据批量写入新索引。 将goods_index alias切换到my_index_new上去，java应用会直接通过index别名使用新的索引中的数据，java应用程序不需要停机，零停机，高可用。 1234567POST /_aliases&#123; "actions": [ &#123; "remove": &#123; "index": "my_index", "alias": "goods_index" &#125;&#125;, &#123; "add": &#123; "index": "my_index_new", "alias": "goods_index" &#125;&#125; ]&#125; 直接通过goods_index别名来查询，是否成功，这里只有一条，因为我只操作了一条。 1234567891011121314151617181920212223242526GET /goods_index/my_type/_search&#123; "took": 24, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "my_index_new", "_type": "my_type", "_id": "2", "_score": 1, "_source": &#123; "title": "2017-01-02" &#125; &#125; ] &#125;&#125;]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch06-_Search API]]></title>
    <url>%2F2018%2F09%2F21%2FelasticSearch06%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 前面讲了简单的搜索、分词和映射，这篇文章讲ES的搜索API search APISearch API的基本语法： 这是一个空查询，空查询将返回所有索引库（indices)中的所有文档： 12GET /_search&#123;&#125; 只用一个查询字符串，你就可以在一个、多个或者 _all 索引库（indices）和一个、多个或者所有types中查询： 12GET /index_2014*/type1,type2/_search&#123;&#125; 同时你可以使用 from 和 size 参数来分页： 12345GET /_search&#123; "from": 30, "size": 10&#125; ##一个带请求体的 GET 请求？ 某些特定语言（特别是 JavaScript）的 HTTP 库是不允许 GET 请求带有请求体的。 事实上，一些使用者对于 GET 请求可以带请求体感到非常的吃惊。 而事实是这个RFC文档 RFC 7231— 一个专门负责处理 HTTP 语义和内容的文档 — 并没有规定一个带有请求体的 GET 请求应该如何处理！结果是，一些 HTTP 服务器允许这样子，而有一些 — 特别是一些用于缓存和代理的服务器 — 则不允许。 对于一个查询请求，Elasticsearch 的工程师偏向于使用 GET 方式，因为他们觉得它比 POST 能更好的描述信息检索（retrieving information）的行为。然而，因为带请求体的 GET 请求并不被广泛支持，所以 search API 同时支持 POST 请求： 12345POST /_search&#123; "from": 30, "size": 10&#125; 类似的规则可以应用于任何需要带请求体的 GET API。 query dsl查询表达式(Query DSL)是一种非常灵活又富有表现力的 查询语言。 Elasticsearch 使用它可以以简单的 JSON 接口来展现 Lucene 功能的绝大部分。在你的应用中，你应该用它来编写你的查询语句。它可以使你的查询语句更灵活、更精确、易读和易调试。 要使用这种查询表达式，只需将查询语句传递给 query 参数： 123456GET /_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125; Query DSL 基本语法123456789101112131415&#123; QUERY_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125;&#125;&#123; QUERY_NAME: &#123; FIELD_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125; &#125;&#125; 示例： 12345678GET /test_index/test_type/_search &#123; "query": &#123; "match": &#123; "test_field": "test" &#125; &#125;&#125; 组合多个搜索条件利用bool查询进行组合搜索，数据准备： 1234567891011121314151617181920PUT /website/article/1&#123; "title": "my elasticsearch article", "content": "es is very good", "author_id": 110&#125;PUT /website/article/2&#123; "title": "my hadoop article", "content": "hadoop is very good", "author_id": 111&#125;PUT /website/article/3&#123; "title": "my elasticsearch article", "content": "es is very bad", "author_id": 111&#125; 搜索需求：title必须包含elasticsearch，content可以包含elasticsearch也可以不包含，author_id必须不为111 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455GET /website/article/_search&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "title": "elasticsearch" &#125; &#125; ], "should": [ &#123; "match": &#123; "content": "elasticsearch" &#125; &#125; ], "must_not": [ &#123; "match": &#123; "author_id": "111" &#125; &#125; ] &#125; &#125;&#125;&#123; "took": 23, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 0.25316024, "hits": [ &#123; "_index": "website", "_type": "article", "_id": "1", "_score": 0.25316024, "_source": &#123; "title": "my elasticsearch article", "content": "es is very good", "author_id": 110 &#125; &#125; ] &#125;&#125; 查询和过滤（query &amp; filter)在ES中查找数据，除了查询还有过滤 query与filter示例1234567891011121314151617181920212223PUT /company/employee/2&#123; "address": &#123; "country": "china", "province": "jiangsu", "city": "nanjing" &#125;, "name": "tom", "age": 30, "join_date": "2016-01-01"&#125;PUT /company/employee/3&#123; "address": &#123; "country": "china", "province": "shanxi", "city": "xian" &#125;, "name": "marry", "age": 35, "join_date": "2015-01-01"&#125; 搜索请求：年龄必须大于等于30，同时join_date必须是2016-01-01 123456789101112131415161718192021GET /company/employee/_search&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "join_date": "2016-01-01" &#125; &#125; ], "filter": &#123; "range": &#123; "age": &#123; "gte": 30 &#125; &#125; &#125; &#125; &#125;&#125; filter与query对比 filter：仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响。 query：会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序。 一般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter。 filter与query性能比较 filter：不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据 query：相反，要计算相关度分数，按照分数进行排序，而且无法cache结果 query搜索实战match all查询match_all 查询简单的 匹配所有文档。在没有指定查询方式时，它是默认的查询： 123456GET /_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125; match查询无论你在任何字段上进行的是全文搜索还是精确查询，match 查询是你可用的标准查询。 如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串，如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 not_analyzed 字符串字段，那么它将会精确匹配给定的值。 1234GET /_search&#123; "query": &#123; "match": &#123; "title": "my elasticsearch article" &#125;&#125;&#125; 对于精确值的查询，你可能需要使用 filter 语句来取代 query，因为 filter 将会被缓存。 multi match查询multi_match 查询可以在多个字段上执行相同的 match 查询： 1234567GET /_search&#123; "multi_match": &#123; "query": "full text search", "fields": [ "title", "body" ] &#125;&#125; range 查询range 查询找出那些落在指定区间内的数字或者时间： 12345678&#123; "range": &#123; "age": &#123; "gte": 20, "lt": 30 &#125; &#125;&#125; 被允许的操作符如下： gt 大于gte 大于等于lt 小于lte 小于等于 term 查询term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串： 1234&#123; "term": &#123; "age": 26 &#125;&#125;&#123; "term": &#123; "date": "2014-09-01" &#125;&#125;&#123; "term": &#123; "public": true &#125;&#125;&#123; "term": &#123; "tag": "full_text" &#125;&#125; term 查询对于输入的文本不 分析 ，所以它将给定的值进行精确查询。 terms查询terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件： 1234GET /_search&#123; "query": &#123; "terms": &#123; "tag": [ "search", "full_text", "nosql" ] &#125;&#125;&#125; 和 term 查询一样，terms 查询对于输入的文本不分析。它查询那些精确匹配的值（包括在大小写、重音、空格等方面的差异）。 exists 查询这是2.x中的查询，现在已经不提供了，但是可以用filter来完成同样的效果： 12345678910111213GET /company/employee/_search&#123; "query": &#123; "bool": &#123; "filter": &#123; "exists": &#123; "field": "age" &#125; &#125; &#125; &#125;&#125; 组合查询现实的查询需求从来都没有那么简单；它们需要在多个字段上查询多种多样的文本，并且根据一系列的标准来过滤。为了构建类似的高级查询，你需要一种能够将多查询组合成单一查询的查询方法。 你可以用 bool 查询来实现你的需求。这种查询将多查询组合在一起，成为用户自己想要的布尔查询。它接收以下参数： must 文档 必须 匹配这些条件才能被包含进来。 must_not 文档 必须不 匹配这些条件才能被包含进来。 should 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分。 filter 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。 由于这是我们看到的第一个包含多个查询的查询，所以有必要讨论一下相关性得分是如何组合的。每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 下面的查询用于查找 title 字段匹配 how to make millions 并且不被标识为 spam 的文档。那些被标识为 starred 或在2014之后的文档，将比另外那些文档拥有更高的排名。如果 _两者_ 都满足，那么它排名将更高： 12345678910&#123; "bool": &#123; "must": &#123; "match": &#123; "title": "how to make millions" &#125;&#125;, "must_not": &#123; "match": &#123; "tag": "spam" &#125;&#125;, "should": [ &#123; "match": &#123; "tag": "starred" &#125;&#125;, &#123; "range": &#123; "date": &#123; "gte": "2014-01-01" &#125;&#125;&#125; ] &#125;&#125; 增加带过滤器（filter）的查询如果我们不想因为文档的时间而影响得分，可以用 filter 语句来重写前面的例子： 123456789101112&#123; "bool": &#123; "must": &#123; "match": &#123; "title": "how to make millions" &#125;&#125;, "must_not": &#123; "match": &#123; "tag": "spam" &#125;&#125;, "should": [ &#123; "match": &#123; "tag": "starred" &#125;&#125; ], "filter": &#123; "range": &#123; "date": &#123; "gte": "2014-01-01" &#125;&#125; &#125; &#125;&#125; constant_score查询尽管没有 bool 查询使用这么频繁，constant_score 查询也是你工具箱里有用的查询工具。它将一个不变的常量评分应用于所有匹配的文档。它被经常用于你只需要执行一个 filter 而没有其它查询（例如，评分查询）的情况下。 可以使用它来取代只有 filter 语句的 bool 查询。在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助。 { “constant_score”: { “filter”: { “term”: { “category”: “ebooks” } } }} term 查询被放置在 constant_score 中，转成不评分的 filter。这种方式可以用来取代只有 filter 语句的 bool 查询。 验证查询查询可以变得非常的复杂，尤其 和不同的分析器与不同的字段映射结合时，理解起来就有点困难了。不过 validate-query API 可以用来验证查询是否合法。 12345678910111213GET /test_index/test_type/_validate/query?explain&#123; "query": &#123; "math": &#123; "test_field": "test" &#125; &#125;&#125;&#123; "valid": false, "error": "org.elasticsearch.common.ParsingException: no [query] registered for [math]"&#125; 说没有math这个查询，一看是单词写错了，应该是match。 这个一般用在那种特别复杂庞大的搜索下，比如写了上百行的搜索，这个时候可以先用validate api去验证一下，搜索是否合法。 排序为了按照相关性来排序，需要将相关性表示为一个数值。在 Elasticsearch 中， 相关性得分 由一个浮点数进行表示，并在搜索结果中通过 _score 参数返回， 默认排序是 _score 降序。 有时，相关性评分对你来说并没有意义。例如，下面的查询返回所有 user_id 字段包含 1 的结果： 123456789101112GET /_search&#123; "query" : &#123; "bool" : &#123; "filter" : &#123; "term" : &#123; "user_id" : 1 &#125; &#125; &#125; &#125;&#125; 定制排序规则有时候我们查询的数据需要根据时间，数量之类的排序。我们可以使用 sort 参数进行实现： 123456789101112131415161718192021GET /company/employee/_search &#123; "query": &#123; "constant_score": &#123; "filter": &#123; "range": &#123; "age": &#123; "gte": 30 &#125; &#125; &#125; &#125; &#125;, "sort": [ &#123; "join_date": &#123; "order": "asc" &#125; &#125; ]&#125; String排序如果对一个string field进行排序，结果往往不准确，因为分词后是多个单词，再排序就不是我们想要的结果了。 通常解决方案是，将一个string field建立两次索引，一个分词，用来进行搜索；一个不分词，用来进行排序。 实验一下，重新建索引，为title设置一个分词的，和一个不分词的。 12345678910111213141516171819202122232425262728DELETE /websitePUT /website &#123; "mappings": &#123; "article": &#123; "properties": &#123; "title": &#123; "type": "text", "fields": &#123; "raw": &#123; "type": "keyword" &#125; &#125;, "fielddata": true &#125;, "content": &#123; "type": "text" &#125;, "post_date": &#123; "type": "date" &#125;, "author_id": &#123; "type": "long" &#125; &#125; &#125; &#125;&#125; 在 ES2.x 版本字符串数据是没有 keyword 和 text 类型的，只有string类型，ES更新到5版本后，取消了 string 数据类型，代替它的是 keyword 和 text 数据类型，那么 keyword 和 text 有什么区别了？Text 数据类型被用来索引长文本，比如说电子邮件的主体部分或者一款产品的介绍。这些文本会被分析，在建立索引前会将这些文本进行分词，转化为词的组合，建立索引。允许 ES来检索这些词语。text 数据类型不能用来排序和聚合。Keyword 数据类型用来建立电子邮箱地址、姓名、邮政编码和标签等数据，不需要进行分词。可以被用来检索过滤、排序和聚合。keyword 类型字段只能用本身来进行检索。 准备数据： 123456789101112131415PUT /website/article/1&#123; "title": "first article", "content": "this is my first article", "post_date": "2017-01-01", "author_id": 110&#125;PUT /website/article/2&#123; "title": "second article", "content": "this is my second article", "post_date": "2017-01-02", "author_id": 110&#125; 搜索： 12345678910111213141516171819202122232425262728GET /website/article/_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": [ &#123; "title": &#123; "order": "desc" &#125; &#125; ]&#125;GET /website/article/_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": [ &#123; "title.raw": &#123; "order": "desc" &#125; &#125; ]&#125; 相关性评分每个文档都有相关性评分，用一个正浮点数字段 _score 来表示 。 _score 的评分越高，相关性越高。 查询语句会为每个文档生成一个 _score 字段。简单来说，就是计算出一个索引中的文本，与搜索文本他们之间的关联匹配程度。 在 Elasticsearch 中, 标准的算法是 Term Frequency/Inverse Document Frequency, 简写为 TF/IDF, (5.0 以上版本, 改为了据说更先进的 BM25 算法) Term frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关 Inverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关 Field-length norm：field长度越长，相关度越弱 理解评分标准当调试一条复杂的查询语句时， 想要理解 _score 究竟是如何计算是比较困难的。Elasticsearch 在 每个查询语句中都有一个 explain 参数，将 explain 设为 true 就可以得到更详细的信息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231GET /test_index/test_type/_search?explain&#123; "query": &#123; "match": &#123; "test_field": "test" &#125; &#125;&#125;&#123; "took": 2, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 0.25316024, "hits": [ &#123; "_shard": "[test_index][1]", "_node": "LrOx5yeUQQaYbF4L13z5Zw", "_index": "test_index", "_type": "test_type", "_id": "8", "_score": 0.25316024, "_source": &#123; "test_field": "test client 2" &#125;, "_explanation": &#123; "value": 0.25316024, "description": "sum of:", "details": [ &#123; "value": 0.25316024, "description": "weight(test_field:test in 0) [PerFieldSimilarity], result of:", "details": [ &#123; "value": 0.25316024, "description": "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:", "details": [ &#123; "value": 0.2876821, "description": "idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:", "details": [ &#123; "value": 1, "description": "docFreq", "details": [] &#125;, &#123; "value": 1, "description": "docCount", "details": [] &#125; ] &#125;, &#123; "value": 0.88, "description": "tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:", "details": [ &#123; "value": 1, "description": "termFreq=1.0", "details": [] &#125;, &#123; "value": 1.2, "description": "parameter k1", "details": [] &#125;, &#123; "value": 0.75, "description": "parameter b", "details": [] &#125;, &#123; "value": 3, "description": "avgFieldLength", "details": [] &#125;, &#123; "value": 4, "description": "fieldLength", "details": [] &#125; ] &#125; ] &#125; ] &#125;, &#123; "value": 0, "description": "match on required clause, product of:", "details": [ &#123; "value": 0, "description": "# clause", "details": [] &#125;, &#123; "value": 1, "description": "*:*, product of:", "details": [ &#123; "value": 1, "description": "boost", "details": [] &#125;, &#123; "value": 1, "description": "queryNorm", "details": [] &#125; ] &#125; ] &#125; ] &#125; &#125;, &#123; "_shard": "[test_index][3]", "_node": "LrOx5yeUQQaYbF4L13z5Zw", "_index": "test_index", "_type": "test_type", "_id": "7", "_score": 0.25316024, "_source": &#123; "test_field": "test client 2" &#125;, "_explanation": &#123; "value": 0.25316024, "description": "sum of:", "details": [ &#123; "value": 0.25316024, "description": "weight(test_field:test in 0) [PerFieldSimilarity], result of:", "details": [ &#123; "value": 0.25316024, "description": "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:", "details": [ &#123; "value": 0.2876821, "description": "idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from:", "details": [ &#123; "value": 1, "description": "docFreq", "details": [] &#125;, &#123; "value": 1, "description": "docCount", "details": [] &#125; ] &#125;, &#123; "value": 0.88, "description": "tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from:", "details": [ &#123; "value": 1, "description": "termFreq=1.0", "details": [] &#125;, &#123; "value": 1.2, "description": "parameter k1", "details": [] &#125;, &#123; "value": 0.75, "description": "parameter b", "details": [] &#125;, &#123; "value": 3, "description": "avgFieldLength", "details": [] &#125;, &#123; "value": 4, "description": "fieldLength", "details": [] &#125; ] &#125; ] &#125; ] &#125;, &#123; "value": 0, "description": "match on required clause, product of:", "details": [ &#123; "value": 0, "description": "# clause", "details": [] &#125;, &#123; "value": 1, "description": "*:*, product of:", "details": [ &#123; "value": 1, "description": "boost", "details": [] &#125;, &#123; "value": 1, "description": "queryNorm", "details": [] &#125; ] &#125; ] &#125; ] &#125; &#125; ] &#125;&#125; 它提供了 _explanation 。每个 入口都包含一个 description 、 value 、 details 字段，它分别告诉你计算的类型、计算结果和任何我们需要的计算细节。 因为我测试的版本是5.2，所以从结果看出并不是上面介绍的TF/IDF算法，大致可以看到IDF，依然存在，但是Term frequency和Field-length norm则改为了一个组合算法(tfNorm)。 具体可以参考文章 ElasticSearch 的分数 (_score) 是怎么计算得出 (2.X &amp; 5.X) tfNorm反映的该term在所有满足条件的doc中field中的重要性，一般来说，相同的freq 下，field的长度越短，那么取值就越高。 IDF反映的是term的影响因子，如果docCount很大，docFreq很小，标示该term在doc之间具有很好的分辨力，当然IDF值也就越大。 文档是如何被匹配上的当 explain 选项加到某一文档上时， explain api 会帮助你理解为何这个文档会被匹配，更重要的是，一个文档为何没有被匹配。 请求路径为 /index/type/id/_explain ，如下所示： 12345678GET /test_index/test_type/7/_explain&#123; "query": &#123; "match": &#123; "test_field": "world" &#125; &#125;&#125; 不只是我们之前看到的充分解释 ，我们现在有了一个 description 元素，它将告诉我们： &quot;description&quot;: &quot;no match on required clause (test_field:world)&quot;, Doc Values搜索的时候，要依靠倒排索引；排序的时候，需要依靠正排索引，看到每个document的每个field，然后进行排序，所谓的正排索引，其实就是doc values。在建立索引的时候，一方面会建立倒排索引，以供搜索用；一方面会建立正排索引，也就是doc values，以供排序，聚合，过滤等操作使用。doc values是被保存在磁盘上的，此时如果内存足够，os会自动将其缓存在内存中，性能还是会很高；如果内存不足够，os会将其写入磁盘上。 分布式检索过程讨论一下在分布式环境中搜索是怎么执行的，搜索被执行成一个两阶段过程，我们称之为 query then fetch 。 查询阶段在初始 查询阶段 时， 查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的 优先队列。 优先队列一个 优先队列 仅仅是一个存有 top-n 匹配文档的有序列表。优先队列的大小取决于分页参数 from 和 size 。例如，如下搜索请求将需要足够大的优先队列来放入100条文档。 12345GET /_search&#123; "from": 90, "size": 10&#125; 查询过程分布式搜索 查询阶段包含以下三个步骤: 客户端发送一个 search 请求到 Node 3 ， Node 3 会创建一个大小为 from + size 的空优先队列。 Node 3 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。 每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。 这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。 这个也跟前面说的deep paging问题有关，from + size 分页太深，每个分片都要返回大量的数据给协调节点，会消耗大量的带宽，内存，cpu。 replica shard如何增加查询吞吐量 第一步是广播请求到索引中每一个节点的分片拷贝。查询请求可以被某个主分片或某个副本分片处理， 这就是为什么更多的副本（当结合更多的硬件）能够增加搜索吞吐率。 协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。 取回阶段查询阶段标识哪些文档满足 搜索请求，但是我们仍然需要取回这些文档。这是取回阶段的任务, 正如 图 “分布式搜索的取回阶段” 所展示的。 分布式阶段由以下步骤构成： 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。 每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。 一旦所有的文档都被取回了，协调节点返回结果给客户端。 协调节点首先决定哪些文档 确实 需要被取回。例如，如果我们的查询指定了 { “from”: 90, “size”: 10 } ，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。这些文档可能来自和最初搜索请求有关的一个、多个甚至全部分片。 协调节点给持有相关文档的每个分片创建一个 multi-get request ，并发送请求给同样处理查询阶段的分片副本。 一般搜索，如果不加from和size，就默认搜索前10条，按照_score排序。 搜索参数有几个搜索参数可以影响搜索过程。 preferencepreference 参数允许用来控制由哪些分片或节点来处理搜索请求。 它接受像 _primary, _primary_first, _local, _only_node:xyz, _prefer_node:xyz, 和 _shards:2,3 这样的值。 Bouncing Results想象一下有两个文档有同样值的时间戳字段，搜索结果用 timestamp 字段来排序。 由于搜索请求是在所有有效的分片副本间轮询的，那就有可能发生主分片处理请求时，这两个文档是一种顺序， 而副本分片处理请求时又是另一种顺序。 这就是所谓的 bouncing results 问题: 每次用户刷新页面，搜索结果表现是不同的顺序。 让同一个用户始终使用同一个分片，这样可以避免这种问题， 可以设置 preference 参数为一个特定的任意值比如用户会话ID来解决。 timeout通常分片处理完它所有的数据后再把结果返回给协同节点，协同节点把收到的所有结果合并为最终结果。 这意味着花费的时间是最慢分片的处理时间加结果合并的时间。如果有一个节点有问题，就会导致所有的响应缓慢。 参数 timeout 告诉 分片允许处理数据的最大时间。如果没有足够的时间处理所有数据，这个分片的结果可以是部分的，甚至是空数据。 routing定制参数 routing ，它能够在索引时提供来确保相关的文档，比如属于某个用户的文档被存储在某个分片上。 在搜索的时候，不用搜索索引的所有分片，而是通过指定几个 routing 值来限定只搜索几个相关的分片，默认是_id路由，也可以指定字段?routing=user_1,user2。 search_type缺省的搜索类型是 query_then_fetch 。 在某些情况下，你可能想明确设置 search_type 为 dfs_query_then_fetch 来改善相关性精确度： 1GET /_search?search_type=dfs_query_then_fetch 搜索类型 dfs_query_then_fetch 有预查询阶段，这个阶段可以从所有相关分片获取词频来计算全局词频。 scroll 查询如果是用from + size的查询方式，会有我们之前说的Deep Paging问题，而且ES默认也限制了size的大小最多只能是1w条。如果一次性要查出来比如10万条数据，那么性能会很差，此时一般会采取用scoll滚动查询，一批一批的查，直到所有数据都查询完处理完。 使用scoll滚动搜索，可以先搜索一批数据，然后下次再搜索一批数据，以此类推，直到搜索出全部的数据来。scoll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的。采用基于_doc进行排序的方式，性能较高。每次发送scroll请求，我们还需要指定一个scoll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了。 123456789101112131415161718192021GET /test_index/test_type/_search?scroll=1m&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": [ "_doc" ], "size": 1&#125;&#123; "_scroll_id": "DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAOOqFkxyT3g1eWVVUVFhWWJGNEwxM3o1WncAAAAAAADjqxZMck94NXllVVFRYVliRjRMMTN6NVp3AAAAAAAA46wWTHJPeDV5ZVVRUWFZYkY0TDEzejVadwAAAAAAAOOuFkxyT3g1eWVVUVFhWWJGNEwxM3o1WncAAAAAAADjrRZMck94NXllVVFRYVliRjRMMTN6NVp3", "took": 2, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 3, ..... 这个查询的返回结果包括一个字段 _scroll_id， 它是一个base64编码的长字符串 (((“scroll_id”))) 。 现在我们能传递字段 _scroll_id 到 _search/scroll查询接口获取下一批结果：下一次再发送scoll请求的时候，必须带上这个scoll_id 12345GET /_search/scroll&#123; "scroll": "1m", "scroll_id" : "DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAOOqFkxyT3g1eWVVUVFhWWJGNEwxM3o1WncAAAAAAADjqxZMck94NXllVVFRYVliRjRMMTN6NVp3AAAAAAAA46wWTHJPeDV5ZVVRUWFZYkY0TDEzejVadwAAAAAAAOOuFkxyT3g1eWVVUVFhWWJGNEwxM3o1WncAAAAAAADjrRZMck94NXllVVFRYVliRjRMMTN6NVp3"&#125; 这个scroll查询返回的下一批结果。 尽管我们指定字段 size 的值为1000，我们有可能取到超过这个值数量的文档。 当查询的时候， 字段 size 作用于单个分片，所以每个批次实际返回的文档数量最大为 size * number_of_primary_shards 。 scoll看起来挺像分页的，但是其实使用场景不一样。分页主要是用来一页一页搜索，给用户看的；scoll主要是用来一批一批检索数据，让系统进行处理的。]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch05-搜索详解]]></title>
    <url>%2F2018%2F09%2F19%2FelasticSearch05%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 前面讲了Document的增删改查和集群原理，接下来就是ES的重头戏了，搜索。 _search结果解析当发出一个搜索请求的时候，会拿到很多结果，下面说一下搜索结果里的各种数据，都代表了什么含义。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109GET _search&#123; "took": 10, "timed_out": false, "_shards": &#123; "total": 16, "successful": 16, "failed": 0 &#125;, "hits": &#123; "total": 8, "max_score": 1, "hits": [ &#123; "_index": ".kibana", "_type": "config", "_id": "5.2.0", "_score": 1, "_source": &#123; "buildNum": 14695 &#125; &#125;, &#123; "_index": "test_index", "_type": "test_type", "_id": "8", "_score": 1, "_source": &#123; "test_field": "test client 2" &#125; &#125;, &#123; "_index": "test_index", "_type": "test_type", "_id": "10", "_score": 1, "_source": &#123; "test_field1": "test1", "test_field2": "updated test2" &#125; &#125;, &#123; "_index": "ecommerce", "_type": "product", "_id": "2", "_score": 1, "_source": &#123; "name": "jiajieshi yagao", "desc": "youxiao fangzhu", "price": 25, "producer": "jiajieshi producer", "tags": [ "fangzhu" ] &#125; &#125;, &#123; "_index": "ecommerce", "_type": "product", "_id": "1", "_score": 1, "_source": &#123; "name": "gaolujie yagao", "desc": "gaoxiao meibai", "price": 30, "producer": "gaolujie producer", "tags": [ "meibai", "fangzhu" ] &#125; &#125;, &#123; "_index": "test_index", "_type": "test_type", "_id": "7", "_score": 1, "_source": &#123; "test_field": "test client 2" &#125; &#125;, &#123; "_index": "test_index1", "_type": "test_type", "_id": "1", "_score": 1, "_source": &#123; "test": "hello es" &#125; &#125;, &#123; "_index": "ecommerce", "_type": "product", "_id": "3", "_score": 1, "_source": &#123; "name": "zhonghua yagao", "desc": "caoben zhiwu", "price": 40, "producer": "zhonghua producer", "tags": [ "qingxin" ] &#125; &#125; ] &#125;&#125; took：整个搜索请求花费了多少毫秒 hits.total：本次搜索，返回了几条结果 hits.max_score：本次搜索的所有结果中，最大的相关度分数是多少，每一条document对于search的相关度，越相关，_score分数越大，排位越靠前 hits.hits：默认查询前10条数据，包含完整数据，_score降序排序 shards：shards fail的条件（primary和replica全部挂掉），不影响其他shard。默认情况下来说，一个搜索请求，会打到一个index的所有primary shard上去，当然了，每个primary shard都可能会有一个或多个replic shard，所以请求也可以到primary shard的其中一个replica shard上去。 timeout：默认没有所谓的timeout，如果搜索特别慢每个shard都要好几分钟，那么搜索请求会一直等待结果返回。ES提供了timeout机制，指定每个shard在设置的timeout时间内马上已经搜索到的数据（可能是部分，也可能是全部），直接返回给client程序，而不是等到所有的数据全部搜索出来以后再返回。确保一次请求可以在用户指定的timeout时常内完成，为一些时间敏感的搜索应用提供良好支持。 GET /_search?timeout=10m multi-index和multi-type搜索模式如何一次性搜索多个index和多个type下的数据 /_search：所有索引，所有type下的所有数据都搜索出来 /index1/_search：指定一个index，搜索其下所有type的数据 /index1,index2/_search：同时搜索两个index下的数据 /*1,*2/_search：按照通配符去匹配多个索引 /index1/type1/_search：搜索一个index下指定的type的数据 /index1/type1,type2/_search：可以搜索一个index下多个type的数据 /index1,index2/type1,type2/_search：搜索多个index下的多个type的数据 /_all/type1,type2/_search：_all，可以代表搜索所有index下的指定type的数据 搜索基本原理客户端发送一个搜索请求，会把请求分配到所有的primary shard上去执行，因为每个shard都包含部分数据，所以每个shard上都可能会包含搜索请求的结果。但是如果primary shard有replica shard，那么请求也可以分配到replica shard上去执行。 分页搜索分页搜索语法参数： size: 一页多少条 from：从多少条开始 1GET /_search?size=10&amp;from=0 返回结果里面写了总共有多少条： 12"hits": &#123; "total": 8, deep paging问题deep paging就是搜索特别深，比如总共有3w条数据，每页10条数据，搜索最后一页 请求先发到coordinate node（通常是client节点），然后请求会分配到不同的节点上去找数据，每个个shard都会把所有的数据找出来，排序后取最后10条，返回给客户端。 这个过程会耗费很大的网络带宽、内存和CPU，所以deep paging有较大的性能问题，应该尽量避免做出这种deep paging操作。 query string语法介绍123GET /test_index/test_type/_search?q=test_field:testGET /test_index/test_type/_search?q=+test_field:testGET /test_index/test_type/_search?q=-test_field:test test_field包含test test_field不包含test _all metadata介绍GET /test_index/test_type/_search?q=test 直接可以搜索所有的field，任意一个field包含指定的关键字就可以搜索出来。我们在进行中搜索的时候，难道是对document中的每一个field都进行一次搜索吗？不是这样的。 es中的_all元数据，在建立索引的时候，我们插入一条document，它里面包含了多个field，此时es会自动将多个field的值，全部用字符串的方式串联起来，变成一个长的字符串，作为_all field的值，同时建立索引。 后面如果在搜索的时候，没有对某个field指定搜索，就默认搜索_all field，其中是包含了所有field的值的。 举个例子 123456&#123; "name": "jack", "age": 26, "email": "jack@sina.com", "address": "guamgzhou"&#125; jack 26 jack@sina.com guangzhou，作为这一条document的_all field的值，同时进行分词后建立对应的倒排索引。 mapping介绍先插入几条数据，让ES自动建立一个索引： 1234567891011121314151617181920212223PUT /website/article/1&#123; "post_date": "2017-01-01", "title": "my first article", "content": "this is my first article in this website", "author_id": 11400&#125;PUT /website/article/2&#123; "post_date": "2017-01-02", "title": "my second article", "content": "this is my second article in this website", "author_id": 11400&#125;PUT /website/article/3&#123; "post_date": "2017-01-03", "title": "my third article", "content": "this is my third article in this website", "author_id": 11400&#125; 下面进行搜索测试： 1234GET /website/article/_search?q=2017 3条结果 GET /website/article/_search?q=2017-01-01 3条结果GET /website/article/_search?q=post_date:2017-01-01 1条结果GET /website/article/_search?q=post_date:2017 1条结果 这个搜索结果，不太符合我们的期望，这里涉及到ES的mapping了，具体这个数字是怎么搜索出来的，后面案例详解会讲到。 自动或手动为index中的type建立的一种数据结构和相关配置，简称为mapping，dynamic mapping，就是ES自动为我们建立index，创建type，以及type对应的mapping，mapping中包含了每个field对应的数据类型，以及如何分词等设置。 如何查看mapping？ 123456789101112131415161718192021222324252627282930313233343536GET /website/_mapping/article&#123; "website": &#123; "mappings": &#123; "article": &#123; "properties": &#123; "author_id": &#123; "type": "long" &#125;, "content": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "post_date": &#123; "type": "date" &#125;, "title": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 搜索结果为什么不一致，因为es自动建立mapping的时候，为不同的field设置了不同的data type。不同的data type的分词、搜索等行为是不一样的。所以出现了_all field和post_date field的搜索表现完全不一样的结果。 精确搜索和全文搜索对比精确搜索2017-01-01，用精确值搜索的时候，必须输入2017-01-01，才能搜索出来如果你输入一个01，是搜索不出来的 全文搜索 缩写 vs. 全称：cn vs. china 格式转化：like liked likes 大小写：Tom vs tom 同义词：like vs love 2017-01-01，2017 01 01，搜索2017，或者01，都可以搜索出来 china，搜索cn，也可以将china搜索出来likes，搜索like，也可以将likes搜索出来Tom，搜索tom，也可以将Tom搜索出来like，搜索love，同义词，也可以将like搜索出来 就不是说单纯的只是匹配完整的一个值，而是可以对值进行拆分词语后（分词）进行匹配，也可以通过缩写、时态、大小写、同义词等进行匹配 倒排索引倒排索引是实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。 先来两个文档： 12doc1：I really liked my small dogs, and I think my mom also liked them.doc2：He never liked any dogs, so I hope that my mom will not expect me to liked him. 接下来分词，初步的倒排索引的建立。 演示一下倒排索引最简单的建立的一个过程 12345678910111213141516171819202122232425word doc1 doc2I * *really *liked * *my * *small * dogs *and *think *mom * *also *them * He *never *any *so *hope *that *will *not *expect *me *to *him * 如果用mother like little dog去搜索，是搜不到结果的，搜索的时候会被拆成4个单词去搜索，即 1234motherlikelittledog 但是，在我们看来mother和mom是同义词，like和liked也是一样的意思，little和small也一样，dog和dogs也一样。 所以在简历倒排所以的时候，会执行一个操作，对拆分出的各个单词进行相应的处理，以提升后面搜索的时候能够搜索到相关联的文档的概率，这个过程叫normalization。 比如时态的转换，单复数的转换，同义词的转换，大小写的转换等等 mom —&gt; motherliked —&gt; likesmall —&gt; littledogs —&gt; dog 重新建立倒排索引，加入normalization，再次用mother liked little dog搜索，doc1和doc2都会搜索出来。 分词器介绍分词器，是将用户输入的一段文本，分析成符合逻辑的一种工具，给你一段文本，然后将这段句子拆分成一个一个的单个的单词，同时对每个单词进行normalization（时态转换，单复数转换），分词器提升recall召回率（召回率：搜索的时候，增加能够搜索到的结果的数量） character filter：在一段文本进行分词之前，先进行预处理，比如说最常见的就是，过滤html标签（hello –&gt; hello），&amp; –&gt; and（I&amp;you –&gt; I and you） tokenizer：分词，hello you and me –&gt; hello, you, and, me token filter：lowercase，stop word，synonymom，dogs –&gt; dog，liked –&gt; like，Tom –&gt; tom，a/the/an –&gt; 干掉，mother –&gt; mom，small –&gt; little 分词器很重要，它将一段文本进行各种处理，最后处理好的结果才会拿去建立倒排索引。 内置分词器介绍Set the shape to semi-transparent by calling set_trans(5) standard analyzer：set, the, shape, to, semi, transparent, by, calling, set_trans, 5（默认的是standard） simple analyzer：set, the, shape, to, semi, transparent, by, calling, set, trans whitespace analyzer：Set, the, shape, to, semi-transparent, by, calling, set_trans(5) language analyzer（特定的语言的分词器，比如说，english，英语分词器）：set, shape, semi, transpar, call, set_tran, 5 query string分词query string必须以和index建立时相同的analyzer进行分词 比如我们有一个document，其中有一个field，包含的value是：hello you and me。 我们要搜索这个document对应的index，搜索文本是hell me，这个搜索文本就是query string。query string，默认情况下，es会使用它对应的field建立倒排索引时相同的分词器去进行分词和normalization，只有这样才能实现正确的搜索。 建立倒排索引的时候，将dogs –&gt; dog。结果搜索的时候用dogs去搜索，那不就搜索不到了吗？所以搜索的时候，那个dogs也必须变成dog才行。才能搜索到。 query string对exact value（精确搜索）和full text（全文搜索）的区别对待 所以不同类型的field，可能有的就是full text，有的就是exact value。 案例详解GET /_search?q=2017 这个查询前面说了，搜索的是_all field，document所有的field都会拼接成一个大字符串，进行分词搜索。 拼接以后的字符串：2017-01-02 my second article this is my second article in this website 11400 12345 doc1 doc2 doc32017 * * *01 * 02 *03 * 所以用2017去搜索，自然会搜索到3个。 那么用GET /_search?q=2017-01-01去搜索呢 2017-01-01这个query string会用跟建立倒排索引一样的分词器去进行分词，所以搜索的条件会被分词成这样： 12320170101 所以还是会搜索到3条。 接下来是GET /_search?q=post_date:2017-01-01，这里加了查询的字段了，这个字段的类型是date，es会用特别的方式进行处理，转换成时间去对这个字段进行搜索。 测试分词器给一段文本和指定分词器进行分词： 12345678910111213141516171819202122232425262728293031GET /_analyze&#123; "analyzer": "standard", "text": "Text to analyze"&#125;&#123; "tokens": [ &#123; "token": "text", "start_offset": 0, "end_offset": 4, "type": "&lt;ALPHANUM&gt;", "position": 0 &#125;, &#123; "token": "to", "start_offset": 5, "end_offset": 7, "type": "&lt;ALPHANUM&gt;", "position": 1 &#125;, &#123; "token": "analyze", "start_offset": 8, "end_offset": 15, "type": "&lt;ALPHANUM&gt;", "position": 2 &#125; ]&#125; mapping详解为了能够将时间域视为时间，数字域视为数字，字符串域视为全文或精确值字符串， Elasticsearch 需要知道每个域中数据的类型。这个信息包含在mapping中。 mapping就是index的type的元数据，每个type都有一个自己的mapping，决定了数据类型，建立倒排索引的行为，还有进行搜索的行为。 往es里面直接插入数据，es会自动建立索引，同时建立type以及对应的mapping。 mapping中定义了每个field的数据类型。 不同的数据类型（比如说text和date），可能有的是exact value，有的是full text。 exact value：在建立倒排索引、分词的时候，是将整个值一起作为一个关键词建立到倒排索引中的；full text：会经历各种各样的处理、分词、normaliztion（时态转换，同义词转换，大小写转换），才会建立到倒排索引中。 同时呢，exact value和full text类型的field就决定了，在一个搜索过来的时候，对exact value field或者是full text field进行搜索的行为也是不一样的，会跟建立倒排索引的行为保持一致；比如说exact value搜索的时候，就是直接按照整个值进行匹配，full text query string，也会进行分词和normalization再去倒排索引中去搜索。 可以用es的dynamic mapping，让其自动建立mapping，包括自动设置数据类型；也可以提前手动创建index和type的mapping，自己对各个field进行设置，包括数据类型，包括索引行为、分词器，等等。 mapping数据类型Elasticsearch支持如下简单域类型： 字符串: string 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date dynamic mapping：当你索引一个包含新域的文档–之前未曾出现– Elasticsearch 会使用 动态映射 ，通过JSON中基本数据类型，尝试猜测域类型，使用如下规则： true or false –&gt; boolean123 –&gt; long123.45 –&gt; double2017-01-01 –&gt; date“hello world” –&gt; string/text 这意味着如果你通过引号( “123” )索引一个数字，它会被映射为 string 类型，而不是 long 。但是，如果这个域已经映射为 long ，那么 Elasticsearch 会尝试将这个字符串转化为 long ，如果无法转化，则抛出一个异常。 查看mapping123456789101112131415161718192021222324252627282930313233343536GET website/_mapping/article&#123; "website": &#123; "mappings": &#123; "article": &#123; "properties": &#123; "author_id": &#123; "type": "long" &#125;, "content": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "post_date": &#123; "type": "date" &#125;, "title": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 自定义mappingmapping在建立以后，只能新增字段，不能修改字段类型。 先创建一个mapping： 1234567891011121314151617181920212223242526PUT /website&#123; "mappings": &#123; "article": &#123; "properties": &#123; "author_id": &#123; "type": "long" &#125;, "title": &#123; "type": "text", "analyzer": "english" &#125;, "content": &#123; "type": "text" &#125;, "post_date": &#123; "type": "date" &#125;, "publisher_id": &#123; "type": "text", "index": "not_analyzed" &#125; &#125; &#125; &#125;&#125; 试着修改mapping，比如author_id，会得到一个错误： 12345678910111213141516171819202122232425262728293031PUT /website&#123; "mappings": &#123; "article": &#123; "properties": &#123; "author_id": &#123; "type": "text" &#125; &#125; &#125; &#125;&#125;&#123; "error": &#123; "root_cause": [ &#123; "type": "index_already_exists_exception", "reason": "index [website/8KNSiw4wRq-67EVN20ll3A] already exists", "index_uuid": "8KNSiw4wRq-67EVN20ll3A", "index": "website" &#125; ], "type": "index_already_exists_exception", "reason": "index [website/8KNSiw4wRq-67EVN20ll3A] already exists", "index_uuid": "8KNSiw4wRq-67EVN20ll3A", "index": "website" &#125;, "status": 400&#125; 但是如果是给已经存在的mapping新增一个field，就没问题了： 12345678910111213PUT /website/_mapping/article&#123; "properties" : &#123; "new_field" : &#123; "type" : "string", "index": "not_analyzed" &#125; &#125;&#125;&#123; "acknowledged": true&#125; 测试mapping你可以使用 analyze API 测试字符串域的映射，下面测试某一个mapping的字段分词情况： 12345GET /website/_analyze&#123; "field": "content", "text": "my-dogs" &#125; mapping复杂数据类型除了我们提到的简单标量数据类型， JSON 还有 null 值，数组，和对象，这些 Elasticsearch 都是支持的 multivalue field（多值域）很有可能，我们希望 tag 域 包含多个标签。我们可以以数组的形式索引标签： { &quot;tag&quot;: [ &quot;search&quot;, &quot;nosql&quot; ]} 对于数组，没有特殊的映射需求。任何域都可以包含0、1或者多个值，就像全文域分析得到多个词条。 这暗示 数组中所有的值必须是相同数据类型的 。你不能将日期和字符串混在一起。如果你通过索引数组来创建新的域，Elasticsearch 会用数组中第一个值的数据类型作为这个域的 类型。 empty field（空域）当然，数组可以为空。 这相当于存在零值。 事实上，在 Lucene 中是不能存储 null 值的，所以我们认为存在 null 值的域为空域。 下面三种域被认为是空的，它们将不会被索引： 123"null_value": null,"empty_array": [],"array_with_null_value": [ null ] object field（多层级对象）我们讨论的最后一个 JSON 原生数据类是 对象 – 在其他语言中称为哈希，哈希 map，字典或者关联数组。 内部对象 经常用于 嵌入一个实体或对象到其它对象中。例如，与其在 tweet 文档中包含 user_name 和 user_id 域，我们也可以这样写： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192PUT /tweet/tweet/1&#123; "tweet": "Elasticsearch is very flexible", "user": &#123; "id": "@johnsmith", "gender": "male", "age": 26, "name": &#123; "full": "John Smith", "first": "John", "last": "Smith" &#125; &#125;&#125;GET /tweet/tweet/_mapping&#123; "tweet": &#123; "mappings": &#123; "tweet": &#123; "properties": &#123; "tweet": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "user": &#123; "properties": &#123; "age": &#123; "type": "long" &#125;, "gender": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "id": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "name": &#123; "properties": &#123; "first": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "full": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "last": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 内部对象如何索引Lucene 不理解内部对象。 Lucene 文档是由一组键值对列表组成的。为了能让 Elasticsearch 有效地索引内部类，它把我们的文档转化成这样： 123456789&#123; "tweet": [elasticsearch, flexible, very], "user.id": [@johnsmith], "user.gender": [male], "user.age": [26], "user.name.full": [john, smith], "user.name.first": [john], "user.name.last": [smith]&#125;]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch04-Document、Index详解和操作以及并发问题]]></title>
    <url>%2F2018%2F08%2F19%2FelasticSearch04%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 Document元数据_index元数据 代表一个document存放在哪个index中。 类似的数据放在一个索引，非类似的数据放不同索引：product index（包含了所有的商品），sales index（包含了所有的商品销售数据），inventory index（包含了所有库存相关的数据）。如果你把比如product，sales，human resource（employee），全都放在一个大的index里面，比如company index，就不合适。 index中包含了很多类似的document：类似是什么意思，其实指的就是这些document的fields很大一部分是相同的，比如你放了3个document，每个document的fields都完全不一样，这就不是类似了，就不太适合放到一个index里面去了。 索引名称必须是小写的，不能用下划线开头，不能包含逗号，例如：product，website，blog _type元数据 代表document属于index中的哪个类别（type） 一个索引通常会划分为多个type，逻辑上对index中有些许不同的几类数据进行分类：因为一批相同的数据，可能有很多相同的fields，但是还是可能会有一些轻微的不同，可能会有少数fields是不一样的，举个例子，比如商品，可能划分为电子商品，生鲜商品，日化商品，等等。 type名称可以是大写或者小写，但是同时不能用下划线开头，不能包含逗号 _id元数据 代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document 我们可以手动指定document的id（put /index/type/id），也可以不指定，由es自动为我们创建一个id 手动指定document id根据应用情况看是否满足手动指定document id的前提一般来说，是从某些其他的系统中导入一些数据到es时，会采取这种方式，就是使用系统中已有数据的唯一标识，作为es中document的id。 举个例子，我们现在在开发一个电商网站，做搜索功能，或者是OA系统的做员工检索功能。这个时候，数据首先会在网站系统或者IT系统内部的数据库中，会先有一份，此时就肯定会有一个数据库的primary key（自增长，UUID，或者是业务编号）。如果将数据导入到es中，此时就比较适合采用数据在数据库中已有的primary key。 但是如果是在做一个系统，这个系统主要的数据存储就是es，也就是数据产生出来以后，可能就没有id，直接就存es。那么这个时候，可能就不太适合手动指定document id的形式了，因为你也不知道id应该是什么，此时可以采取下面的让es自动生成id的方式。 put /index/type/id手动指定ID的方式 1234PUT /test_index/test_type/2&#123; "test_content": "my test"&#125; 自动生成document idpost /index/type后面不加ID，ES会为我们自动生成ID 1234POST /test_index/test_type&#123; "test_content": "my test"&#125; output: 12345678910111213&#123; "_index": "test_index", "_type": "test_type", "_id": "AVp4RN0bhjxldOOnBxaE", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125; GUID自动生成的id，它的长度为20个字符，是URL安全的，基于base64编码。基于GUID的算法，分布式系统并行生成时不可能会发生冲突。 _source元数据12345678910111213141516171819PUT /test_index/test_type/1&#123; "test_field1": "test field1", "test_field2": "test field2"&#125;GET /test_index/test_type/1&#123; "_index": "test_index", "_type": "test_type", "_id": "1", "_version": 2, "found": true, "_source": &#123; "test_field1": "test field1", "test_field2": "test field2" &#125;&#125; _source元数据：在创建一个document的时候，传入的json传在默认情况下，在get的时候，会原封不动的给我们返回回来。 如果要定制返回的结果，可以加_srouce参数，指定返回哪些字段，多个字段用逗号分隔。 123456789101112GET /test_index/test_type/1?_source=test_field1,test_field2&#123; "_index": "test_index", "_type": "test_type", "_id": "1", "_version": 2, "found": true, "_source": &#123; "test_field2": "test field2" &#125;&#125; Document的创建、替换和删除document的全量替换 语法与创建文档是一样的，如果document id不存在，那么就是创建；如果document id已经存在，那么就是全量替换操作，替换document的json串内容。 其实document是不可变的，如果要修改document的内容，第一种方式就是全量替换，直接对document重新建立索引，替换里面所有的内容。 es会将老的document标记为deleted，然后新增我们给定的一个document，当我们创建越来越多的document的时候，es会在适当的时机在后台自动删除标记为deleted的document。 document的强制创建创建文档与全量替换的语法是一样的，有时我们只是想新建文档，不想替换文档，如果强制进行创建呢？ PUT /index/type/id?op_type=create，PUT /index/type/id/_create 但是强制创建一个已经存在的Document会得到一个冲突的错误。 12345678910111213141516171819&#123; "error": &#123; "root_cause": [ &#123; "type": "version_conflict_engine_exception", "reason": "[test_type][1]: version conflict, document already exists (current version [1])", "index_uuid": "arBg_MfmRWCMSKQHqGIrDw", "shard": "3", "index": "test_index1" &#125; ], "type": "version_conflict_engine_exception", "reason": "[test_type][1]: version conflict, document already exists (current version [1])", "index_uuid": "arBg_MfmRWCMSKQHqGIrDw", "shard": "3", "index": "test_index1" &#125;, "status": 409&#125; document的删除DELETE /index/type/id ES不会立即物理删除，只会将其标记为deleted，当数据越来越多的时候，在后台自动删除。 Elasticsearch并发冲突问题多个线程去同时访问es中的一份数据，然后各自去修改之后更新到es，由于线程的先后顺序不同，可能会导致后续的修改覆盖掉之前的修改，显然一些场景下我们是不允许发生这种并发冲突的问题，例如电商库存的修改等 悲观锁和乐观锁并发控制方案数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。 乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。不要把他们和数据中提供的锁机制（行锁、表锁、排他锁、共享锁）混为一谈。其实，在DBMS中，悲观锁正是利用数据库本身提供的锁机制来实现的。 悲观锁 如何理解悲观锁 它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度(悲观)，因此，在整个数据处理过程中，将数据处于锁定状态。 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 优点与不足 悲观锁的优点：方便，直接加锁，对应用程序来说透明，不需要额外的操作； 悲观锁的缺点：并发能力很低，同一时间只能有一条线程操作数据。 乐观锁 如何理解乐观锁 它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。 与悲观锁区别 相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 数据版本是为数据增加的一个版本标识。当读取数据时，将版本标识的值一同读出，数据每更新一次，同时对版本标识进行更新。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的版本标识进行比对，如果数据库表当前版本号与第一次取出来的版本标识值相等，则予以更新，否则认为是过期数据。 实现数据版本有两种方式，第一种是使用版本号，第二种是使用时间戳。 优点与不足 乐观锁的优点：并发能力很高，不给数据加锁，可以进行大量线程并发操作； 乐观锁的缺点：麻烦，每次更新的时候都要先比对版本号，然后可能需要重新加载数据，再次修改，在写；这个过程可能要重复好几次。 基于_version字段进行乐观锁并发控制_version元数据123456789101112131415161718PUT /test_index/test_type/6&#123; "test_field": "test test"&#125;&#123; "_index": "test_index", "_type": "test_type", "_id": "6", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125; 第一次创建一个document的时候，它的_version内部版本号就是1；以后，每次对这个document执行修改或者删除操作，都会对这个_version版本号自动加1；哪怕是删除，也会对这条数据的版本号加1 12345678910111213&#123; "found": true, "_index": "test_index", "_type": "test_type", "_id": "6", "_version": 4, "result": "deleted", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;&#125; 我们会发现，在删除一个document之后，可以从一个侧面证明，它不是立即物理删除掉的，因为它的一些版本号等信息还是保留着的。先删除一条document，再重新创建这条document，其实会在delete version基础之上，再把version号加1。 ES内部很多类似于副本集的同步请求，都是多线程异步的，也就意味着多个修改请求之间是乱序的，所以ES内部也是采用了乐观锁的方案，基于version版本号去进行并发控制。 并发控制方案上机动手实战演练基于_version进行乐观锁并发控制1、先模拟一条数据 1234PUT /test_index/test_type/7&#123; "test_field": "test test"&#125; 2、模拟两个客户端，都获取到了同一条数据（开2个kibana的网页） 123456789101112GET test_index/test_type/7&#123; "_index": "test_index", "_type": "test_type", "_id": "7", "_version": 1, "found": true, "_source": &#123; "test_field": "test test" &#125;&#125; 3、其中一个客户端先更新了数据 更新时带上了数据的版本号，确保ES中数据的版本号跟客户端的版本号是相同的才能修改。 123456789101112131415161718PUT /test_index/test_type/7?version=1 &#123; "test_field": "test client 1"&#125;&#123; "_index": "test_index", "_type": "test_type", "_id": "7", "_version": 2, "result": "updated", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": false&#125; 4、另外一个客户端尝试基于version=1的数据去进行修改，也带上version版本号，进行乐观锁的并发控制 123456789101112131415161718192021222324PUT /test_index/test_type/7?version=1 &#123; "test_field": "test client 2"&#125;&#123; "error": &#123; "root_cause": [ &#123; "type": "version_conflict_engine_exception", "reason": "[test_type][7]: version conflict, current version [2] is different than the one provided [1]", "index_uuid": "I8nYYk8URXmXpcx0SS7wyw", "shard": "3", "index": "test_index" &#125; ], "type": "version_conflict_engine_exception", "reason": "[test_type][7]: version conflict, current version [2] is different than the one provided [1]", "index_uuid": "I8nYYk8URXmXpcx0SS7wyw", "shard": "3", "index": "test_index" &#125;, "status": 409&#125; 版本冲突，更新失败。 5、在乐观锁成功阻止并发问题之后，尝试正确的完成更新 123456789101112GET /test_index/test_type/7&#123; "_index": "test_index", "_type": "test_type", "_id": "7", "_version": 2, "found": true, "_source": &#123; "test_field": "test client 1" &#125;&#125; 首先去查询ES里当前数据的版本号，然后带上最新的版本号去修改数据，可能这个步骤会需要反复执行好几次才能成功，特别是在多线程并发更新同一条数据很频繁的情况下。 123456789101112131415161718PUT /test_index/test_type/7?version=2 &#123; "test_field": "test client 2"&#125;&#123; "_index": "test_index", "_type": "test_type", "_id": "7", "_version": 3, "result": "updated", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": false&#125; 上机动手实战演练基于external version进行乐观锁并发控制ES提供了一个功能可以让我们不用它提供的内部_version版本号来进行并发控制，我们可以基于自己维护的一个本版好类进行并发控制。举个例子，假如你的数据在MYSQL里也有一份，然后在MYSQL里维护了一个版本号，无论是怎么生成的，这个时候进行乐观锁并发控制，可能并不是想要用es内部的_version来进行控制，而是用自己维护的那个version来进行控制。 12?version=1?version=1&amp;version_type=external 区别：只有当你提供的version与es中的_version一样的时候才能修改，否则就报错；当version_type=external的时候，只要你提供的version比es中的_version大，就能完成修改。 1、先构造一条数据 123456789101112131415161718PUT /test_index/test_type/8&#123; "test_field": "test"&#125;&#123; "_index": "test_index", "_type": "test_type", "_id": "8", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125; 2、模拟两个客户端同时查询到这条数据 123456789101112GET /test_index/test_type/8&#123; "_index": "test_index", "_type": "test_type", "_id": "8", "_version": 1, "found": true, "_source": &#123; "test_field": "test" &#125;&#125; 3、第一个客户端先进行修改，此时客户端在自己的数据库中获取到了这条数据的最新版本号，比如说是3 123456789101112131415161718PUT /test_index/test_type/8?version=3&amp;version_type=external&#123; "test_field": "test client 1"&#125;&#123; "_index": "test_index", "_type": "test_type", "_id": "8", "_version": 3, "result": "updated", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": false&#125; 4、模拟第二个客户端，同时拿到了自己数据库中维护的那个版本号，也是3，同时基于version=3发起了修改 123456789101112131415161718192021222324PUT /test_index/test_type/8?version=3&amp;version_type=external&#123; "test_field": "test client 2"&#125;&#123; "error": &#123; "root_cause": [ &#123; "type": "version_conflict_engine_exception", "reason": "[test_type][8]: version conflict, current version [3] is higher or equal to the one provided [3]", "index_uuid": "I8nYYk8URXmXpcx0SS7wyw", "shard": "1", "index": "test_index" &#125; ], "type": "version_conflict_engine_exception", "reason": "[test_type][8]: version conflict, current version [3] is higher or equal to the one provided [3]", "index_uuid": "I8nYYk8URXmXpcx0SS7wyw", "shard": "1", "index": "test_index" &#125;, "status": 409&#125; 一样的，也是并发冲突，只不过这次是基于我们自己提供的version来控制的，而且报错的提示是必须大于等于3。 5、在并发冲突以后，重新基于新的版本号发起更新 123456789101112131415161718PUT /test_index/test_type/8?version=6&amp;version_type=external&#123; "test_field": "test client 2"&#125;&#123; "_index": "test_index", "_type": "test_type", "_id": "8", "_version": 6, "result": "updated", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": false&#125; partial updatepartial update是修改文档的另一种方式 之前的PUT操作对应到应用程序中，每次的执行流程是这样的： 应用程序先发起一个get请求，获取到document，展示到前台界面，供用户查看和修改 用户在前台界面修改数据，发送到后台 后台代码，会将用户修改的数据在内存中进行执行，然后封装好修改后的全量数据 然后发送PUT请求，到es中，进行全量替换 es将老的document标记为deleted，然后重新创建一个新的document partial update post /index/type/id/_update{ “doc”: { “要修改的少数几个field即可，不需要全量的数据” }} 看起来，好像就比较方便了，每次就传递少数几个发生修改的field即可，不需要将全量的document数据发送过去 partial update实现原理以及其优点partial update直接将数据更新到document中就完成了修改，不用事先先发起一个GET请求数据进行修改然后在将修改后的数据发回去。 es内部：partial update的执行和全量替换一致。 内部先get获取document 将更新的field更新到document的json中 将老的document标记为deleted 创建新的document 优点： 所有查询，修改和写回操作均发生在同一个shard内，避免了不必要的网络数据传输带来的开销，大大提升了性能（减少了两次请求，一次GET请求，一次回写请求） 减少修改和查询中的时间间隔，有效减少并发冲突的情况 内置乐观锁并发控制 123456POST /test_index/test_type/id/_update?retry_on_conflict=2&#123; "doc": &#123; "num":32 &#125;&#125; 如果更新失败，则获取最新的版本号再次进行更新，最多重试retry_on_conflict指定的次数 123456POST /test_index/test_type/11/_update?version=3&#123; "doc": &#123; "num":32 &#125;&#125; 实验123456789101112PUT /test_index/test_type/10&#123; "test_field1": "test1", "test_field2": "test2"&#125;POST /test_index/test_type/10/_update&#123; "doc": &#123; "test_field2": "updated test2" &#125;&#125; 批量操作批量查询如果一条一条的查询100条数据，那么就要发送100次网络请求，这个开销还是很大的如果进行批量查询的话，查询100条数据，就只要发送1次网络请求，网络请求的性能开销缩减100倍。 ES提供了批量查询的API，它的mget的语法： 123456789101112131415161718192021222324252627282930313233GET /_mget&#123; "docs" : [ &#123; "_index" : "test_index", "_type" : "test_type", "_id" : 1 &#125;, &#123; "_index" : "test_index", "_type" : "test_type", "_id" : 2 &#125; ]&#125;&#123; "docs": [ &#123; "_index": "test_index", "_type": "test_type", "_id": "1", "found": false &#125;, &#123; "_index": "test_index", "_type": "test_type", "_id": "2", "found": false &#125; ]&#125; 1、如果查询的document是一个index下的不同type种的话 12345678910111213GET /test_index/_mget&#123; "docs" : [ &#123; "_type" : "test_type", "_id" : 1 &#125;, &#123; "_type" : "test_type", "_id" : 2 &#125; ]&#125; 2、如果查询的数据都在同一个index下的同一个type下，最简单了 1234GET /test_index/test_type/_mget&#123; "ids": [1, 2]&#125; 批量增删改ES对于批量增删改，是提供的bulk api。 12345678POST /_bulk&#123; "delete": &#123; "_index": "test_index", "_type": "test_type", "_id": "3" &#125;&#125; &#123; "create": &#123; "_index": "test_index", "_type": "test_type", "_id": "12" &#125;&#125;&#123; "test_field": "test12" &#125;&#123; "index": &#123; "_index": "test_index", "_type": "test_type", "_id": "2" &#125;&#125;&#123; "test_field": "replaced test2" &#125;&#123; "update": &#123; "_index": "test_index", "_type": "test_type", "_id": "1", "_retry_on_conflict" : 3&#125; &#125;&#123; "doc" : &#123;"test_field2" : "bulk test1"&#125; &#125; 上面是一些例子，它的语法是每一个操作有两个json，语法如下： 12&#123;"action": &#123;"metadata"&#125;&#125;&#123;"data"&#125; 举例，比如你现在要创建一个文档，放bulk里面，看起来会是这样子的： 12&#123;"index": &#123;"_index": "test_index", "_type", "test_type", "_id": "1"&#125;&#125;&#123;"test_field1": "test1", "test_field2": "test2"&#125; bulk api提供了以下4种操作： delete：删除一个文档，只要1个json串就可以了 create：PUT /index/type/id/_create，强制创建 index：普通的put操作，可以是创建文档，也可以是全量替换文档 update：执行的partial update操作 bulk api对json的语法，有严格的要求，每个json串不能换行，只能放一行，同时一个json串和一个json串之间，必须有一个换行。 bulk操作中，任意一个操作失败，是不会影响其他的操作的，但是在返回结果里，会告诉你异常日志 12345678910POST /test_index/_bulk&#123; "delete": &#123; "_type": "test_type", "_id": "3" &#125;&#125; &#123; "create": &#123; "_type": "test_type", "_id": "12" &#125;&#125;&#123; "test_field": "test12" &#125;&#123; "index": &#123; "_type": "test_type" &#125;&#125;&#123; "test_field": "auto-generate id test" &#125;&#123; "index": &#123; "_type": "test_type", "_id": "2" &#125;&#125;&#123; "test_field": "replaced test2" &#125;&#123; "update": &#123; "_type": "test_type", "_id": "1", "_retry_on_conflict" : 3&#125; &#125;&#123; "doc" : &#123;"test_field2" : "bulk test1"&#125; &#125; 12345678910POST /test_index/test_type/_bulk&#123; "delete": &#123; "_id": "3" &#125;&#125; &#123; "create": &#123; "_id": "12" &#125;&#125;&#123; "test_field": "test12" &#125;&#123; "index": &#123; &#125;&#125;&#123; "test_field": "auto-generate id test" &#125;&#123; "index": &#123; "_id": "2" &#125;&#125;&#123; "test_field": "replaced test2" &#125;&#123; "update": &#123; "_id": "1", "_retry_on_conflict" : 3&#125; &#125;&#123; "doc" : &#123;"test_field2" : "bulk test1"&#125; &#125; bulk size最佳大小bulk request会加载到内存里，如果太大的话，性能反而会下降，因此需要反复尝试一个最佳的bulk size。一般从1000~5000条数据开始，尝试逐渐增加。另外，如果看大小的话，最好是在5~15MB之间。 Document数据路由在ES中，一个index的数据会分散在多个分片(shard)中，所以当客户端创建Document的时候，需要决定这个Document放在ES的哪一个shard上，这个过程被称之为数据路由。 路由算法shard = hash(routing) % number_of_primary_shards 举个例子，一个index有3个primary shard，P0，P1，P2。每次增删改查一个document的时候，都会带过来一个routing number，默认就是这个document的_id（可能是手动指定，也可能是自动生成）。所以routing = _id，假设_id=1，会将这个routing值，传入一个hash函数中，产出一个routing值的hash值，hash(routing) = 21。然后将hash函数产出的值对这个index的primary shard的数量求余数，21 % 3 = 0就决定了，这个document就放在P0上。决定一个document在哪个shard上，最重要的一个值就是routing值，默认是_id，也可以手动指定，相同的routing值，每次过来，从hash函数中，产出的hash值一定是相同的。无论hash值是几，无论是什么数字，对number_of_primary_shards求余数，结果一定是在0~number_of_primary_shards-1之间这个范围内的，这里是0,1,2。 _id还是自定义routing值默认的routing就是_id也可以在发送请求的时候，手动指定一个routing值，比如put /index/type/id?routing=user_id 手动指定routing value是很有用的，可以保证某一类document一定被路由到一个shard上去，那么在后续进行应用级别的负载均衡，以及提升批量读取的性能的时候，是很有帮助的。 比如在实际的工作当中，如果大量的查询是基于某一个字段的查询，那么可以在添加数据的时候设置这个字段的ID为routing值，比如用户ID，这样在做查询和聚合的时候，ES只需要去一个shard里就能找到所有的数据，提升性能。 primary shard数量不可变的谜底ES在创建index的时候设置了primary shard数量和replica shard数量，replica数量是可以修改的，但是primary shard的数量却不能修改。正是因为跟Document的路由公式有关，所以如果primary shard数量发生了变化，如果后面根据ID去查询一个数据，新的路由算法去计算分配，会发现根本找不到这个数据，间接导致数据丢失。 Document增删改的内部原理客户端先选择一个节点发送请求，在一般的ES部署架构中，会有一个client节点，专门用来接收客户端的请求，它既不保存元数据，也不保存数据，只是协调请求转发和数据的聚合，分担master节点的压力。 节点对请求进行路由，将请求转发到路由以后的节点上，然后primary shard会在自己本地创建Document，建立索引，最后把响应结果返回给client。 所有的增删改操作，都只能由primary shard处理。 写一致性原理我们在发送任何一个增删改操作的时候，比如说put /index/type/id，都可以带上一个consistency参数，指明我们想要的写一致性是什么？put /index/type/id?consistency=quorum one：要求我们这个写操作，只要有一个primary shard是active活跃可用的，就可以执行 all：要求我们这个写操作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作 quorum：默认的值，要求所有的shard中，必须是大部分的shard都是活跃的，可用的，才可以执行这个写操作 quorum机制写之前必须确保大多数shard都可用，那么大多数是多少？这里有一个算法： quroum=int( (primary + number_of_replicas) / 2 ) + 1，当number_of_replicas&gt;1时才生效 举个例子，3个primary shard，number_of_replicas=1，总共有3 + 3 * 1 = 6个shard。 quorum = int( (3 + 1) / 2 ) + 1 = 3 所以，要求6个shard中至少有3个shard是active状态的，才可以执行这个写操作。 如果节点数量少于quorum数量，可能导致quorum不齐全，进而导致无法执行任何写操作。 比如3个primary shard，replica=1，要求至少3个shard是active，3个shard按照之前学习的shard&amp;replica机制，必须在不同的节点上(primary shard和replica shard不能放在一台机器上，同一个primary shard的replica shard也不能放在同一个机器上)，如果说只有1台机器的话，3个shard肯定都没法分配齐全，此时就可能会出现写操作无法执行的情况。 但是ES提供了一种特殊的处理场景，就是说当number_of_replicas&gt;1时才生效，因为假如说，你就一个primary shard，replica=1，此时就2个shard，套用公式算一下。 (1 + 1 / 2) + 1 = 2，要求必须有2个shard是活跃的，但是可能就1个node，此时就1个shard是活跃的，如果你不特殊处理的话，导致我们的单节点集群就无法工作。 quorum不齐全的时候，ES会等待，默认1分钟。等待期间，期望活跃的shard数量可以增加，最后实在不行，就会timeout。 我们其实可以在写操作的时候，加一个timeout参数，比如说put /index/type/id?timeout=30，这个就是自己去设定quorum不齐全的时候，es的timeout时长，可以缩短，也可以增长。 ES查询原理对于读请求，coordinate node（协作节点）不一定会将请求转发到primary节点上去，因为replica也是可以服务读请求的，而且在转发的时候会采用轮询的负载均衡算法，让读请求均匀的转发到replica shard上。 如果Document正在建立索引的过程中，只在primary shard上存在，此时replica shard上没有，但是协调节点可能将请求转发到replica shard上，此时就会找不到这个Document。 bulk api原理上面在学bulk api的时候，ES对json格式要求非常严格，格式紧凑，对换行也有要求。 1、bulk中的每个操作都可能要转发到不同的node的shard去执行 2、如果采用比较良好的json数组格式 允许任意的换行，整个可读性非常棒，读起来很爽，es拿到那种标准格式的json串以后，要按照下述流程去进行处理 将json数组解析为JSONArray对象，这个时候，整个数据，就会在内存中出现一份一模一样的拷贝，一份数据是json文本，一份数据是JSONArray对象 解析json数组里的每个json，对每个请求中的document进行路由 为路由到同一个shard上的多个请求，创建一个请求数组 将这个请求数组序列化 将序列化后的请求数组发送到对应的节点上去 3、耗费更多内存，更多的jvm gc开销 我们之前提到过bulk size最佳大小的那个问题，一般建议说在几千条，然后大小在10MB左右，所以可怕的事情来了。假设说现在100个bulk请求发送到了一个节点上去，然后每个请求是10MB，100个请求，就是1000MB = 1GB，然后每个请求的json都copy一份为jsonarray对象，此时内存中的占用就会翻倍，就会占用2GB的内存，甚至还不止。因为弄成jsonarray之后，还可能会多搞一些其他的数据结构，2GB+的内存占用。 占用更多的内存可能就会积压其他请求的内存使用量，比如说最重要的搜索请求，分析请求，等等，此时就可能会导致其他请求的性能急速下降。另外的话，占用内存更多，就会导致java虚拟机的垃圾回收次数更多，跟频繁，每次要回收的垃圾对象更多，耗费的时间更多，导致es的java虚拟机停止工作线程的时间更多。 4、现在的奇特格式 {“action”: {“meta”}}\n{“data”}\n{“action”: {“meta”}}\n{“data”}\n 不用将其转换为json对象，不会出现内存中的相同数据的拷贝，直接按照换行符切割json 对每两个一组的json，读取meta，进行document路由 直接将对应的json发送到node上去 5、最大的优势在于，不需要将json数组解析为一个JSONArray对象，形成一份大数据的拷贝，浪费内存空间，尽可能地保证性能]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解JVM01-类加载]]></title>
    <url>%2F2018%2F08%2F16%2Funderstandjvm-1%2F</url>
    <content type="text"><![CDATA[类加载在Java代码中，类型的加载、连接与初始化过程都是在程序运行期间完成的。 类加载常见的行为是将磁盘上的class文件加载到内存中 连接将是类与类之间的关系处理好 初始化对一些静态的变量进行赋值 这提供了更大的灵活性，增加了更多的可能性。 类加载深入剖析加载类的工具，叫做类加载器 Java虚拟机的生命周期在如下几种情况，Java虚拟机将结束生命周期 执行了System.exit()方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致Java虚拟机进程终止 类的加载、连续、与初始化 加载：查找并加载类的二进制数据 连接 验证：确保被加载的类的正确性 准备：为类的静态变量分配内存，并将其初始化为默认值 解析：把类中的符号引用转换为直接引用 初始化：为类的静态变量赋予正确的初始值。 类的使用和卸载]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effective Java 第三版-条款1_考虑使用静态工厂方法而非构造方法]]></title>
    <url>%2F2018%2F08%2F13%2Feffective-java-3rd-1%2F</url>
    <content type="text"><![CDATA[先尝试自己翻译，再和龙哥的译文进行对比。 条款1: 考虑使用静态工厂方法而非构造方法传统方式允许客户端获取一个类的实例，是提供一个公有的构造方法。有另外一种技术获取实例，它应该在每个程序员的工具箱里。类可以提供一个公有的静态工厂方法，就是一个简单的静态方法，它会返回这个类的实例。这里有一个来自Boolean(原生类型boolean的包装类)类的简单例子，这个方法将一个原生类型的布尔值转换成一个Boolean对象引用： 对于一个类来说，若想让客户端能够获取到其实例，传统⽅式是提供⼀个公有的构造⽅法。 实际上，还有另外一种技术应该成为每个程序员⼯具箱的一部分。类可以提供⼀个公有的静态⼯厂方法，它仅仅是一个可以返回类实例的静态⽅法⽽已。如下这个简单的示例来⾃于 Boolean(即boolean装箱后的类型)。该方法会将⼀个boolean原⽣值转换为⼀一个Boolean对象引用: The traditional way for a class to allow a client to obtain an instance is to provide a public constructor. There is another technique that should be a part of every programmer’s toolkit. A class can provide a public static factory method, which is simply a static method that returns an instance of the class. Here’s a simple example from Boolean (the boxed primitive class for boolean). This method translates a boolean primitive value into a Boolean object reference: 123public static Boolean valueOf(boolean b) &#123; return b ? Boolean.TRUE : Boolean.FALSE;&#125; 注意静态工厂方法和设计模式[Gamma95]里的工厂方法模式并不一样，这里描述的静态工厂方法跟设计模式没有关联。 请注意，上面这个静态⼯厂⽅法与设计模式[Gamma95]中的⼯厂方法模式并不一样。本条款所介绍的静态⼯厂⽅法在设计模式一书中并没有直接的等价物。 Note that a static factory method is not the same as the Factory Method pattern from Design Patterns [Gamma95]. The static factory method described in this item has no direct equivalent in Design Patterns. 一个类可以提供静态工厂方法给它的客户端以替代构造方法。用静态工厂方法替代构造方法既有好处，也有坏处。 除了了公有构造方法外，类还可以向其客户端提供静态⼯厂方法。相⽐于公有构造方法来说，提供静态⼯厂⽅法有利也有弊。 A class can provide its clients with static factory methods instead of, or in addition to, public constructors. Providing a static factory method instead of a public constructor has both advantages and disadvantages. 静态工厂方法的一个好处是不像构造方法，它们有方法名字。如果是构造方法，它的参数并不能描述构造方法返回的实例，但是有一个好名字的静态工厂方法更好理解，也让客户端代码更有易读性。举个例子，构造方法BigInteger(int, int, Random)返回一个可能为质数的BigInteger，用静态工厂方法BigInteger.probablePrime(此方法在 Java 4中加入)可能会更好表达它的意思。 静态⼯厂⽅法的一个好处在于，相⽐于构造方法来说，他们拥有名字。如果构造方法的参数本身没有描述出将要返回的对象，那么拥有恰当名字的静态⼯厂将会更加易于使用，所生成的客户端代码的可读性也更好。比如说，构造方法BigInteger(int, int, Random)会返回⼀一个可能为质数的BigInteger，不不过使⽤用静态⼯厂方法BigInteger.probablePrime的表述性会更更棒(该⽅法是在Java 4中被加⼊进来的)。 One advantage of static factory methods is that, unlike constructors, they have names. If the parameters to a constructor do not, in and of themselves, describe the object being returned, a static factory with a well-chosen name is easier to use and the resulting client code easier to read. For example, the constructor BigInteger(int, int, Random), which returns a BigInteger that is probably prime, would have been better expressed as a static factory method named BigInteger.probablePrime. (This method was added in Java 4.) 一个类只能有一个相同签名的构造方法。程序员知道如何绕过这个限制，那就是提供2个构造方法，但是参数的顺序不一样。这是很糟糕的主意，用户永远不能记住哪个构造方法是哪个，然后会不小心调用到错误的构造方法。人们在读代码的时候如果不看类的文档也不会知道这些构造方法干了些什么。 一个类只能拥有唯⼀一个具有给定签名的构造⽅法。程序员们已经知道如何绕过这个限制了， 那就是提供两个构造方法，这两个构造方法之间唯一的差别就是参数列列表中参数类型的顺序是不同的。这是一个⾮常差劲的想法。这种API的使用者永远都记不住哪个构造⽅法是哪个，最终陷⼊到调用了错误的构造⽅法的窘境。当⽤户阅读了使用这种构造⽅法的代码时，他们在不查阅类文档的情况下是不可能搞清楚代码到底在做什么事情。 A class can have only a single constructor with a given signature. Program- mers have been known to get around this restriction by providing two constructors whose parameter lists differ only in the order of their parameter types. This is a really bad idea. The user of such an API will never be able to remember which constructor is which and will end up calling the wrong one by mistake. People reading code that uses these constructors will not know what the code does without referring to the class documentation. 因为它们有名字，静态工厂方法没有前面提到的限制。如果一个类需要多个相同签名的构造方法，用静态工厂方法替代构造方法，并小心地给它们取名字以标记它们的不同之处。 由于拥有名字，因此静态⼯厂方法不会遇到上面所讨论的限制。当⼀个类需要多个拥有相同签名的构造方法时，只需使⽤静态⼯厂⽅法来代替构造方法，并精心选择好名字来明确他们之间的差别即可。 Because they have names, static factory methods don’t share the restriction discussed in the previous paragraph. In cases where a class seems to require multiple constructors with the same signature, replace the constructors with static factory methods and carefully chosen names to highlight their differences. 静态工厂方法的第二个好处是不像构造方法，它们不需要每次被调用的时候都创建一个新的对象。这允许不可变对象（条款17）用一个预设好的实例，或者缓存一个已经创建好的实例，这样可以反复的分发它们来避免创建不必要的重复对象。Boolean.valueOf(boolean)方法说明了这种技巧：它永远不创建对象。这种技巧跟享元模式比较相似，如果相同的对象经常被请求到，它可以大幅度地提升性能，特别是当创建对象开销很大的时候。 静态⼯厂⽅法的第2个好处在于，相⽐比于构造⽅法来说，他们不必在每次调用时都创建⼀个新的对象。这样就可以让不变类使⽤用预先构造好的实例，或是在构造时将其缓存起来，从⽽避免了创建不必要的重复对象的情况。Boolean.valueOf(boolean)⽅法就使用了这项技术: 它永远不会创建对象。该项技术类似于享元模式。如果经常需要请求同样的对象，那么这种做法将会极大改进性能，特别是在对象创建成本很高的情况下更是如此。 A second advantage of static factory methods is that, unlike constructors, they are not required to create a new object each time they’re invoked. This allows immutable classes (Item 17) to use preconstructed instances, or to cache instances as they’re constructed, and dispense them repeatedly to avoid creating unnecessary duplicate objects. The Boolean.valueOf(boolean) method illustrates this technique: it never creates an object. This technique is similar to the Flyweight pattern [Gamma95]. It can greatly improve performance if equivalent objects are requested often, especially if they are expensive to create. 静态工厂方法返回经常被请求的同一个对象的能力允许类在任何时刻都对这些实例维持严格的控制。能做到这样的类被称为实例受控的类。有很多理由去写实例受控的类，实例受控允许一个类保证它是单例的（条款3）或者不可实例化（条款4）。而且，它能让一个不可变的值类（条款17）保证不会有2个相等的实例存在：有且只有当a == b时候，才会有a.equals(b)。这是享元模式的基础，枚举也提供了这种保证。 静态⼯厂⽅法可以在重复调⽤的情况下返回同一个对象的能力使得类可以在任何时候都能严格控制哪些实例可以存在。采取这种做法的类叫做实例控制。编写实例控制类有几个原因。 借助于实例控制，类可以确保它⾃身是一个单例或是不可实例化的。此外，还可以让不可变的值类确保不会存在两个相等的实例:当且仅当a == b时，a.equals(b)才为true。这是享元模式的基础。枚举类型提供了这种保证。 The ability of static factory methods to return the same object from repeated invocations allows classes to maintain strict control over what instances exist at any time. Classes that do this are said to be instance-controlled. There are several reasons to write instance-controlled classes. Instance control allows a class to guar- antee that it is a singleton (Item 3) or noninstantiable (Item 4). Also, it allows an immutable value class (Item 17) to make the guarantee that no two equal instances exist: a.equals(b) if and only if a == b. This is the basis of the Flyweight pattern [Gamma95]. Enum types (Item 34) provide this guarantee. 静态工厂方法第三个好处是不像构造方法，它们可以返回它返回类型的任何子类的对象。这给你了很大的灵活性去选择返回对象的类型。 静态⼯厂⽅法的第3个好处在于，相比于构造方法来说，他们可以返回所声明的返回类型的任何子类型的对象。这样，我们在选择所返回的对象类型时就拥有了更大的灵活性。 A third advantage of static factory methods is that, unlike constructors, they can return an object of any subtype of their return type. This gives you great flexibility in choosing the class of the returned object. 灵活性的一种应用是一个API可以返回非公有类的对象。用这种方式隐藏类的实现提供了很紧凑的API。这种技术适用于基于接口的框架(interface-based frameworks 条款20)，接口为静态工厂方法提供了自然的返回类型。 这种灵活性的⼀个应⽤用场景就是API能够在无需将类声明为公有的情况下就可以返回对象。 以这种⽅式隐藏实现类使得API变得⾮常紧凑。这项技术也被应⽤用到了基于接口的框架中， 其中接口就为静态⼯厂⽅法提供了了⾃然⽽然的返回类型。One application of this flexibility is that an API can return objects without making their classes public. Hiding implementation classes in this fashion leads to a very compact API. This technique lends itself to interface-based frameworks (Item 20), where interfaces provide natural return types for static factory methods. 在Java 8之前，接口不能有静态方法。按照惯例，Type接口的静态工厂方法会被放进叫做Types的不可实例化的伴生类中(noninstantiable companion class)。例如，Java集合框架对它们自己的接口有45个实用的实现，提供不可变的集合，同步的集合等等。差不多所有的实现都是通过静态工厂方法导出到一个不可实例化的类中（java.util.Collections），所有返回对象的类都是非公有的。 在Java 8之前，接口是不能拥有静态方法的。根据约定，针对名为Type的接口的静态⼯厂⽅法会被放到名为Types的不可实例例化的伴生类当中。⽐如说，Java集合框架有接⼝的45个辅助实现，提供了不可修改的集合、同步集合等等。⼏乎所有这些实现都是通过⼀个不可实例化的类(java.util.Collections)中的静态⼯厂⽅方法公开的。所返回对象的类型都是⾮公有的。 Prior to Java 8, interfaces couldn’t have static methods. By convention, static factory methods for an interface named Type were put in a noninstantiable companion class (Item 4) named Types. For example, the Java Collections Framework has forty-five utility implementations of its interfaces, providing unmodifiable collections, synchronized collections, and the like. Nearly all of these implemen- tations are exported via static factory methods in one noninstantiable class (java.util.Collections). The classes of the returned objects are all nonpublic. 集合框架API比导出45个独立的公共类要小得多，每个类都有一个方便的实现。减少的不仅仅是大量的API，还有概念的权重: 程序员为了使用API必须掌握的概念的数量和难度。程序员知道返回的对象精确地具有其接口指定的API，所以不需要阅读额外的实现类的类文档。此外，用这种静态工厂方法需要客户端通过接口引用返回的对象，而不是通过实现类引用，这通常是很好的实践（条款64）。 集合框架API要⽐它本来的样⼦小很多，它公开了45个独立的公有类，每个类都针对于⼀个便捷的实现。这并不仅仅只是API的数量少了，更为重要的是概念上的数量少了:程序员使用API所需掌握的概念的数量和难度都降低了了。程序员知道所返回的对象是由其接口API所精确描述的，因此⽆需再去阅读实现类的⽂档了。此外，使用这种静态工厂⽅法要求客户端引用接口而非实现类所返回的对象，这通常来说是⼀个很好的实践。 The Collections Framework API is much smaller than it would have been had it exported forty-five separate public classes, one for each convenience implementation. It is not just the bulk of the API that is reduced but the conceptual weight: the number and difficulty of the concepts that programmers must master in order to use the API. The programmer knows that the returned object has precisely the API specified by its interface, so there is no need to read additional class documentation for the implementation class. Furthermore, using such a static factory method requires the client to refer to the returned object by interface rather than implementation class, which is generally good practice (Item 64). 对Java8来说，接口不能包含静态方法的限制已经被移除了，所以没有理由为接口提供一个不可实例化的伴生类。许多这样的类中的公有静态成员变量应该放在接口中。但是，请注意，这些静态方法的一些实现代码还是有必要放在一个单独包级别的私有类中。这是因为Java8要求所有接口的静态成员都是公有的。Java 9允许私有的静态方法，但是静态变量和静态成员类仍然只能是公有的。 Java 8已经取消了接口中不能包含静态⽅法的限制，这样一般来说，我们就没必要再为接⼝提供不可实例化的伴生类了。很多本应该位于这种类中的公有静态成员现在应该放到接⼝自身当中了。不过，值得注意的是，我们还是需要将这些静态⽅法的实现代码放到单独的包级别的私有类中。这是因为Java 8要求接口的所有静态成员都必须是公有的。Java 9允许私有的静态方法，不过静态字段与静态成员类依旧得是公有的。 As of Java 8, the restriction that interfaces cannot contain static methods was eliminated, so there is typically little reason to provide a noninstantiable companion class for an interface. Many public static members that would have been at home in such a class should instead be put in the interface itself. Note, however, that it may still be necessary to put the bulk of the implementation code behind these static methods in a separate package-private class. This is because Java 8 requires all static members of an interface to be public. Java 9 allows private static methods, but static fields and static member classes are still required to be public. 静态工厂的第四个好处是返回对象的类型作为输入参数的函数，它可以随调用的不同而变化。所声明返回类型的任何子类型都是被允许的。返回对象的类型也可以随着版本的变化而变化。 静态⼯厂的第4个好处在于，作为输入参数的函数，返回对象所属的类会随着调⽤用的不同而不同。所声明的返回类型的任何子类型都是允许的。返回对象所属的类也会随着调⽤的不同而不不同。 A fourth advantage of static factories is that the class of the returned object can vary from call to call as a function of the input parameters. Any sub- type of the declared return type is permissible. The class of the returned object can also vary from release to release. EnumSet类（条款36）没有公有的构造方法，只有静态工厂。在OpenJDK的实现里，他们会返回2个子类中的一个实例，这取决于枚举类型的长度：如果它有64以下个元素，那么大多数枚举类型会返回RegularEnumSet的实例，底层是用一个long实现的；如果枚举类型有超过65个元素，那么静态工厂方法会返回JumboEnumSet的实例，底层是用一个long数组实现的。 EnumSet类(条款36)并没有公有构造⽅方法，只有静态工厂。在OpenJDK实现中，他们会返回两个子类的实例，到底返回哪⼀一个则取决于底层枚举类型的⼤小:如果拥有的元素数量小于等于64个(这也是⼤大多数枚举类型的情况)，那么静态⼯厂就会返回⼀个RegularEnumSet实例，其底层是个long类型;如果枚举类型拥有的元素数量⼤于等于65个，那么⼯厂就会返回⼀个JumboEnumSet实例，其底层是个long类型的数组。 The EnumSet class (Item 36) has no public constructors, only static factories. In the OpenJDK implementation, they return an instance of one of two subclasses, depending on the size of the underlying enum type: if it has sixty-four or fewer elements, as most enum types do, the static factories return a RegularEnumSet instance, which is backed by a single long; if the enum type has sixty-five or more elements, the factories return a JumboEnumSet instance, backed by a long array. 这2个实现类对客户端来说是不可见的，如果RegularEnumSet停止对小枚举类型提供高性能的优势，它可以在未来的发布中被移除掉，且不会有什么副作用。同样地，在将来的发布中也可以添加第三种或者第四种EnumSet的实现，如果能带来性能上的提升。客户端也不需要关心从工厂获得的对象类型是什么；他们只需要知道那是EnumSet的某个子类。 这两个实现类对于客户端来说是不可见的。如果RegularEnumSet对于小的枚举类型不再有性能上的优势，那么就可以在未来的版本中将其剔除而不会产⽣生任何副作用。与之类似，如果经过验证能够提供更好的性能，那么未来的版本中就可以增加第3种或是第4种EnumSet实现。客户端既不不知晓，也不不关⼼心他们从⼯厂中所得到的对象的真正类型是什么;他们只关 ⼼所得到的是EnumSet的某个⼦类。 The existence of these two implementation classes is invisible to clients. If RegularEnumSet ceased to offer performance advantages for small enum types, it could be eliminated from a future release with no ill effects. Similarly, a future release could add a third or fourth implementation of EnumSet if it proved beneficial for performance. Clients neither know nor care about the class of the object they get back from the factory; they care only that it is some subclass of EnumSet. 静态工厂的第五个好处是当返回对象的类包含了需要的方法，这个类不需要存在。这样灵活的静态工厂方法是服务提供者框架（service provider frameworks）的基础，就像Java Database Connectivity API (JDBC)。一个服务提供者框架是系统提供一个服务的实现，系统让实现对客户端可用，对客户端实现进行解耦。 静态⼯厂的第5个好处在于，在使用包含了方法的类时，返回对象所属的类不必事先存在。 这种灵活的静态工厂方法构成了服务提供者框架的基础，比如说Java Database Connectivity API(JDBC)。服务提供者框架是这样一种系统，提供者实现了某个服务，系统将其实现公开给客户端，从⽽实现了客户端与实现之间的解耦。 A fifth advantage of static factories is that the class of the returned object need not exist when the class containing the method is written. Such flexible static factory methods form the basis of service provider frameworks, like the Java Database Connectivity API (JDBC). A service provider framework is a system in which providers implement a service, and the system makes the implementations available to clients decoupling the clients from the implementations. 服务提供者框架有三个基本的组件：一个服务接口，代表一个实现；一个提供者注册的API，提供用来注册实现；一个服务访问API，客户端用来获得一个服务的实例。客户端访问API可能会允许客户端指定一个特定的标准来选择一个实现。如果没有这样的标准，API会返回一个默认实现的实例，或者让客户端遍历所有可用的实现。服务访问API是灵活的静态工厂，构成了服务提供者框架的基础。 服务提供者框架存在3个基本组件:服务接口(表示实现)、提供者注册API(提供者通过它来注册实现)以及服务访问API(客户端通过它来获取服务实例例)。客户端可以通过服务访问API来指定标准，从⽽而选择相应的实现。如果不存在这样的标准，那么API就会返回默认实现的实例，或是让客户端遍历所有可用的实现。服务访问API是一种灵活的静态⼯厂，它构成了服务提供者框架的基础。 There are three essential components in a service provider framework: a service interface, which represents an implementation; a provider registration API, which providers use to register implementations; and a service access API, which clients use to obtain instances of the service. The service access API may allow clients to specify criteria for choosing an implementation. In the absence of such criteria, the API returns an instance of a default implementation, or allows the client to cycle through all available implementations. The service access API is the flexible static factory that forms the basis of the service provider framework. 服务提供者框架的第四个可选组件是服务提供者接口，它描述了生产服务接口实例的工厂对象。在没有服务提供者接口的情况下，实现必须以反射的方式实例化（条款65）。在JDBC中，Connection扮演了服务接口，DriverManager.registerDriver是提供者注册API，DriverManager.getConnection是服务访问API，Driver是服务提供者接口。 服务提供者框架第4个可选的组件是服务提供者接口，它描述了了⽤于⽣产服务接口实例的⼯⼚对象。如果服务提供者接口不存在，那么实现就必须要通过反射的方式来实例化(条款 65)。对于JDBC来说，Connection就扮演着服务接⼝的⻆色， DriverManager.registerDriver是提供者注册API，DriverManager.getConnection是服务访问 API，而Driver则是服务提供者接口。 An optional fourth component of a service provider framework is a service provider interface, which describes a factory object that produce instances of the service interface. In the absence of a service provider interface, implementations must be instantiated reflectively (Item 65). In the case of JDBC, Connection plays the part of the service interface, DriverManager.registerDriver is the provider registration API, DriverManager.getConnection is the service access API, and Driver is the service provider interface. 有许多服务提供者框架的变种。例如，服务访问API可以向客户端返回一个比服务提供者供应的更丰富的接口。这是桥接模式。依赖注入框架可以被认为是一个很强的服务提供者，从Java6开始，平台包含了一个强大的服务提供者框架，java.util.ServiceLoader，所以你不需要通常也不应该自己写一个（条款59）。JDBC不用ServiceLoader，因为它更早发布。 服务提供者框架模式有很多变种。⽐如说，服务访问API可以向客户端返回比提供者所规定的更为宽泛的服务接口。这就是桥接模式[Gamma95]。依赖注⼊框架(条款5)可以看作是一种强大的服务提供者。从Java 6开始，平台包含了一个通⽤的服务提供者框架，即 java.util.ServiceLoader。因此，你无需，通常来说也不应该再编写⾃己的了(条款59)。 JDBC并未使用ServiceLoader，因为前者出现的时间要更早⼀些。 There are many variants of the service provider framework pattern. For exam- ple, the service access API can return a richer service interface to clients than the one furnished by providers. This is the Bridge pattern [Gamma95]. Dependency injection frameworks (Item 5) can be viewed as powerful service providers. Since Java 6, the platform includes a general-purpose service provider framework, java.util.ServiceLoader, so you needn’t, and generally shouldn’t, write your own (Item 59). JDBC doesn’t use ServiceLoader, as the former predates the latter. 仅提供静态工厂方法的主要限制是，没有公有或受保护构造函数的类不能被子类化。例如，子类化集合框架的实现类就很方便(翻译的什么鬼)。可以说这是一种因祸得福的做法，因为它鼓励程序员用组合而不是继承（条款18），并且对于不可变类是必须的。 只提供静态⼯厂方法的主要限制在于，没有公有或是受保护构造⽅法的类是⽆法被⼦子类化的。⽐如说，我们⽆法子类化集合框架中的任何便捷实现类。另一方面，这么做会⿎鼓励程序员们使⽤用组合⽽而⾮继承(条款18)，并且这对于不变类型来说也是需要的(条款17)。The main limitation of providing only static factory methods is that classes without public or protected constructors cannot be subclassed. For example, it is impossible to subclass any of the convenience implementation classes in the Collections Framework. Arguably this can be a blessing in disguise because it encourages programmers to use composition instead of inheritance (Item 18), and is required for immutable types (Item 17). 静态工厂方法的第二个缺点是程序员很难找到它们。它们在API文档中不像构造函数那样清晰，因此很难弄清楚如何用提供的静态工厂方法实例化一个类而不是构造函数。Javadoc工具某一天可能将注意力转换到静态工厂方法上。与此同时，你可以通过在类或接口的静态工厂方法的文档上引起注意(是多注意文档里的静态工厂方法)，以及遵守通用命名约定来减少这个问题。下面是一些静态工厂方法的常见名称，这个列表还远远不够详尽： 静态⼯厂方法的第2个缺点在于，程序员们很难找到他们。他们并不像构造⽅法那样在API⽂档中有清楚的说明，这样对于既提供静态⼯厂⽅法，⼜提供构造方法的类来说，我们就很难知晓到底该⽤那种⽅式来实例化它。可能在未来的某⼀一天，Javadoc工具会重视起静态⼯厂方法。与此同时，你可以多注意到类或接⼝文档中的静态⼯厂并坚持使用常见的命名约定来减少此类问题的发生。如下是静态工⼚方法的一些常⻅见名字，这个列表只是部分，还不不完全: A second shortcoming of static factory methods is that they are hard for programmers to find. They do not stand out in API documentation in the waythat constructors do, so it can be difficult to figure out how to instantiate a class that provides static factory methods instead of constructors. The Javadoc tool may someday draw attention to static factory methods. In the meantime, you can reduce this problem by drawing attention to static factories in class or interface documentation and by adhering to common naming conventions. Here are some common names for static factory methods. This list is far from exhaustive: form–一个类型转换方法接受单个参数，并返回这个类型一个相应的实例，例如： Date d = Date.from(instant); of–一个聚合方法接受多个参数，并返回一个组装他们的类型的实例，例如： Set&lt;Rank&gt; faceCards = EnumSet.of(JACK, QUEEN, KING); valueOf–from和of的一种冗长的形式，例如： BigInteger prime = BigInteger.valueOf(Integer.MAX_VALUE); instance或者getInstance–根据它的参数返回一个实例，但是他们不是相同的值，例如： StackWalker luke = StackWalker.getInstance(options); create或者newInstance–跟instance或者getInstance是一样的，但是方法能保证每次返回的都是新的实例，例如： Object newArray = Array.newInstance(classObject, arrayLen); getType–和getInstance一样，但是这个工厂方法是用在另外一个不同的类中。Type是工厂方法返回对象的类型。例如： FileStore fs = Files.getFileStore(path); newType–和newInstance一样，但是这个工厂方法是用在另外一个不同的类中。Type是工厂方法返回对象的类型。例如： BufferedReader br = Files.newBufferedReader(path); type–getType和newType的一种另外一种简介的形式，例如： List&lt;Complaint&gt; litany = Collections.list(legacyLitany); 总之，静态工厂方法和公有的构造方法都有各自的用途，理解它们的有点是值得的。通常静态工厂更好，所以避免在没有首先考虑静态工厂的情况下提供公有的构造方法。 总结⼀一下，静态⼯厂方法与公有的构造方法都有各自的适用场景，我们需要理解他们各自的优点。通常，静态⼯厂是优先选择的，这样可以避免习惯性地在没有考虑静态⼯厂的情况下就提供公有构造⽅方法的情况发⽣生。 In summary, static factory methods and public constructors both have their uses, and it pays to understand their relative merits. Often static factories are preferable, so avoid the reflex to provide public constructors without first consid- ering static factories.]]></content>
      <categories>
        <category>Effective Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装redis4.0.10并进行生产环境部署]]></title>
    <url>%2F2018%2F07%2F19%2Fredis-install%2F</url>
    <content type="text"><![CDATA[下载直奔主题，官网下载redis最新版本（2018年07月19日） http://download.redis.io/releases/redis-4.0.10.tar.gz 环境准备安装tcl 1yum install tcl 安装单机版Reids12345mkdir /usr/local/redis &amp;&amp; cd /usr/local/rediswget http://download.redis.io/releases/redis-4.0.10.tar.gztar -zxvf redis-4.0.10.tar.gzcd redis-4.0.10make &amp;&amp; make test &amp;&amp; make install 最后一步可能需要等的有点久，安装完成以后就开始进行生产环境配置。 Redis生产环境启动方案 redis的utils目录下，有个redis_init_script脚本 将redis_init_script脚本拷贝到linux的/etc/init.d目录中，将redis_init_script重命名为redis_6370，6370是我们希望这个redis实例监听的端口号 1cp /usr/local/redis/redis-4.0.10/utils/redis_init_scrip /etc/init.d/redis_6370 修改redis_6370脚本的第6行的REDISPORT，设置为相同的端口号（默认是6379） 创建两个目录：/etc/redis（存放redis的配置文件），/var/redis/6370（存放redis的持久化文件） 修改redis配置文件（默认在根目录下，redis.conf），拷贝到/etc/redis目录中，修改名称为6370.conf 1cp /usr/local/redis/redis-4.0.10/redis.conf /etc/redis/6370.conf 修改redis.conf中的部分配置为生产环境 123456789101112131415# 让redis以daemon进程运行daemonize yes # 设置redis的pid文件位置pidfile /var/run/redis_6370.pid # 设置redis的监听端口号port 6370 # 设置持久化文件的存储位置dir /var/redis/6370 # 打开数据持久化appendonly yes # 设置密码requirepass yourpass # 设置连接Redis的地址# 如果提供给其他机器访问，请在此处设置IP为机器IPbind 127.0.0.1 启动redis 123cd /etc/init.d/chmod 777 redis_6370./redis_6370 start 确认redis进程是否启动 1ps -ef | grep redis 让redis跟随系统启动自动启动，在/etc/init.d/redis_6370脚本中，最上面，加入两行注释 123#!/bin/sh# chkconfig: 2345 90 10# description: Redis is a persistent key-value database 然后执行命令: 1chkconfig redis_6370 on redis-cli测试PING12redis-cli -p 6370 -a yourpass PINGPONG 停机1redis-cli -h 127.0.0.1 -p 6370 SHUTDOWN 连接123456789redis-cli -p 6370 127.0.0.1:6370&gt; auth yourpassOK127.0.0.1:6370&gt; PINGPONG127.0.0.1:6370&gt; set k1 v1OK127.0.0.1:6370&gt; get k1"v1" 是不是很简单明了？文章到此结束。。。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch03-集群和架构讲解]]></title>
    <url>%2F2018%2F06%2F20%2FelasticSearch03%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 ES基础分布式架构Elasticsearch对复杂分布式机制的透明隐藏特性Elasticsearch是一套分布式的系统，分布式就是为了应对大数据量。它隐藏了复杂的分布式机制，其中有几个很重要的机制和概念。 分片机制（之前将一些document插入到es集群中去，不用关心数据怎么进行分片的，数据到哪个shard中去，这是ES自动完成的） cluster discovery（集群发现机制：在第一篇文章中做那个集群status从yellow转green的实验里，直接启动了第二个es进程，那个进程就作为一个node自动就发现了集群，并且加入了进去，还接受了部分数据） shard负载均衡（举例，假设现在有3个节点，总共有25个shard要分配到3个节点上去，es会自动进行均匀分配，以保持每个节点的均衡的读写负载请求） 总结下来就是：分片副本，请求路由，集群扩容，分片重分配 Elasticsearch的垂直扩容与水平扩容 垂直扩容：采购更强大的服务器，成本非常高昂，而且会有瓶颈，假设世界上最强大的服务器容量就是10T，但是当你的总数据量达到5000T的时候，你要采购多少台最强大的服务器？ 水平扩容：这是业界经常采用的方案，采购越来越多的普通服务器，虽然性能比较一般，但是很多普通服务器组织在一起，就能构成强大的计算和存储能力。 例如： 普通服务器：1T，1万一台，需要100万强大服务器：10T，50万一台，需要500万 一般是采用水平扩容的方式 增减或减少节点时的数据rebalance每当增加或者减少节点的时候，ES会自动的负载均衡保持每个节点的shard负载均衡，保证每台服务器的分片数量均衡。 master节点作用ES集群都有一个master节点，用来管理ES集群的元数据：比如索引的创建和删除，维护索引的元数据；节点的增加和移除等等。 默认情况下会自动的选出一台节点作为master节点，master不承载请求，所以没有单点瓶颈。 节点对等的分布式架构所有的节点都可以接受请求，也可以存储数据，如果数据不在自己的节点上，就去别的节点将数据找到然后返回给客户端。 节点对等，每个节点都能接收所有的请求 自动请求路由 响应收集 shard&amp;replica机制再次梳理 一个index包含一个或者多个shard 每个shard都是一个最小工作单元，它承载部分数据，每一个都是lucene实例，具有完整的建立索引和处理请求的能力。 增减节点时，shard会自动在nodes中负载均衡 primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard。 replica shard是primary shard的副本，负责容错，以及承担读请求负载。 primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改。 primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上。 单node环境下创建index是什么样子的1234567PUT /test_index&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 单node环境下，创建一个index，它有3个primary shard，3个replica shard 集群status是yellow 这个时候，只会将3个primary shard分配到仅有的一个node上去，另外3个replica shard是无法分配的（primary shard不能和自己的replica shard放在同一个节点上，集群状态是yellow的原因） 集群可以正常工作，但是一旦出现节点宕机，数据全部丢失，而且集群不可用，无法承接任何请求 2个node环境下replica shard是如何分配的 replica shard分配：5个primary shard，5个replica shard，2个node primary和replica的数据是同步的 primary/replica都可能会收到读请求 上面有灰色边框的是primary shard 横向扩容的过程，如何超出扩容极限，以及如何提升容错性 如果本身是2个节点，扩容1个节点后primary&amp;replica会自动负载均衡。6个shard，3 primary，3 replica会平均的被分配3个节点中。 扩容后每个node有更少的shard，意味着IO/CPU/Memory资源给每个shard分配更多，每个shard性能更好。 扩容的极限是什么？如果是6个shard（3 primary，3 replica），那么最多扩容到6台机器，每个shard可以占用单台服务器的所有资源，这个时候性能最好。 如果要超出扩容极限，那就动态修改replica数量，9个shard（3primary，6 replica），扩容到9台机器，比3台机器时，拥有3倍的读吞吐量。 在3台机器的情况下，9个shard（3 primary，6 replica），虽然资源更少，但是容错性更好，最多容纳2台机器宕机，如果是配置的6个shard那么只能容纳1台机器宕机。 这里的这些知识点综合起来看，一方面是说扩容的原理，怎么扩容，怎么提升系统整体吞吐量；另一方面要考虑到系统的容错性，怎么保证提高容错性，让尽可能多的服务器宕机，保证数据不丢失。 Elasticsearch容错机制：master选举，replica容错，数据恢复假设我们一共有9 shard，3个node。 如果master node宕机了，那么ES集群会自动进行master选举，自动选举另外一个node成为新的master，这个时候集群状态会变成red。 replica容错：新的master将replica shard提升为primary shard，集群状态变成yellow。 然后我们再重启宕机的node，master会将确实的副本复制到该node，会使用原有的shard并同步宕机后的修改，然后集群变成green。]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用缓存架构实战6-缓存雪崩及解决方案]]></title>
    <url>%2F2018%2F06%2F12%2Fcache06%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程笔记，记录以供以后翻看 缓存雪崩缓存雪崩这种场景，缓存架构中非常重要的一个环节，应对缓存雪崩的解决方案，避免缓存雪崩的时候，造成整个系统崩溃，带来巨大的经济损失 缓存雪崩的过程和后果缓存雪崩，一般首先是redis集群彻底崩溃，它导致崩溃的流程如下： redis集群彻底崩溃 缓存服务大量对redis的请求hang住，占用资源 缓存服务大量的请求打到源头服务去查询mysql，直接打死mysql 源头服务因为mysql被打死也崩溃，对源服务的请求也hang住，占用资源 缓存服务大量的资源全部耗费在访问redis和源服务无果，最后自己被拖死，无法提供服务 nginx无法访问缓存服务，redis和源服务，只能基于本地缓存提供服务，但是缓存过期后，没有数据提供 网站崩溃 缓存雪崩的解决方案相对来说，考虑的比较完善的一套方案，分为事前，事中，事后三个层次去思考怎么来应对缓存雪崩的场景 事前解决方案所谓事前解决方案，就是发生缓存雪崩之前，事情之前，怎么去避免redis彻底挂掉。 那就是保证redis的高可用性，我们利用redis本身的高可用性，复制，主从架构等功能，操作主节点去读写，数据同步到从节点，一旦主节点挂掉，从节点就跟上。 一般是建议双机房部署，一套redis cluster，部分机器在一个机房，另一部分机器在另外一个机房。 还有一种部署方式，两套redis cluster，两套redis cluster之间做一个数据的同步，redis集群是可以搭建成树状的结构的。一旦单个机房出了故障，至少另外一个机房还能有些redis实例提供服务。 事中解决方案如果redis cluster已经彻底崩溃了，已经开始大量的访问无法访问到redis了，那之前文章讲到过的多级缓存就起作用了。 ehcache缓存，第一应对零散的redis中数据被清除掉的现象，另外一个主要是预防redis彻底崩溃。这样多台机器上部署的缓存服务实例的内存中，还有一套ehcache的缓存，可以基于本地的ehcache的缓存提供一部分的数据。 一旦redis集群彻底崩溃了需要做以下几个步骤： 对redis的访问做资源隔离，避免所有资源hang在访问redis上 对redis访问失败的情况做相应的熔断和降级策略 使用ehcache本地缓存 对源服务访问的限流以及资源隔离（mysql层） 事后解决方案如何恢复Redis Cluster，有两种情况 redis数据可以恢复，做了备份，redis数据备份和恢复，redis重新启动起来 redis数据彻底丢失了，或者数据过旧，快速缓存预热，redis重新启动起来 其实这套方案没什么东西，事前的Redis文章前面也说了，事中ehcache也做过了。但是，如何将缓存服务如何设计成高可用的架构，需要配合Hystrix来开发。我们的熔断，降级，限流等等操作都需要Hystrix的配合。 Hystrix系列文章 使用Hystrix对Redis进行资源隔离接下来就要对redis的访问这一块加上保护措施，给商品服务的访问加上限流的保护措施。redis这一块，全都用hystrix的command进行封装，做资源隔离，确保redis的访问只能在固定的线程池内的资源来进行访问，哪怕是redis访问的很慢，有等待和超时，也不要紧，只有少量额线程资源用来访问，缓存服务不会被拖垮。 找到之前的缓存项目https://github.com/sail-y/eshop-cache ，引入Hystrix的依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-core&lt;/artifactId&gt; &lt;version&gt;1.5.12&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-metrics-event-stream&lt;/artifactId&gt; &lt;version&gt;1.4.10&lt;/version&gt;&lt;/dependency&gt; 在CacheServiceImpl里有几处用到redis的地方，我们就需要开发几个相应的command。 SaveProductInfo2RedisCacheCommandSaveProductInfo2RedisCacheCommand.java 12345678910111213141516171819202122232425/** * @author yangfan * @date 2018/06/12 */public class SaveProductInfo2RedisCacheCommand extends HystrixCommand&lt;Boolean&gt; &#123; private ProductInfo productInfo; public SaveProductInfo2RedisCacheCommand(ProductInfo productInfo) &#123; super(HystrixCommandGroupKey.Factory.asKey("RedisGroup")); this.productInfo = productInfo; &#125; @Override protected Boolean run() &#123; StringRedisTemplate redisTemplate = SpringContext.getApplicationContext().getBean(StringRedisTemplate.class); String key = "product_info_" + productInfo.getId(); redisTemplate.opsForValue().set(key, JSON.toJSONString(productInfo)); return true; &#125;&#125; 然后用command替换之前的实现 CacheServiceImpl.saveProductInfo2RedisCache 123456789/** * 将商品信息保存到redis中 * * @param productInfo */public void saveProductInfo2RedisCache(ProductInfo productInfo) &#123; SaveProductInfo2RedisCacheCommand command = new SaveProductInfo2RedisCacheCommand(productInfo); command.execute();&#125; SaveShopInfo2RedisCacheCommandSaveShopInfo2RedisCacheCommand.java 123456789101112131415161718192021222324252627/** * 保存商品信息到Redis * * @author yangfan * @date 2018/06/12 */public class SaveShopInfo2RedisCacheCommand extends HystrixCommand&lt;Boolean&gt; &#123; private ShopInfo shopInfo; public SaveShopInfo2RedisCacheCommand(ShopInfo shopInfo) &#123; super(HystrixCommandGroupKey.Factory.asKey("RedisGroup")); this.shopInfo = shopInfo; &#125; @Override protected Boolean run() &#123; StringRedisTemplate redisTemplate = SpringContext.getApplicationContext().getBean(StringRedisTemplate.class); String key = "shop_info_" + shopInfo.getId(); redisTemplate.opsForValue().set(key, JSONObject.toJSONString(shopInfo)); return true; &#125;&#125; 然后用command替换之前的实现 CacheServiceImpl.saveShopInfo2RedisCache 123456789/** * 将店铺信息保存到redis中 * * @param shopInfo */public void saveShopInfo2RedisCache(ShopInfo shopInfo) &#123; SaveShopInfo2RedisCacheCommand command = new SaveShopInfo2RedisCacheCommand(shopInfo); command.execute();&#125; GetProductInfoFromRedisCacheCommandGetProductInfoFromRedisCacheCommand.java 1234567891011121314151617181920212223242526272829/** * 从Redis获取商品Command * * @author yangfan * @date 2018/06/12 */public class GetProductInfoFromRedisCacheCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; public GetProductInfoFromRedisCacheCommand(Long productId) &#123; super(HystrixCommandGroupKey.Factory.asKey("RedisGroup")); this.productId = productId; &#125; @Override protected ProductInfo run() &#123; StringRedisTemplate redisTemplate = SpringContext.getApplicationContext().getBean(StringRedisTemplate.class); String key = "product_info_" + productId; String json = redisTemplate.opsForValue().get(key); if (json != null) &#123; return JSONObject.parseObject(json, ProductInfo.class); &#125; return null; &#125;&#125; 然后用command替换之前的实现 CacheServiceImpl.getProductInfoFromRedisCache 1234567891011/** * 从Redis从获取商品信息 * * @param id * @return */@Overridepublic ProductInfo getProductInfoFromRedisCache(Long id) &#123; GetProductInfoFromRedisCacheCommand command = new GetProductInfoFromRedisCacheCommand(id); return command.execute();&#125; GetShopInfoFromRedisCacheCommandGetShopInfoFromRedisCacheCommand.java 123456789101112131415161718192021222324252627282930/** * 从Redis获取店铺信息Command * @author yangfan * @date 2018/06/12 */public class GetShopInfoFromRedisCacheCommand extends HystrixCommand&lt;ShopInfo&gt; &#123; private Long shopId; public GetShopInfoFromRedisCacheCommand(Long shopId) &#123; super(HystrixCommandGroupKey.Factory.asKey("RedisGroup")); this.shopId = shopId; &#125; @Override protected ShopInfo run() &#123; StringRedisTemplate redisTemplate = SpringContext.getApplicationContext().getBean(StringRedisTemplate.class); String key = "shop_info_" + shopId; String json = redisTemplate.opsForValue().get(key); if (json != null) &#123; return JSONObject.parseObject(json, ShopInfo.class); &#125; return null; &#125;&#125; 然后用command替换之前的实现 CacheServiceImpl.getShopInfoFromRedisCache 1234567891011/** * 从Redis中获取商品店铺信息 * * @param id * @return */@Overridepublic ShopInfo getShopInfoFromRedisCache(Long id) &#123; GetShopInfoFromRedisCacheCommand command = new GetShopInfoFromRedisCacheCommand(id); return command.execute();&#125; 使用Hystrix对Redis访问进行降级上面已经通过hystrix command对redis的访问进行了资源隔离，避免redis访问频繁失败，或者频繁超时的时候，耗尽大量的tomcat容器的资源去hang在redis的访问上。 这样就限定只有一部分线程资源可以用来访问redis，如果redis集群彻底崩溃了，这个时候，可能command对redis的访问大量的报错和timeout超时，熔断（短路），我们就需要对redis进行降级，用Hystrix的fallback机制。建议是使用fail silent模式，fallback里面直接返回一个空值，比如一个null，最简单。 在外面调用redis的代码（CacheService类），只要你把timeout、熔断、熔断恢复、降级，都做好了，是感知不到redis的访问异常的。可能会出现的情况是，当redis集群崩溃的时候，CacheService会获取到的是大量的null空值。 根据这个null空值，我们还可以去做多级缓存的降级访问，nginx本地缓存，redis分布式集群缓存，ehcache本地缓存等等。 1234@Overrideprotected ProductInfo getFallback() &#123; return null;&#125; 顺便回顾一下之前CacheController的代码，在从redis里获取null值以后，会自动去别的地方一步步获取。 12345678910111213141516171819202122232425@GetMapping("/getProductInfo")public ProductInfo getProductInfo(Long productId) &#123; // 先从Redis从获取数据 ProductInfo productInfo = cacheService.getProductInfoFromRedisCache(productId); if (productInfo != null) &#123; System.out.println("=================从redis中获取缓存，商品信息=" + productInfo); &#125; if (productInfo == null) &#123; productInfo = cacheService.getProductInfoFromLocalCache(productId); System.out.println("================从ehcache从获取缓存，商品信息=" + productInfo); &#125; if (productInfo == null) &#123; // 就需要从数据源重新拉取数据，重建缓存，模拟获取 String productInfoJSON = "&#123;\"id\": " + productId + ", \"name\": \"iphone7手机\", \"price\": 5599, \"pictureList\":\"a.jpg,b.jpg\", \"specification\": \"iphone7的规格\", \"service\": \"iphone7的售后服务\", \"color\": \"红色,白色,黑色\", \"size\": \"5.5\", \"shopId\": 2, \"modifiedTime\": \"2018-02-21 22:11:34\"&#125;"; productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); // 将数据推送到一个内存队列中 RebuildCacheQueue rebuildCacheQueue = RebuildCacheQueue.getInstance(); rebuildCacheQueue.putProductInfo(productInfo); &#125; return productInfo;&#125; 经过这样一个简单的改造，我们使用Hystrix对redis的线程资源隔离和降级都很容易的完成了。 Redis集群崩溃定制化熔断策略缓存雪崩的事中解决方案 redis集群崩溃的时候，Hystrix会怎么样？ 大量的等待，超时，报错 如果是短时间内报错，会直接走fallback降级，直接返回null 超时控制，应该是判断redis访问超过了多长时间，就直接给报错timeout了 不推荐用默认的值，一般不太精准，redis的访问先统计一下访问时长的百分比，hystrix dashboard里可以看到TP90，TP95，TP99的时间分别是多少。一般redis访问TP99在100ms以内，那么此时timeout时长稍微设置多一些，比如100ms。 timeout设置HystrixCommandProperties.Setter() .withExecutionTimeoutInMilliseconds(int value) 意义在于哪里？一旦redis出现了大面积的故障，此时肯定是访问的时候大量的超过100ms，大量的在等待和超时，这样就可以确保大量的请求不会hang住过长的时间，比如hang住个1s，500ms。如果100ms直接就报timeout，就会走fallback降级了。 熔断策略开启熔断有2个参数 circuitBreaker.requestVolumeThreshold设置一个rolling window，滑动窗口中，最少要有多少个请求时，才触发开启短路。举例，如果设置为20（默认值），那么在一个10秒的滑动窗口内，如果只有19个请求，即使这19个请求都是异常的，也是不会触发开启短路器的。 HystrixCommandProperties.Setter() .withCircuitBreakerRequestVolumeThreshold(int value) 我们应该根据我们自己的平时的访问流量去设置，而不是用默认值，比如，我们认为平时一般的时候，流量也可以在每秒在QPS 100，10秒的滑动窗口就是1000，一般可以设置600或者800一个值，需要根据自己的系统的流量去设置。假如你设置的太少了，或者太多了，都不太合适。举个例子，你设置一个20，结果在晚上最低峰的时候，刚好是30，可能晚上的时候因为访问不频繁，大量的找不到缓存，可能超时频繁了一些，结果直接就给短路了。 circuitBreaker.errorThresholdPercentage设置异常请求量的百分比，当异常请求达到这个百分比时，就触发打开短路器，默认是50，也就是50% 12HystrixCommandProperties.Setter() .withCircuitBreakerErrorThresholdPercentage(int value) 我们最好还是自己定制，自己设置，如果是要50%的时候才短路的话，会有什么情况呢?10%短路，也不太靠谱，90%异常，才短路也不行。这个值可以稍微高一些，如果redis集群彻底崩溃，那么基本上就是所有的请求，100%都会异常，所以一般设置60%，70%。也有可能偶然出现网络的抖动，导致比如就这10秒钟，访问延时高了一些，其实可能并不需要立即就短路，可能下个10秒马上就恢复了。 金融支付类的接口，可能这个比例就会设置的很低，因为对异常系统必须要很敏感，可能就是10%异常了，就直接短路了，不让继续访问了。金融支付类的接口是很重要的，而且必须是很稳定，我们不能容忍任何的延迟或者是报错。一旦支付类的接口，有10%的异常的话，我们基本就可以认为这个接口已经出问题了，再继续访问的话，也许访问的就是有问题的接口，可能造成资金的错乱，给公司造成损失。所以直接熔断吧，不让访问了，走降级策略，这就是对整个系统的一个安全性保障。 circuitBreaker.sleepWindowInMilliseconds设置在短路之后，需要在多长时间内直接reject请求，然后在这段时间之后，再重新导half-open状态，尝试允许请求通过以及自动恢复，默认值是5000毫秒 12HystrixCommandProperties.Setter() .withCircuitBreakerSleepWindowInMilliseconds(int value) 如果redis集群崩溃了，会在5s内就直接恢复。 Hystrix保护源服务，防止Mysql崩溃做缓存服务，redis集群彻底崩溃的时候，除了对redis本身做资源隔离、超时控制、熔断策略。还要保护源服务，因为Redis集群崩溃后，大量的请求会高并发会去访问源服务-商品服务（提供商品数据）。如果QPS10000去访问商品服务，基于mysql去查询，那mysql肯定会挂掉，商品服务也就死掉了。 所以要对商品服务这种源服务的访问施加限流的措施，限流怎么限，hystrix本身就是提供了两种机制，线程池（内部做了异步化处理，可以处理超时），semaphore（信号量，让tomcat线程执行运行逻辑，没有内部的异步化处理，一旦超时，会导致tomcat线程就hang住了）。 一般推荐线程池用来做有网络访问的这种资源隔离，因为涉及到网络，就很容易超时；sempahore是用来做对服务纯内存的一些复杂业务逻辑的操作进行限流，因为不涉及网络访问，就是纯粹为了避免对内存内的复杂业务逻辑进行太高并发的访问，造成系统本身的故障。semaphore在以下情况是很合适的：比如一些推荐、搜索，有部分算法，复杂的算法，是放在服务内部纯内存去运行的，一个服务暴露出来的就是某个算法的执行。 我们这里是访问外部的商品服务，所以还是用线程池做限流，需要算一下，要限多少，怎么限？ 假设每次商品服务的访问性能在200ms，1个线程一秒可以执行5次访问，假设我们一个缓存服务实例对这个商品服务的访问每秒在150次。所以这个时候，我们就需要30个线程，每个线程每秒可以访问5次，总共每秒30个线程可以访问150次。 我们算的这个每秒150次访问时正常情况下，如果是非正常情况下，每秒1000次，甚至1w次，此时就可以自然限流，因为我们的线程池就30个。在非正常情况下，直接线程池+等待队列全满，此时就会出现大量的reject操作，然后就会去调用降级逻辑。接着我们要做限流，设置的就是线程池的大小，还有等待队列的大小，30个线程可以每秒处理150个请求，但是偶尔会多一些出来，同时30个线程处理150个请求会快一些，不用花费1秒钟，等待队列给一些buffer，不要偶尔1秒钟来了200条请求，50条直接给reject掉了。等待队列设置150个，30个线程直接500ms处理完了，等待队列中的50个请求就可以继续处理。 12345678910111213141516171819202122232425262728293031323334public class GetProductInfoCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; public GetProductInfoCommand(Long productId) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ProductInfoService")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey("GetProductInfoPool")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() .withCoreSize(10) .withMaxQueueSize(12) .withQueueSizeRejectionThreshold(8) .withMaximumSize(30) .withAllowMaximumSizeToDivergeFromCoreSize(true) .withKeepAliveTimeMinutes(1) .withMaxQueueSize(50) .withQueueSizeRejectionThreshold(100)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() // 多少个请求以上才会判断断路器是否需要开启。 .withCircuitBreakerRequestVolumeThreshold(30) // 错误的请求达到40%的时候就开始断路。 .withCircuitBreakerErrorThresholdPercentage(40) // 3秒以后尝试恢复 .withCircuitBreakerSleepWindowInMilliseconds(4000)) ); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; String productInfoJSON = "&#123;\"id\": " + productId + ", \"name\": \"iphone7手机\", \"price\": 5599, \"pictureList\":\"a.jpg,b.jpg\", \"specification\": \"iphone7的规格\", \"service\": \"iphone7的售后服务\", \"color\": \"红色,白色,黑色\", \"size\": \"5.5\", \"shopId\": 1, \"modifiedTime\": \"2017-01-01 12:01:00\"&#125;"; return JSONObject.parseObject(productInfoJSON, ProductInfo.class); &#125;&#125; 源服务fallback降级机制现在nginx本地缓存没有，redis集群崩溃，ehcache也找不到这条数据对应的缓存，只能去源头服务里面查询，但是查询的请求又被限流了，现在请求到了这里，被限流了以后只能走降级逻辑。 这里的一种降级机制叫做stubbed fallback降级机制（残缺的降级），就是用请求参数中少量的数据，加上纯内存中缓存的少量的数据来提供残缺的数据服务。 缓存雪崩预防和解决方案回顾 事前，redis高可用性，redis cluster，sentinal，复制，主从，从-&gt;主，双机房部署 事中，ehcache可以扛一扛，redis挂掉之后的资源隔离、超时控制、熔断，商品服务的访问限流、多级降级，缓存服务在雪崩场景下存活下来，基于ehcache和存活的商品服务提供数据 事后，快速恢复Redis，备份+恢复，快速的缓存预热的方案 缓存穿透如果一直访问的根本不存在的时候，那么就会导致缓存穿透，所有的这种请求都会直接到mysql这边来。 缓存穿透的解决方案其实非常简单，就是如果从源服务（商品服务）查询到的数据是空，就说明这个数据根本就不存在。那么如果这个数据不存在的话，我们也往redis和ehcache等缓存中写入一个数据，可以写入一个空的数据，比如空的productInfo的json串，给nginx也是，返回一个空的productInfo的json串。 我们有异步监听数据变更的机制在里面，如果数据变更的话，某个数据本来是没有的，可能会导致缓存穿透，所以我们给了个空数据，但是现在这个数据有了，我们接收到这个变更的消息过后，就可以将数据再次从源服务中查询出来，然后设置到各级缓存中去了。 缓存失效之前在nginx中设置本地的缓存的时候，给了一个过期的时间（10分钟）。10分钟以后自动过期，过期了以后，就会重新从redis中去获取数据。10分钟到期自动过期，就叫做缓存的失效。如果缓存失效以后，那么实际上此时，就会有大量的请求回到redis中去查询。 如果同一时间来了1000个请求，都将缓存cache在了nginx自己的本地，缓存失效的时间都设置了10分钟，那么是不是可能导致10分钟过后，这些数据，就自动全部在同一时间失效了。如果同一时间全部失效，会不会导致同一时间大量的请求过来，在nginx里找不到缓存数据，全部高并发走到redis上去了。加重大量的网络请求，网络负载也会加重。 解决方案很简单，就是把10分钟的时间改成一个随机数,随机一个失效的时间。 12math.randomseed(tostring(os.time()):reverse():sub(1, 7))local expireTime = math.random(600, 1200)]]></content>
      <categories>
        <category>高可用缓存架构实战</category>
      </categories>
      <tags>
        <tag>多级缓存架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix监控和运维]]></title>
    <url>%2F2018%2F06%2F10%2Fhystrix04%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 监控为什么需要监控与报警？HystrixCommand执行的时候，会生成一些执行耗时等方面的统计信息。这些信息对于系统的运维来说，是很有帮助的，因为我们通过这些统计信息可以看到整个系统是怎么运行的。hystrix对每个command key都会提供一份metric，而且是秒级统计粒度的。这些统计信息，无论是单独看，还是聚合起来看，都是很有用的。如果将一个请求中的多个command的统计信息拿出来单独查看，包括耗时的统计，对debug系统是很有帮助的。聚合起来的metric对于系统层面的行为来说，是很有帮助的，很适合做报警或者报表。hystrix dashboard就很适合。 hystrix的事件类型对于hystrix command来说，只会返回一个值，execute只有一个event type，fallback也只有一个event type，那么返回一个SUCCESS就代表着命令执行的结束 对于hystrix observable command来说，多个值可能被返回，所以emit event代表一个value被返回，success代表成功，failure代表异常 execute event typeEMIT observable command返回一个valueSUCCESS 完成执行，并且没有报错FAILURE 执行时抛出了一个异常，会触发fallbackTIMEOUT 开始执行了，但是在指定时间内没有完成执行，会触发fallbackBAD_REQUEST 执行的时候抛出了一个HystrixBadRequestExceptionSHORT_CIRCUITED 短路器打开了，触发fallbackTHREAD_POOL_REJECTED 线程成的容量满了，被reject，触发fallbackSEMAPHORE_REJECTED 信号量的容量满了，被reject，触发fallback fallback event typeFALLBACK_EMIT observable command，fallback value被返回了FALLBACK_SUCCESS fallback逻辑执行没有报错FALLBACK_FAILURE fallback逻辑抛出了异常，会报错FALLBACK_REJECTION fallback的信号量容量满了，fallback不执行，报错FALLBACK_MISSING fallback没有实现，会报错 其他的event typeEXCEPTION_THROWN command生命自周期是否抛出了异常RESPONSE_FROM_CACHE command是否在cache中查找到了结果COLLAPSED command是否是一个合并batch中的一个 thread pool event typeEXECUTED 线程池有空间，允许command去执行了REJECTED 线程池没有空间，不允许command执行，reject掉了 collapser event typeBATCH_EXECUTED collapser合并了一个batch，并且执行了其中的commandADDED_TO_BATCH command加入了一个collapser batchRESPONSE_FROM_CACHE 没有加入batch，而是直接取了request cache中的数据 metric storagemetric被生成之后，就会按照一段时间来存储，存储了一段时间的数据才会推送到其他系统中，比如hystrix dashboard。 另外一种方式，就是每次生成metric就实时推送metric流到其他地方，但是这样的话，会给系统带来很大的压力。 hystrix的方式是将metric写入一个内存中的数据结构中，在一段时间之后就可以查询到，hystrix 1.5x之后，采取的是为每个command key都生成一个start event和completion event流，而且可以订阅这个流。每个thread pool key也是一样的，包括每个collapser key也是一样的。 每个command的event是发送给一个线程安全的RxJava中的rx.Subject，因为是线程安全的，所以不需要进行线程同步。 因此每个command级别的，threadpool级别的，每个collapser级别的，event都会发送到对应的RxJava的rx.Subject对象中。这些rx.Subject对象接着就会被暴露出Observable接口，可以被订阅。 metric统计相关的配置metrics.rollingStats.timeInMilliseconds设置统计的rolling window，单位是毫秒，hystrix只会维持这段时间内的metric供短路器统计使用 这个属性是不允许热修改的，默认值是10000，就是10秒钟 HystrixCommandProperties.Setter() .withMetricsRollingStatisticalWindowInMilliseconds(int value) metrics.rollingStats.numBuckets该属性设置每个滑动窗口被拆分成多少个bucket，而且滑动窗口对这个参数必须可以整除，同样不允许热修改 默认值是10，也就是说，每秒钟是一个bucket 随着时间的滚动，比如又过了一秒钟，那么最久的一秒钟的bucket就会被丢弃，然后新的一秒的bucket会被创建 HystrixCommandProperties.Setter() .withMetricsRollingStatisticalWindowBuckets(int value) metrics.rollingPercentile.enabled控制是否追踪请求耗时，以及通过百分比方式来统计，默认是true HystrixCommandProperties.Setter() .withMetricsRollingPercentileEnabled(boolean value) metrics.rollingPercentile.timeInMilliseconds设置rolling window被持久化保存的时间，这样才能计算一些请求耗时的百分比，默认是60000，60s，不允许热修改 相当于是一个大的rolling window，专门用于计算请求执行耗时的百分比 HystrixCommandProperties.Setter() .withMetricsRollingPercentileWindowInMilliseconds(int value) metrics.rollingPercentile.numBuckets设置rolling percentile window被拆分成的bucket数量，上面那个参数除以这个参数必须能够整除，不允许热修改 默认值是6，也就是每10s被拆分成一个bucket HystrixCommandProperties.Setter() .withMetricsRollingPercentileWindowBuckets(int value) metrics.rollingPercentile.bucketSize设置每个bucket的请求执行次数被保存的最大数量，如果在一个bucket内，执行次数超过了这个值，那么就会重新覆盖从bucket的开始再写 举例来说，如果bucket size设置为100，而且每个bucket代表一个10秒钟的窗口，但是在这个bucket内发生了500次请求执行，那么这个bucket内仅仅会保留100次执行 如果调大这个参数，就会提升需要耗费的内存，来存储相关的统计值，不允许热修改 默认值是100 HystrixCommandProperties.Setter() .withMetricsRollingPercentileBucketSize(int value) metrics.healthSnapshot.intervalInMilliseconds控制成功和失败的百分比计算，与影响短路器之间的等待时间，默认值是500毫秒 HystrixCommandProperties.Setter() .withMetricsHealthSnapshotIntervalInMilliseconds(int value) 监控部署找到之前的eshop-cache-ha项目，引入配置： https://github.com/sail-y/eshop-cache-ha 12345&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-metrics-event-stream&lt;/artifactId&gt; &lt;version&gt;1.4.10&lt;/version&gt;&lt;/dependency&gt; 然后再注册一个servlet的bean 12345public ServletRegistrationBean indexServletRegistration() &#123; ServletRegistrationBean registration = new ServletRegistrationBean(new HystrixMetricsStreamServlet()); registration.addUrlMappings("/hystrix.stream"); return registration;&#125; Tomcat 准备一个tomcat部署HystrixDashboard，网上下载一个hystrix-dashboard-1.5.12.war，再装一个turbin，turbin是用来监控一个集群的，可以将一个集群的所有机器都配置在这里。 在/WEB-INF/classes下添加一个配置文件，告诉turbin需要监控哪些实例。 config.properties 12turbine.ConfigPropertyBasedDiscovery.default.instances=localhostturbine.instanceUrlSuffix=:8081/hystrix.stream 然后启动我们自己的eshop-cache-ha项目， 访问页面查看 http://localhost:8080/hystrix-dashboard/ 填入我们要监控的URL. http://localhost:8081/hystrix.stream，监控单个机器 http://localhost:8080/turbine/turbine.stream，监控整个集群 现在还看不到什么东西，我们对项目发送几个请求，再看看效果 123456789101112131415161718/** * 监控请求测试 * * @author yangfan * @date 2018/06/10 */public class HystrixDashboardTest &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 50; i++) &#123; HttpUtil.get("http://localhost:8081/getProductInfo?productId=1"); try &#123; Thread.sleep(3000L); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; hystrix的dashboard可以支持实时监控metric netflix开始用这个dashboard的时候，大幅度优化了工程运维的操作，帮助节约了恢复系统的时间。大多数生产系统的故障持续时间变得很短，而且影响幅度小了很多，主要是因为hystrix dashborad提供了可视化的监控。 截图说明，dashboard上的指标都是什么？ 圆圈的颜色和大小代表了健康状况以及流量，折线代表了最近2分钟的请求流量 集群中的机器数量，请求延时的中位数以及平均值 最近10秒内的异常请求比例，请求QPS，每台机器的QPS，以及整个集群的QPS 断路器的状态 最近一分钟的请求延时百分比，TP90，TP99，TP99.5 几个有颜色的数字，代表了最近10秒钟的统计，以1秒钟为粒度 成功的请求数量，绿颜色的数字; 短路的请求数量，蓝色的数字; timeout超时的请求数量，黄色的数字; 线程池reject的请求数量，紫色的数字; 请求失败，抛出异常的请求数量，红色的数字 生产环境运维如果发现了严重的依赖调用延时，先不用急着去修改配置，如果一个command被限流了，可能本来就应该限流 在netflix早期的时候，经常会有人在发现短路器因为访问延时发生的时候，去热修改一些配置，比如线程池大小，队列大小，超时时长，等等，给更多的资源，但是这其实是不对的。 如果我们之前对系统进行了良好的配置，然后现在在高峰期，系统在进行线程池reject，超时，短路，那么此时我们应该集中精力去看底层根本的原因，而不是调整配置 为什么在高峰期，一个10个线程的线程池，搞不定这些流量呢？那就是代码写的太烂了，可以使用异步，或者更好的算法。 千万不要急于给你的依赖调用过多的资源，比如线程池大小，队列大小，超时时长，信号量容量，等等，因为这可能导致我们自己对自己的系统进行DDOS攻击。 举例来说，想象一下，我们现在有100台服务器组成的集群，每台机器有10个线程大小的线程池去访问一个服务，那么我们对那个服务就有1000个线程资源去访问了，在正常情况下，可能只会用到其中200~300个线程去访问那个后端服务。但是如果再高峰期出现了访问延时，可能导致1000个线程全部被调用去访问那个后端服务，如果我们调整到每台服务器20个线程呢？ 如果因为你的代码等问题导致访问延时，即使有20个线程可能还是会导致线程池资源被占满，此时就有2000个线程去访问后端服务，可能对后端服务就是一场灾难。 这就是断路器的作用了，如果我们把后端服务打死了，或者产生了大量的压力，有大量的timeout和reject，那么就自动短路，一段时间后，等流量洪峰过去了，再重启访问。 简单来说，让系统自己去限流，短路，超时，以及reject，直到系统重新变得正常了，就是不要随便乱改资源配置，不要随便乱增加线程池大小，等待队列大小，异常情况是正常的。]]></content>
      <categories>
        <category>hystrix</category>
      </categories>
      <tags>
        <tag>hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch02-快速入门]]></title>
    <url>%2F2018%2F06%2F05%2FelasticSearch02%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 快速入门环境准备和安装 安装JDK，至少1.8.0_73以上版本，java -version 下载和解压缩Elasticsearch安装包（https://www.elastic.co/downloads/past-releases/elasticsearch-5-2-0） 启动Elasticsearch：bin\elasticsearch，es本身特点之一就是开箱即用，如果是中小型应用，数据量少，操作不是很复杂的，直接启动就可以用了。 检查ES是否启动成功：http://localhost:9200/?pretty 1234567891011121314151617name: node名称cluster_name: 集群名称（默认的集群名称就是elasticsearch）version.number: 5.2.0，es版本号&#123; "name" : "4onsTYV", "cluster_name" : "elasticsearch", "cluster_uuid" : "nKZ9VK_vQdSQ1J0Dx9gx1Q", "version" : &#123; "number" : "5.2.0", "build_hash" : "24e05b9", "build_date" : "2017-01-24T19:52:35.800Z", "build_snapshot" : false, "lucene_version" : "6.4.0" &#125;, "tagline" : "You Know, for Search"&#125; 修改集群名称：elasticsearch.yml 修改elasticsearch.yml里面的cluster.name: my-application即可。 下载和解压缩Kibana安装包，使用里面的开发界面，去操作elasticsearch，作为我们学习es知识点的一个主要的界面入口 https://www.elastic.co/downloads/past-releases/kibana-5-2-0 访问http://localhost:5601/，使用devtools进行测试和开发 hello world学习一门新技术，搭建好环境后第一件事当然是做个helloworld的demo，我们来做个crud document数据格式 es的数据结构存储模式，熟悉mongodb的人应该知道，他们的数据结构是差不多的，都是面向文档的，一条数据就是一个json。 对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦。 ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能 1234567891011121314151617181920212223242526272829public class Employee &#123; private String email; private String firstName; private String lastName; private EmployeeInfo info; private Date joinDate;&#125;private class EmployeeInfo &#123; private String bio; // 性格 private Integer age; private String[] interests; // 兴趣爱好&#125;EmployeeInfo info = new EmployeeInfo();info.setBio("curious and modest");info.setAge(30);info.setInterests(new String[]&#123;"bike", "climb"&#125;);Employee employee = new Employee();employee.setEmail("zhangsan@sina.com");employee.setFirstName("san");employee.setLastName("zhang");employee.setInfo(info);employee.setJoinDate(new Date()); 上述代码，在mysql中，肯定就是两张表：employee表，employee_info表，将employee对象的数据重新拆开来，变成Employee数据和EmployeeInfo数据employee表：email，first_name，last_name，join_date，4个字段employee_info表：bio，age，interests，3个字段；此外还有一个外键字段，比如employee_id，关联着employee表 demo背景有一个电商网站，需要为其基于ES构建一个后台系统，提供以下功能： 对商品信息进行CRUD（增删改查）操作 执行简单的结构化查询 可以执行简单的全文检索，以及复杂的phrase（短语）检索 对于全文检索的结果，可以进行高亮显示 对数据进行简单的聚合分析 简单的集群管理快速检查集群的健康状况es提供了一套api，叫做cat api，可以查看es中各种各样的数据 GET /_cat/health?v 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1528602515 11:48:35 elasticsearch yellow 1 1 1 1 0 0 1 0 - 50.0% 如何快速了解集群的健康状况？green、yellow、red？ green：每个索引的primary shard和replica shard都是active状态的 yellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态 red：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了 为什么现在会处于一个yellow状态？ 我们现在就一个笔记本电脑，就启动了一个es进程，相当于就只有一个node。现在es中有一个index，就是kibana自己内置建立的index。由于默认的配置是给每个index分配5个primary shard和5个replica shard，而且primary shard和replica shard不能在同一台机器上（为了容错）。现在kibana自己建立的index是1个primary shard和1个replica shard。当前就一个node，所以只有1个primary shard被分配了和启动了，但是一个replica shard没有第二台机器去启动。 做一个小实验：此时只要启动第二个es进程，就会在es集群中有2个node，然后那1个replica shard就会自动分配过去，然后cluster status就会变成green状态。 快速查看集群中有哪些索引GET /_cat/indices?v 12health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open .kibana T5J6wa4FSy6ErH1zTOmIEg 1 1 1 0 3.1kb 3.1kb 简单的索引操作创建索引：PUT /test_index?pretty 12345678910health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open test_index cyS7pUwcQKmvTFtUCGNgHg 5 1 0 0 650b 650byellow open .kibana T5J6wa4FSy6ErH1zTOmIEg 1 1 1 0 3.1kb 3.1kb``` 删除索引：DELETE /test_index?pretty```texthealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open .kibana T5J6wa4FSy6ErH1zTOmIEg 1 1 1 0 3.1kb 3.1kb 商品的CRUD操作新增商品：新增文档，建立索引12345678910111213141516171819202122232425262728293031323334353637383940PUT /ecommerce/product/1&#123; "name" : "gaolujie yagao", "desc" : "gaoxiao meibai", "price" : 30, "producer" : "gaolujie producer", "tags": [ "meibai", "fangzhu" ]&#125;&#123; "_index": "ecommerce", "_type": "product", "_id": "1", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125;PUT /ecommerce/product/2&#123; "name" : "jiajieshi yagao", "desc" : "youxiao fangzhu", "price" : 25, "producer" : "jiajieshi producer", "tags": [ "fangzhu" ]&#125;PUT /ecommerce/product/3&#123; "name" : "zhonghua yagao", "desc" : "caoben zhiwu", "price" : 40, "producer" : "zhonghua producer", "tags": [ "qingxin" ]&#125; 查询商品：检索文档GET /index/type/id GET /ecommerce/product/1 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; "_index": "ecommerce", "_type": "product", "_id": "1", "_version": 1, "found": true, "_source": &#123; "name": "gaolujie yagao", "desc": "gaoxiao meibai", "price": 30, "producer": "gaolujie producer", "tags": [ "meibai", "fangzhu" ] &#125;&#125;``` #### 修改商品：替换文档```jsonPUT /ecommerce/product/1&#123; "name" : "jiaqiangban gaolujie yagao", "desc" : "gaoxiao meibai", "price" : 30, "producer" : "gaolujie producer", "tags": [ "meibai", "fangzhu" ]&#125;&#123; "_index": "ecommerce", "_type": "product", "_id": "1", "_version": 2, "result": "updated", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": false&#125; 替换方式有一个不好，即使必须带上所有的field，才能去进行信息的修改，否则文档的数据结构会被修改。 修改商品：更新文档12345678910111213141516171819POST /ecommerce/product/1/_update&#123; "doc": &#123; "name": "jiaqiangban gaolujie yagao" &#125;&#125;&#123; "_index": "ecommerce", "_type": "product", "_id": "1", "_version": 2, "result": "updated", "_shards": &#123; "total": 0, "successful": 0, "failed": 0 &#125;&#125; 删除商品：删除文档123456789101112131415DELETE /ecommerce/product/1&#123; "found": true, "_index": "ecommerce", "_type": "product", "_id": "1", "_version": 4, "result": "deleted", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;&#125; 多种搜索方式上面做了CRUD，接下里就是ES里最重要的搜索部分。 query string search搜索全部商品：GET /ecommerce/product/_search 123456took：耗费了几毫秒timed_out：是否超时，这里没有_shards：数据拆成了5个分片，所以对于搜索请求，会分配所有的primary shard（或者是它的某个replica shard）hits.total：查询结果的数量，3个documenthits.max_score：score的含义，就是document对于一个search的相关度的匹配分数，越相关，就越匹配，分数也高hits.hits：包含了匹配搜索的document的详细数据 query string search的由来，因为search参数都是以http请求的query string来附带的(也就是?后面的) 搜索商品名称中包含yagao的商品，而且按照售价降序排序： GET /ecommerce/product/_search?q=name:yagao&amp;sort=price:desc 适用于临时的在命令行使用一些工具，比如curl，快速的发出请求，来检索想要的信息；但是如果查询请求很复杂，是很难去构建的。在生产环境中，几乎很少使用query string search query DSLDSL：Domain Specified Language，特定领域的语言。http request body：请求体，可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法，比query string search肯定强大多了 查询所有的商品 1234GET /ecommerce/product/_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125; 查询名称包含yagao的商品，同时按照价格降序排序 1234567891011GET /ecommerce/product/_search&#123; "query" : &#123; "match" : &#123; "name" : "yagao" &#125; &#125;, "sort": [ &#123; "price": "desc" &#125; ]&#125; 分页查询商品，总共3条商品，假设每页就显示1条商品，现在显示第2页，所以就查出来第2个商品 123456GET /ecommerce/product/_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "from": 1, "size": 1&#125; 指定要查询出来商品的名称和价格就可以 12345GET /ecommerce/product/_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "_source": ["name", "price"]&#125; 更加适合生产环境的使用，可以构建复杂的查询。 query filter搜索商品名称包含yagao，而且售价大于25元的商品 1234567891011121314151617GET /ecommerce/product/_search&#123; "query" : &#123; "bool" : &#123; "must" : &#123; "match" : &#123; "name" : "yagao" &#125; &#125;, "filter" : &#123; "range" : &#123; "price" : &#123; "gt" : 25 &#125; &#125; &#125; &#125; &#125;&#125; full-text search（全文检索）12345678GET /ecommerce/product/_search&#123; "query" : &#123; "match" : &#123; "producer" : "yagao producer" &#125; &#125;&#125; 因为查询了2个单词，producer这个字段，会先被拆解，建立倒排索引。 在分词以后，只要任意命中其中一个单词，都会被查询出来，如果2个单词都命中会排在前面。 phrase search（短语搜索）跟全文检索相对应，相反，全文检索会将输入的搜索串拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回。 phrase search，要求输入的搜索串，必须在指定的字段文本中，完全包含一模一样的，才可以算匹配，才能作为结果返回。 12345678GET /ecommerce/product/_search&#123; "query" : &#123; "match_phrase" : &#123; "producer" : "yagao producer" &#125; &#125;&#125; highlight search（高亮搜索结果）12345678910111213GET /ecommerce/product/_search&#123; "query" : &#123; "match" : &#123; "producer" : "producer" &#125; &#125;, "highlight": &#123; "fields" : &#123; "producer" : &#123;&#125; &#125; &#125;&#125; 聚合分析快速入门计算每个tag下的商品数量12345678GET /ecommerce/product/_search&#123; "aggs": &#123; "group_by_tags": &#123; "terms": &#123; "field": "tags" &#125; &#125; &#125;&#125; 报错了，text类型的字段默认不支持聚合，得做个操作让它支持聚合，将文本field的fielddata属性设置为true。 123456789101112``````jsonPUT /ecommerce/_mapping/product&#123; "properties": &#123; "tags": &#123; "type": "text", "fielddata": true &#125; &#125;&#125; 再测试一次聚合查询，就好了 对名称中包含yagao的商品，计算每个tag下的商品数量在聚合分析的基础上加上条件筛选 12345678910111213141516GET /ecommerce/product/_search&#123; "size": 0, "query": &#123; "match": &#123; "name": "yagao" &#125; &#125;, "aggs": &#123; "all_tags": &#123; "terms": &#123; "field": "tags" &#125; &#125; &#125;&#125; 先分组，再算每组的平均值，计算每个tag下的商品的平均价格1234567891011121314GET /ecommerce/product/_search&#123; "size": 0, "aggs" : &#123; "group_by_tags" : &#123; "terms" : &#123; "field" : "tags" &#125;, "aggs" : &#123; "avg_price" : &#123; "avg" : &#123; "field" : "price" &#125; &#125; &#125; &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; "took": 22, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 0, "hits": [] &#125;, "aggregations": &#123; "group_by_tags": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "fangzhu", "doc_count": 2, "avg_price": &#123; "value": 27.5 &#125; &#125;, &#123; "key": "meibai", "doc_count": 1, "avg_price": &#123; "value": 30 &#125; &#125;, &#123; "key": "qingxin", "doc_count": 1, "avg_price": &#123; "value": 40 &#125; &#125; ] &#125; &#125;&#125; 计算每个tag下的商品的平均价格，并且按照平均价格降序排序1234567891011121314GET /ecommerce/product/_search&#123; "size": 0, "aggs" : &#123; "all_tags" : &#123; "terms" : &#123; "field" : "tags", "order": &#123; "avg_price": "desc" &#125; &#125;, "aggs" : &#123; "avg_price" : &#123; "avg" : &#123; "field" : "price" &#125; &#125; &#125; &#125; &#125;&#125; 按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格123456789101112131415161718192021222324252627282930313233343536373839GET /ecommerce/product/_search&#123; "size": 0, "aggs": &#123; "group_by_price": &#123; "range": &#123; "field": "price", "ranges": [ &#123; "from": 0, "to": 20 &#125;, &#123; "from": 20, "to": 40 &#125;, &#123; "from": 40, "to": 50 &#125; ] &#125;, "aggs": &#123; "group_by_tags": &#123; "terms": &#123; "field": "tags" &#125;, "aggs": &#123; "average_price": &#123; "avg": &#123; "field": "price" &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch01-介绍]]></title>
    <url>%2F2018%2F06%2F03%2FelasticSearch01%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 简介官网：http://www.elastic.co/products/elasticsearch 系列文章版本基于ElasticSearch5.2 什么是ElasticsearchElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 什么是搜索？百度：我们比如说想找寻任何的信息的时候，就会上百度去搜索一下，比如说找一部自己喜欢的电影，或者说找一本喜欢的书，或者找一条感兴趣的新闻（提到搜索的第一印象）但是百度 != 搜索，这是不对的 还有一种是垂直搜索（站内搜索）： 互联网的搜索：电商网站，招聘网站，新闻网站，各种appIT系统的搜索：OA软件，办公自动化软件，会议管理，日程管理，项目管理，员工管理，搜索“张三”，“张三儿”，“张小三”；有个电商网站，卖家，后台管理系统，搜索“牙膏”，订单，“牙膏相关的订单” 搜索，就是在任何场景下，找寻你想要的信息，输入一段你要搜索的关键字，期望找到这个关键字相关的有些信息 如果用数据库做搜索会怎么样？做软件开发或者对IT、计算机有一定的了解的话，都知道数据都是存储在数据库里面的，比如说电商网站的商品信息，招聘网站的职位信息，新闻网站的新闻信息，等等。所以说很自然的一点，如果说从技术的角度去考虑，如何实现如电商网站内部的搜索功能，就可以考虑去使用数据库去进行搜索。 弊端： 每条记录的指定字段的文本，可能会很长，比如说“商品描述”字段的长度，有长达数千个，甚至数万个字符，这个时候，每次都要对每条记录的所有文本进行扫描，懒判断，包不包含指定的关键词（比如说“牙膏”） 不能将搜索词拆分开来，尽可能去搜索更多的符合你的期望的结果，比如输入“生化机”，就搜索不出来“生化危机”，也就是没有分词功能。 而且用数据库来实现搜索，是不太靠谱的。通常来说，性能会很差。 什么是全文检索和Lucene？ 全文检索，倒排索引 lucene，就是一个jar包，里面包含了封装好的各种建立倒排索引，以及进行搜索的代码，包括各种算法。我们就用java开发的时候，引入lucene jar，然后基于lucene的api进行去进行开发就可以了。用lucene，我们就可以去将已有的数据建立索引，lucene会在本地磁盘上面，给我们组织索引的数据结构。另外的话，我们也可以用lucene提供的一些功能和api来针对磁盘上额 什么是Elasticsearch？ Elasticsearch功能介绍分布式的搜索引擎和数据分析引擎搜索：百度，网站的站内搜索，IT系统的检索 数据分析：电商网站，最近7天牙膏这种商品销量排名前10的商家有哪些；新闻网站，最近1个月访问量排名前3的新闻版块是哪些 全文检索，结构化检索，数据分析 全文检索：我想搜索商品名称包含牙膏的商品，select * from products where product_name like &quot;%牙膏%&quot; 结构化检索：我想搜索商品分类为日化用品的商品都有哪些，select * from products where category=&#39;日化用品&#39; 部分匹配、自动完成、搜索纠错、搜索推荐 数据分析：我们分析每一个商品分类下有多少个商品，select category_id,count(*) from products group by category_id 对海量数据进行近实时的处理 分布式：ES自动可以将海量数据分散到多台服务器上去存储和检索 海量数据的处理：分布式以后，就可以采用大量的服务器去存储和检索数据，自然而然就可以实现海量数据的处理了 近实时：如果检索数据要花费1小时（这就不叫近实时，这叫离线批处理，batch-processing）；近实时是在秒级别对数据进行搜索和分析 跟分布式/海量数据相反：lucene，单机应用，只能在单台服务器上使用，最多只能处理单台服务器可以处理的数据量 Elasticsearch的适用场景 维基百科，类似百度百科，牙膏，牙膏的维基百科，全文检索，高亮，搜索推荐 The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击，浏览，收藏，评论）+社交网络数据（对某某新闻的相关看法），数据分析，给到每篇新闻文章的作者，让他知道他的文章的公众反馈（好，坏，热门，垃圾，鄙视，崇拜） Stack Overflow（国外的程序异常讨论论坛），IT问题，程序的报错，提交上去，有人会跟你讨论和回答，全文检索，搜索相关问题和答案，程序报错了，就会将报错信息粘贴到里面去，搜索有没有对应的答案 GitHub（开源代码管理），搜索上千亿行代码 电商网站，检索商品 日志数据分析，logstash采集日志，ES进行复杂的数据分析（ELK技术，elasticsearch+logstash+kibana） 商品价格监控网站，用户设定某商品的价格阈值，当低于该阈值的时候，发送通知消息给用户，比如说订阅牙膏的监控，如果高露洁牙膏的家庭套装低于50块钱，就通知我，我就去买 BI系统，商业智能，Business Intelligence。比如说有个大型商场集团，BI，分析一下某某区域最近3年的用户消费金额的趋势以及用户群体的组成构成，产出相关的数张报表，**区，最近3年，每年消费金额呈现100%的增长，而且用户群体85%是高级白领，开一个新商场。ES执行数据分析和挖掘，Kibana进行数据可视化 国内：站内搜索（电商，招聘，门户，等等），IT系统搜索（OA，CRM，ERP，等等），数据分析（ES热门的一个使用场景） Elasticsearch的特点 可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上，服务小公司 Elasticsearch不是什么新技术，主要是将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的ES；lucene（全文检索），商用的数据分析软件（也是有的），分布式数据库（mycat） 对用户而言，是开箱即用的，非常简单，作为中小型的应用，直接3分钟部署一下ES，就可以作为生产环境的系统来使用了，数据量不大，操作不是太复杂 数据库的功能面对很多领域是不够用的（事务，还有各种联机事务型的操作）；特殊的功能，比如全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理；Elasticsearch作为传统数据库的一个补充，提供了数据库所不能提供的很多功能 ElasticSearch核心概念lucene和elasticsearch的前世今生elasticsearch，基于lucene，隐藏复杂性，提供简单易用的restful api接口、java api接口（还有其他语言的api接口） 分布式的文档存储引擎 分布式的搜索引擎和分析引擎 分布式海量数据，支持PB级数据 开箱即用，优秀的默认参数，不需要任何额外设置，完全开源 关于elasticsearch的一个传说，有一个程序员失业了，陪着自己老婆去英国伦敦学习厨师课程。程序员在失业期间想给老婆写一个菜谱搜索引擎，觉得lucene实在太复杂了，就开发了一个封装了lucene的开源项目，compass。后来程序员找到了工作，是做分布式的高性能项目的，觉得compass不够，就写了elasticsearch，让lucene变成分布式的系统。 elasticsearch的核心概念 Near Realtime（NRT）：近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级 Cluster：集群，包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常 Node：节点，集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为“elasticsearch”的集群，如果直接启动一堆节点，那么它们会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群 Document&amp;field：文档，es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。一个document里面有多个field，每个field就是一个数据字段。 Index：索引，包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。 Type：类型，每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type。 12345678910111213141516171819202122232425262728293031商品index，里面存放了所有的商品数据，商品document但是商品分很多种类，每个种类的document的field可能不太一样，比如说电器商品，可能还包含一些诸如售后时间范围这样的特殊field；生鲜商品，还包含一些诸如生鲜保质期之类的特殊fieldtype，日化商品type，电器商品type，生鲜商品type日化商品type：product_id，product_name，product_desc，category_id，category_name电器商品type：product_id，product_name，product_desc，category_id，category_name，service_period生鲜商品type：product_id，product_name，product_desc，category_id，category_name，eat_period每一个type里面，都会包含一堆document&#123; &quot;product_id&quot;: &quot;2&quot;, &quot;product_name&quot;: &quot;长虹电视机&quot;, &quot;product_desc&quot;: &quot;4k高清&quot;, &quot;category_id&quot;: &quot;3&quot;, &quot;category_name&quot;: &quot;电器&quot;, &quot;service_period&quot;: &quot;1年&quot;&#125;&#123; &quot;product_id&quot;: &quot;3&quot;, &quot;product_name&quot;: &quot;基围虾&quot;, &quot;product_desc&quot;: &quot;纯天然，冰岛产&quot;, &quot;category_id&quot;: &quot;4&quot;, &quot;category_name&quot;: &quot;生鲜&quot;, &quot;eat_period&quot;: &quot;7天&quot;&#125; shard：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个shard都是一个lucene index。 replica：任何一个服务器随时可能故障或宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据不丢失，多个replica还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认5个），replica shard（随时修改数量，默认1个），默认每个索引10个shard，5个primary shard，5个replica shard，最小的高可用配置，是2台服务器。 elasticsearch核心概念 vs. 数据库核心概念 Elasticsearch 数据库 Document 行 Type 表 Index 库]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix高级进阶]]></title>
    <url>%2F2018%2F04%2F15%2Fhystrix03%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 请求合并技术前面的两篇文章讲解了hystrix的入门，以及它的原理和执行流程。 之前我们有提到Request Cache，在一次请求上下文中，如果有多个command，参数都是一样的，调用的接口也是一样的，其实结果可以认为也是一样的。 但是如果获取多个商品，需要发送多次网络请求，调用多次接口才能拿到结果。Hystrix还为我们提供了一种叫做请求合并的技术，可以使用HystrixCollapser将多个HystrixCommand合并到一起，多个command放在一个command里面去执行，发送一次网络请求，就拉取到多条数据。用请求合并技术，将多个请求合并起来，可以减少高并发访问下需要使用的线程数量以及网络连接数量，可以提升性能。 请求合并有多种级别 global context，tomcat所有调用线程，对一个依赖服务的任何一个command调用都可以被合并在一起，hystrix就传递一个HystrixRequestContext user request context，tomcat内某一个调用线程，将某一个tomcat线程对某个依赖服务的多个command调用合并在一起 object modeling，基于对象的请求合并，如果有几百个对象，遍历后依次调用每个对象的某个方法，可能导致发起几百次网络请求，基于hystrix可以自动将对多个对象模型的调用合并到一起 请求合并技术的开销有多大使用请求合并技术的开销就是导致延迟大幅度增加，因为需要一定的时间将多个请求合并起来。比如发送过来10个请求，每个请求本来大概是2ms可以返回，要把10个请求合并在一个command内，统一一起执行，先后等待一下，可能就需要5ms（延时翻N倍了）。 所以说，要考量一下使用请求合并技术是否合适，如果一个请求本来耗费的时间就比较长，那么进行请求合并，增加一些延迟影响并不大，这样可以大幅度削减你的线程池的资源耗费，也可以减少后端服务的网络资源开销。如果一个请求本来就很快，用请求合并后反而还变慢了很多倍了，那就没有必要了。 每个请求就2ms，batch，8~10ms，延迟增加了4~5倍 每个请求本来就30ms~50ms，batch，35ms~55ms，延迟增加不太明显 实战批量查询本质上我们还是采用HystrixObservableCommand，HystrixCommand+request cache，依然每个商品发起一次网络请求。 什么意思？就是一个批量的商品过来以后，我们还是多个command的方式去执行，request collapser+request cache，相同的商品还是就查询一次，不同的商品合并到一起通过一个网络请求得到结果，我们结合之前的request cache开发。 我们需要开发合并请求的命令，还需要开发一个批量查询商品的接口。 collapser开发https://github.com/sail-y/eshop-cache-ha/blob/master/src/main/java/com/roncoo/eshop/cache/ha/hystrix/command/GetProductInfosCollapser.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * @author yangfan * @date 2018/04/15 */public class GetProductInfosCollapser extends HystrixCollapser&lt;List&lt;ProductInfo&gt;, ProductInfo, Long&gt; &#123; private Long productId; public GetProductInfosCollapser(Long productId) &#123; this.productId = productId; &#125; @Override public Long getRequestArgument() &#123; return productId; &#125; @Override protected HystrixCommand&lt;List&lt;ProductInfo&gt;&gt; createCommand(Collection&lt;CollapsedRequest&lt;ProductInfo, Long&gt;&gt; requests) &#123; String params = requests.stream().map(CollapsedRequest::getArgument).map(Object::toString).collect(Collectors.joining(",")); System.out.println("createCommand方法执行，params=" + params); return new BatchCommand(requests); &#125; @Override protected void mapResponseToRequests(List&lt;ProductInfo&gt; batchResponse, Collection&lt;CollapsedRequest&lt;ProductInfo, Long&gt;&gt; requests) &#123; int count = 0; for (CollapsedRequest&lt;ProductInfo, Long&gt; request : requests) &#123; request.setResponse(batchResponse.get(count++)); &#125; &#125; private static final class BatchCommand extends HystrixCommand&lt;List&lt;ProductInfo&gt;&gt; &#123; public final Collection&lt;CollapsedRequest&lt;ProductInfo, Long&gt;&gt; requests; public BatchCommand(Collection&lt;CollapsedRequest&lt;ProductInfo, Long&gt;&gt; requests) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ProductInfoService")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetProductInfosCollapserBatchCommand"))); this.requests = requests; &#125; @Override protected List&lt;ProductInfo&gt; run() throws Exception &#123; // 将一个批次内的商品id给拼接到了一起 String params = requests.stream().map(CollapsedRequest::getArgument).map(Object::toString).collect(Collectors.joining(",")); // 将多个商品id合并到一个batch内，直接发送一次网络请求，获取所有的商品 String url = "http://localhost:8082/getProductInfos?productIds=" + params; String response = HttpUtil.get(url); List&lt;ProductInfo&gt; productInfos = JSONArray.parseArray(response, ProductInfo.class); for (ProductInfo productInfo : productInfos) &#123; System.out.println("BatchCommand内部， productInfo=" + JSON.toJSONString(productInfo)); &#125; return productInfos; &#125; &#125;&#125; 批量查询接口开发https://github.com/sail-y/eshop-product-ha/blob/master/src/main/java/com/roncoo/esjop/product/ha/controller/ProductController.java 123456789101112131415@GetMapping("/getProductInfos")public String getProductInfos(String productIds) &#123; System.out.println("getProductInfos接收到一次请求，productIds=" + productIds); JSONArray jsonArray = new JSONArray(); for (String productId : productIds.split(",")) &#123; String json = "&#123;\"id\": " + productId + ", \"name\": \"iphone7手机\", \"price\": 5599, \"pictureList\":\"a.jpg,b.jpg\", \"specification\": \"iphone7的规格\", \"service\": \"iphone7的售后服务\", \"color\": \"红色,白色,黑色\", \"size\": \"5.5\", \"shopId\": 2, \"modifiedTime\": \"2018-02-21 21:11:34\", \"cityId\": 1&#125;"; jsonArray.add(JSON.parse(json)); &#125; return jsonArray.toJSONString();&#125; 测试代码1234567891011/** * * 请求合并测试 * */public class RequestCollapserTest &#123; public static void main(String[] args) &#123; HttpUtil.get("http://localhost:8081/getProductInfos?productIds=1,2,3,4,5,6,7"); &#125;&#125; CacheController#getProductInfos 12345678910111213List&lt;Future&lt;ProductInfo&gt;&gt; futures = new ArrayList&lt;&gt;();for (String productId : productIds.split(",")) &#123; GetProductInfosCollapser getProductInfosCollapser = new GetProductInfosCollapser(Long.valueOf(productId)); futures.add(getProductInfosCollapser.queue());&#125;for (Future&lt;ProductInfo&gt; future : futures) &#123; try &#123; System.out.println("CacheController结果：" + future.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125;&#125; 把所有需要查询的商品通过HystrixCollapser发送，HystrixCollapser会为自动为我们讲请求合并以后访问。可能第一次访问的时候会超时，因为开发环境项目刚启动，第一次访问比较慢，第二次就好了。 输出结果 123456789101112131415createCommand方法执行，params=1,2,3,4,5,6,7BatchCommand内部， productInfo=&#123;&quot;cityId&quot;:1,&quot;color&quot;:&quot;红色,白色,黑色&quot;,&quot;id&quot;:1,&quot;modifiedTime&quot;:&quot;2018-02-21 21:11:34&quot;,&quot;name&quot;:&quot;iphone7手机&quot;,&quot;pictureList&quot;:&quot;a.jpg,b.jpg&quot;,&quot;price&quot;:5599.0,&quot;service&quot;:&quot;iphone7的售后服务&quot;,&quot;shopId&quot;:2,&quot;size&quot;:&quot;5.5&quot;,&quot;specification&quot;:&quot;iphone7的规格&quot;&#125;BatchCommand内部， productInfo=&#123;&quot;cityId&quot;:1,&quot;color&quot;:&quot;红色,白色,黑色&quot;,&quot;id&quot;:2,&quot;modifiedTime&quot;:&quot;2018-02-21 21:11:34&quot;,&quot;name&quot;:&quot;iphone7手机&quot;,&quot;pictureList&quot;:&quot;a.jpg,b.jpg&quot;,&quot;price&quot;:5599.0,&quot;service&quot;:&quot;iphone7的售后服务&quot;,&quot;shopId&quot;:2,&quot;size&quot;:&quot;5.5&quot;,&quot;specification&quot;:&quot;iphone7的规格&quot;&#125;BatchCommand内部， productInfo=&#123;&quot;cityId&quot;:1,&quot;color&quot;:&quot;红色,白色,黑色&quot;,&quot;id&quot;:3,&quot;modifiedTime&quot;:&quot;2018-02-21 21:11:34&quot;,&quot;name&quot;:&quot;iphone7手机&quot;,&quot;pictureList&quot;:&quot;a.jpg,b.jpg&quot;,&quot;price&quot;:5599.0,&quot;service&quot;:&quot;iphone7的售后服务&quot;,&quot;shopId&quot;:2,&quot;size&quot;:&quot;5.5&quot;,&quot;specification&quot;:&quot;iphone7的规格&quot;&#125;BatchCommand内部， productInfo=&#123;&quot;cityId&quot;:1,&quot;color&quot;:&quot;红色,白色,黑色&quot;,&quot;id&quot;:4,&quot;modifiedTime&quot;:&quot;2018-02-21 21:11:34&quot;,&quot;name&quot;:&quot;iphone7手机&quot;,&quot;pictureList&quot;:&quot;a.jpg,b.jpg&quot;,&quot;price&quot;:5599.0,&quot;service&quot;:&quot;iphone7的售后服务&quot;,&quot;shopId&quot;:2,&quot;size&quot;:&quot;5.5&quot;,&quot;specification&quot;:&quot;iphone7的规格&quot;&#125;BatchCommand内部， productInfo=&#123;&quot;cityId&quot;:1,&quot;color&quot;:&quot;红色,白色,黑色&quot;,&quot;id&quot;:5,&quot;modifiedTime&quot;:&quot;2018-02-21 21:11:34&quot;,&quot;name&quot;:&quot;iphone7手机&quot;,&quot;pictureList&quot;:&quot;a.jpg,b.jpg&quot;,&quot;price&quot;:5599.0,&quot;service&quot;:&quot;iphone7的售后服务&quot;,&quot;shopId&quot;:2,&quot;size&quot;:&quot;5.5&quot;,&quot;specification&quot;:&quot;iphone7的规格&quot;&#125;BatchCommand内部， productInfo=&#123;&quot;cityId&quot;:1,&quot;color&quot;:&quot;红色,白色,黑色&quot;,&quot;id&quot;:6,&quot;modifiedTime&quot;:&quot;2018-02-21 21:11:34&quot;,&quot;name&quot;:&quot;iphone7手机&quot;,&quot;pictureList&quot;:&quot;a.jpg,b.jpg&quot;,&quot;price&quot;:5599.0,&quot;service&quot;:&quot;iphone7的售后服务&quot;,&quot;shopId&quot;:2,&quot;size&quot;:&quot;5.5&quot;,&quot;specification&quot;:&quot;iphone7的规格&quot;&#125;BatchCommand内部， productInfo=&#123;&quot;cityId&quot;:1,&quot;color&quot;:&quot;红色,白色,黑色&quot;,&quot;id&quot;:7,&quot;modifiedTime&quot;:&quot;2018-02-21 21:11:34&quot;,&quot;name&quot;:&quot;iphone7手机&quot;,&quot;pictureList&quot;:&quot;a.jpg,b.jpg&quot;,&quot;price&quot;:5599.0,&quot;service&quot;:&quot;iphone7的售后服务&quot;,&quot;shopId&quot;:2,&quot;size&quot;:&quot;5.5&quot;,&quot;specification&quot;:&quot;iphone7的规格&quot;&#125;CacheController结果：ProductInfo(id=1, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=2, modifiedTime=2018-02-21 21:11:34, cityId=1, cityName=null, brandId=null, brandName=null)CacheController结果：ProductInfo(id=2, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=2, modifiedTime=2018-02-21 21:11:34, cityId=1, cityName=null, brandId=null, brandName=null)CacheController结果：ProductInfo(id=3, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=2, modifiedTime=2018-02-21 21:11:34, cityId=1, cityName=null, brandId=null, brandName=null)CacheController结果：ProductInfo(id=4, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=2, modifiedTime=2018-02-21 21:11:34, cityId=1, cityName=null, brandId=null, brandName=null)CacheController结果：ProductInfo(id=5, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=2, modifiedTime=2018-02-21 21:11:34, cityId=1, cityName=null, brandId=null, brandName=null)CacheController结果：ProductInfo(id=6, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=2, modifiedTime=2018-02-21 21:11:34, cityId=1, cityName=null, brandId=null, brandName=null)CacheController结果：ProductInfo(id=7, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=2, modifiedTime=2018-02-21 21:11:34, cityId=1, cityName=null, brandId=null, brandName=null) 配置项HystrixCollapser提供了一些配置： maxRequestsInBatch 控制一个Batch中最多允许多少个request被合并，然后才会触发一个batch的执行 timerDelayInMilliseconds 控制一个batch创建之后，多长时间以后就自动触发batch的执行，默认是10毫秒 fail-fast和fail-silentHystrixCommand如果命令执行执行中出错了，或者抛异常了它有两种方式后续逻辑： fail-fast，就是不给fallback降级逻辑，HystrixCommand.run()，会直接从Hystrix的线程池中抛出异常，打印出日志，无法在调用方捕获 fail-silent，给一个fallback降级逻辑，如果HystrixCommand.run()，报错了，会走fallback降级，但是降级逻辑返回一个null值 很少会用fail-fast模式，比较常用的可能还是fail-silent，不过既然都到了fallback里面，肯定要做点降级的事情。 stubbed fallbackstubbed fallback: 残缺的降级 用请求中的部分数据拼装成结果，然后再填充一些默认值返回。比如说你发起了一个请求，然后请求中可能本身就附带了一些信息，如果主请求失败了，走到降级逻辑。在降级逻辑里面，可以将这个请求中的数据，以及部分本地缓存有的数据拼装在一起，再给数据填充一些简单的默认值然后尽可能将自己有的数据返回到请求方。 多级降级先降一级，尝试用一个备用方案去执行，如果备用方案失败了，再用最后下一个备用方案去执行。 hystrix command fallback语义，很容易就可以实现多级降级的策略，command嵌套command就可以达到多级降级的效果，第二个command其实是第一级降级策略。常见的多级降级的做法有一个操作，如果访问MySQL数据库，mysql数据库访问报错，降级，去redis中获取数据，如果说redis又挂了，然后就去从本地ehcache缓存中获取数据。 伪代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Overrideprotected ProductInfo getFallback() &#123; return new FirstLevelFallbackCommand(productId).execute();&#125;private static class FirstLevelFallbackCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; public FirstLevelFallbackCommand(Long productId) &#123; // 第一级的降级策略，因为这个command是运行在fallback中的 // 所以至关重要的一点是，在做多级降级的时候，要将降级command的线程池单独做一个出来 // 如果主流程的command都失败了，可能线程池都已经被占满了 // 降级command必须用自己的独立的线程池 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ProductInfoService")) .andCommandKey(HystrixCommandKey.Factory.asKey("FirstLevelFallbackCommand")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey("FirstLevelFallbackPool")) ); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; // 这里，因为是第一级降级的策略，所以说呢，其实是要从备用机房的机器去调用接口 // 但是，我们这里没有所谓的备用机房，所以说还是调用同一个服务来模拟 if(productId.equals(-2L)) &#123; throw new Exception(); &#125; String url = "http://127.0.0.1:8082/getProductInfo?productId=" + productId; String response = HttpClientUtils.sendGetRequest(url); return JSONObject.parseObject(response, ProductInfo.class); &#125; @Override protected ProductInfo getFallback() &#123; // 第二级降级策略，第一级降级策略，都失败了 ProductInfo productInfo = new ProductInfo(); // 从请求参数中获取到的唯一条数据 productInfo.setId(productId); // 从本地缓存中获取一些数据 productInfo.setBrandId(BrandCache.getBrandId(productId)); productInfo.setBrandName(BrandCache.getBrandName(productInfo.getBrandId())); productInfo.setCityId(LocationCache.getCityId(productId)); productInfo.setCityName(LocationCache.getCityName(productInfo.getCityId())); // 手动填充一些默认的数据 productInfo.setColor("默认颜色"); productInfo.setModifiedTime(new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date())); productInfo.setName("默认商品"); productInfo.setPictureList("default.jpg"); productInfo.setPrice(0.0); productInfo.setService("默认售后服务"); productInfo.setShopId(-1L); productInfo.setSize("默认大小"); productInfo.setSpecification("默认规格"); return productInfo; &#125; &#125; 手动降级 手动降级实现方式是写一个command，在这个command它的主流程中，根据一个标识位，判断要执行哪个流程，可以执行主流程，也可以执行一个备用降级的command。 它的使用场景：一般来说都是去执行一个主流程的command，如果说你现在知道主流程有问题了，希望能够手动降级的话，动态给服务发送个请求。在请求中修改标识位，自动就让command以后都直接过来执行备用command。 一般会嵌套3个command，套在最外面的command，是用semaphore信号量做限流和资源隔离的，因为这个command不用去care timeout的问题，嵌套调用的command会自己去管理timeout超时的 代码片段如下，通过IsDegrade.isDegrade()可以设置是否手动降级。 1234567891011121314151617181920212223242526272829303132/** * @author yangfan * @date 2018/04/15 */public class GetProductInfoFacadeCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; public GetProductInfoFacadeCommand(Long productId) &#123; super(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ProductInfoService")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetProductInfoFacadeCommand")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE) .withExecutionIsolationSemaphoreMaxConcurrentRequests(15))); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; if(!IsDegrade.isDegrade()) &#123; return new GetProductInfoCommand(productId).execute(); &#125; else &#123; return new GetProductInfoFromMySQLCommand(productId).execute(); &#125; &#125; @Override protected ProductInfo getFallback() &#123; return new ProductInfo(); &#125;&#125; 总结看了这么多Hystrix的配置和使用方式，我们在生产环境里最需要关注的点是什么？ 线程池大小设置 timeout时长设置 这个配置也没有说固定是多少，但是有一些规律可循。一般一开始需要将一些关键配置设置的大一些，比如timeout超时时长，线程池大小，或信号量容量。然后逐渐优化这些配置，直到在一个生产系统中运作良好。 一开始先不要设置timeout超时时长，默认就是1000ms，也就是1s 一开始也不要设置线程池大小，默认就是10 直接部署hystrix到生产环境，如果运行的很良好，那么就让它这样运行好了 让hystrix应用，24小时运行在生产环境中 依赖标准的监控和报警机制来捕获到系统的异常运行情况 在24小时之后，看一下调用延迟的占比，以及流量，来计算出让短路器生效的最小的配置数字 直接对hystrix配置进行热修改，然后继续在hystrix dashboard上监控 看看修改配置后的系统表现有没有改善 最佳方案： 线程池大小：假设一个请求200ms，QPS30。那么每秒的高峰访问次数 * 99%的访问延时 + buffer = 30 * 0.2 + 4 = 10线程，10个线程每秒处理30次访问应该足够了，每个线程处理3次访问。 timeout：合理的timeout设置应该为300ms，也就是99.5%的访问延时，计算方法是，因为判断每次访问延时最多在250ms（TP99如果是200ms的话），再加一次重试时间50ms，就是300ms，感觉也应该足够了 如果线程池设置得比较死，那么如果某个服务高峰期来了线程不够用，别的服务又占着线程池不用，这样就很不合理了，所以Hystrix也为我们提供了动态调整线程池的方案。 coreSize 设置线程池的大小，默认是10 HystrixThreadPoolProperties.Setter() .withCoreSize(int value) maximumSize 设置线程池的最大大小，只有在设置allowMaximumSizeToDivergeFromCoreSize的时候才能生效 默认是10 HystrixThreadPoolProperties.Setter().withMaximumSize(int value) keepAliveTimeMinutes 设置保持存活的时间，单位是分钟，默认是1 如果设置allowMaximumSizeToDivergeFromCoreSize为true，那么coreSize就不等于maxSize，此时线程池大小是可以动态调整的，可以获取新的线程，也可以释放一些线程 如果coreSize &lt; maxSize，那么这个参数就设置了一个线程多长时间空闲之后，就会被释放掉 HystrixThreadPoolProperties.Setter().withKeepAliveTimeMinutes(int value) allowMaximumSizeToDivergeFromCoreSize 允许线程池大小自动动态调整，设置为true之后，maxSize就生效了，此时如果一开始是coreSize个线程，随着并发量上来，那么就会自动获取新的线程，但是如果线程在keepAliveTimeMinutes内空闲，就会被自动释放掉 默认是false HystrixThreadPoolProperties.Setter().withAllowMaximumSizeToDivergeFromCoreSize(boolean value)]]></content>
      <categories>
        <category>hystrix</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud 不停机发布服务(0-downtime Blue/Green deployments)]]></title>
    <url>%2F2018%2F04%2F14%2Fspring-cloud-gray%2F</url>
    <content type="text"><![CDATA[背景项目初期由于BUG和需求改动可能都会比较多，我们会很频繁的发布我们的应用。但是如果不进行处理，在升级的过程中会导致用户服务中断。 通常我们需要发布的内容如下： 某一个服务BUG紧急修复。 某一个服务新的需求上线。 实际上针对这两种情况，在传统的应用中我们是很容易做到不停机升级的。例如nginx负载均衡2台tomcat实例，在升级的时候切断其中一台访问，升级完成以后切换流量，再升级另外一台。但是我这里用的是Spring Cloud，所有的实例状态都维护在Eureka中，Eureka本身也提供了很多保护机制，所以你的服务在down掉的时候，不会立马从服务列表中剔除掉。具体的配置项可以周立老师一篇文章里查看：如何解决Eureka Server不踢出已关停的节点的问题。 所以如果我们想要做到不停机去升级/发布一个服务，需要我们从Spring Cloud架构本身上着手去进行一些改造。我们需要去了解Eureka的使用方式，Spring Retry的使用，Spring Cloud的负载均衡规则等等，最终达到这个目的。 思路如果一个不了解Spring Cloud的人来做这种不停机发布，比如运维部门的同事。他会将某个需要升级的实例新版本启动起来，然后将老版本的进程杀掉。但是因为Spring Cloud的特性，被干掉的实例并没有被踢出服务列表，客户端仍然会访问到一个不存在的实例，直接返回500错误。可能需要等1~2分钟以后才能恢复正常。 我们知道这个是因为Eureka的机制问题，但是它注定不可能做成实时感知上下线的。Eureka是通过定期扫描去下线已经down掉的服务，不过他的默认时间是60秒，我们可以优化这个配置，让它能比较快的感知到服务已经下线。 关于Eureka的常见问题 问题可以参考：Spring Cloud中，Eureka常见问题总结 生产环境最佳配置：eureka缓存细节以及生产环境的最佳配置 中小规模生产环境参考配置： Eureka Server 1234567891011eureka: server: enable-self-preservation: false # 中小规模下，自我保护模式坑比好处多，所以关闭它 eviction-interval-timer-in-ms: 5000 # 续期时间，即扫描失效服务的间隔时间（缺省为60*1000ms）从服务列表中剔除 use-read-only-response-cache: false # 禁用readOnlyCacheMap instance: lease-renewal-interval-in-seconds: 5 # 心跳时间，即服务续约间隔时间（缺省为30s） lease-expiration-duration-in-seconds: 10 # 没有心跳的淘汰时间，10秒，即服务续约到期时间（缺省为90s） client: service-url: defaultZone: $&#123;defaultZone:http://peer2:8760/eureka/&#125; Eureka Client 1234567891011eureka: instance: lease-renewal-interval-in-seconds: 5 # 心跳时间，即服务续约间隔时间（缺省为30s） lease-expiration-duration-in-seconds: 10 # 没有心跳的淘汰时间，10秒，即服务续约到期时间（缺省为90s） client: # 向注册中心注册 fetch-registry: true # 服务清单的缓存更新时间，默认30秒一次 registry-fetch-interval-seconds: 5 service-url: defaultZone: $&#123;defaultZone:http://$&#123;DISCOVERY_URL:discovery&#125;:8761/eureka/&#125; 通过优化Eureka配置，服务在启动后能够较快的被使用上，Eureka也能较快的感知到服务以及下线并踢出服务列表。 巧用Spring Retry重试机制我在搜寻解决方案的时候，也看到了Github上讨论的一个issue：Best practices for using Eureka for 0-downtime Blue/Green deployments #1290. 这里面讨论了利用重试机制去实现不停机发布的一种方式。前面的Eureka配置已经缩短了服务上线和服务下线的时间，但是这中间仍然一段延迟，可能还是会有请求随机访问一个不存在的服务实例上。 重试机制的原理就是利用Spring Cloud提供的重试机制在请求访问出现错误的时候自动重试当前实例或者其他实例，而不是直接返回错误。 主要配置如下： 12345678910111213ribbon: # ribbon缓存时间 ServerListRefreshInterval: 2000 ReadTimeout: 30000 ConnectTimeout: 30000 # 是否所有操作都重试 # OkToRetryOnAllOperations: true # 重试负载均衡其他的实例最大重试次数,不包括首次server MaxAutoRetriesNextServer: 0 # 同一台实例最大重试次数,不包括首次调用 MaxAutoRetries: 0zuul: retryable: true 但是这里要注意一点，OkToRetryOnAllOperations如果设置为true，那么ribbon超时时间最好设置长一点，否则post等请求如果超时会被提交多次，还要注意hystrix的超时时间要大于ribbion的超时时间，否则hystrix会先超时。 1234567hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 40000 在不同的版本中，Spring Cloud的重试机制是比较混乱的，周立老师对重试机制的详细解释：http://www.itmuch.com/spring-cloud-sum/spring-cloud-retry/ Feign本身也具备重试能力，在早期的Spring Cloud中，Feign使用的是 feign.Retryer.Default#Default() ，重试5次。但Feign整合了Ribbon，Ribbon也有重试的能力，此时，就可能会导致行为的混乱。 Spring Cloud意识到了此问题，因此做了改进，将Feign的重试改为 feign.Retryer#NEVER_RETRY ，如需使用Feign的重试，只需使用Ribbon的重试配置即可。因此，对于Camden以及以后的版本，Feign的重试可使用如下属性进行配置： 1234ribbon: MaxAutoRetries: 1 MaxAutoRetriesNextServer: 2 OkToRetryOnAllOperations: false 相关Issue可参考：https://github.com/spring-cloud/spring-cloud-netflix/issues/467 结合之前对Eureka配置的优化，我们就可以愉快的进行测试了，开启2个服务访问几次，可以发现随机访问。然后干掉一个服务，再次访问，依然没有问题，不会出现500等情况。Feign自动为我们选择了另外可用的服务发送了重试请求。 灰度发布方案还有一种特别的需求，我们除了想做到不停机发布，可能还需要做到某些用户测试新版本代码，实现降级、限流、滚动、灰度、AB、金丝雀等操作。我在Github上发现了一个开源的代码在一定程度上提供了很好的思路去做这个事情。地址：https://github.com/JeromeLiuLly/springcloud-gray 看这个实现方式可以看出来，他的方案是基于spring cloud 实践-降级、限流、滚动、灰度、AB、金丝雀等等等等的方案做的。 因Spring Cloud都是客户端负载均衡，会从Eureka读取服务列表，然后通过一定的负载均衡规则来选择请求的服务器。这个方案就是重写了Ribbon负载均衡的策略，将一些自定义信息放入了Eureka的metdata-map中，在路由的时候根据这些信息来选择服务。我这里不再多说，大家可以自行去查看他们的文章和代码。 这个方案灵活性非常大，你可以根据自定义的信息来构建任何你想做的策略，去实现AB Test等等功能，甚至我在开发环境中也能使用。举个例子，因为我们的服务太多了，如果在本机开发的时候，关联的服务较多，要启动比较多的服务才能够进行开发和测试，可能机器会有点吃不消。我基于上述方案让开发的同学们在启动服务的上将本机的IP添加到metadata-map中，这样我在路由的时候判断客户端请求过来的IP是多少，如果跟实例里的信息匹配，那么所有来自这个IP请求就转发到开发同学启动的那台实例上。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
        <tag>灰度发布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)eureka缓存细节以及生产环境的最佳配置]]></title>
    <url>%2F2018%2F04%2F14%2Fspring-cloud-eureka%2F</url>
    <content type="text"><![CDATA[本文转自 eureka缓存细节以及生产环境的最佳配置 作者：http://bhsc881114.github.io/ eureka作为spring cloud微服务架构里的注册中心，是非常核心的一个组件，它避免了复杂的选主算法，架构比较简单，搭个demo也确实很快，但是如果要用于生产环境，还是得注意很多东西，尤其是下线延迟… 服务获取中的缓存问题本节的内容都是从这个issue翻译的：Documentation: changing Eureka renewal frequency WILL break the self-preservation feature of the server 为什么修改client的默认心跳时间，会导致自我保护模式失效？Eureka Service会认为客户端是以30s的频率来发送心跳的。服务端期望收到的最大心跳时间是：n instances x 2(60s/30s) x threshold 如果是2个实例，Eureka会期望每分钟有：2 instances x 2 x 85% =3.4个心跳,也就是说需要3个心跳。如果client的心跳改成15s，挂掉一个，另一个在1min内会发出4个心跳，而这时候的阈值还是3.4个，自我保护模式就失效了。核心原因就是在Eureka Server计算期望心跳数的时候写死了每分钟的心跳间隔，即30秒，所以他永远会是*2(感觉像是新手写的代码啊啊啊 -_-) 还有一个参数可以调整，eureka.server.renewalThresholdUpdateIntervalMs，心跳阈值重新计算的周期，默认15分钟，可以改短一点，2min 客户端首次注册时间为什么要30s？如何改进？首次注册行为是和首次心跳绑定在一起的，首次心跳发送以后会收到not found的响应,client就知道还没注册过，client就会马上注册。首次心跳由参数eureka.instance.leaseRenewalIntervalInSeconds控制的，默认30 可以通过eureka.client.initialInstanceInfoReplicationIntervalSeconds参数来加快首次注册的速度。他是控制首次改变实例状态（UP/DOWN ）的时间，启动的时候状态肯定是需要改变的，所以他可以用来加快首次注册速度，并且改变这个值不会影响到保护模式 另外如果你使用的是spring cloud eureka的话没首次注册延迟的问题，他会马上注册 其他影响快速获取服务信息的因素【服务端缓存】 因为服务端默认会有个read only response cache（下面会细说），每30秒更新一次(eureka.server.response-cache-update-interval-ms),所以可能注册了不是马上能看到（虽然通过rest api不能看到，但是你可以在web ui上看到，因为ui没有缓存） 【客户端缓存】 Eureka Client缓存的定期更新周期，他由eureka.client.registryFetchIntervalSeconds控制，默认30秒， 改成5秒 【Ribbon缓存】 如果你采用Ribbon来访问服务，那么这里会有个缓存（他的数据来源是本地Eureka Client缓存），他由ribbon. ServerListRefreshInterval控制，默认30秒， 改成2秒 怎么更快的踢掉没有心跳的机器eureka.instance.leaseExpirationDurationInSeconds，这个值用来控制多久踢掉机器，默认是3个心跳周期，有点久，可以考虑改成2个，他不会影响到保护模式（如果开启自我保护模式，心跳间隔因为上面的bug不能改，只能改这个了 -_-） 服务端缓存细节Eureka内部的缓存分很多级，主要有registry、readWriterCacheMap、readOnlyCacheMap；另外还有一个维护最近180s增量的队列recentlyChangedQueue 写操作包括注册、取消注册等，都直接操作在registry上，同时也会更新recentlyChangedQueue和readWriterCacheMap 读操作读默认是从readOnlyCacheMap读取，读不到的话再从readWriterCacheMap，还没有再从registry 滥用缓存的读操作这个读操作的三级缓存结构，非常让人困惑，registry已经是ConcurrentHashMap，纯内存操作，性能非常高了，为什么前面还要加两级缓存；readWriterCacheMap的数据是在写入以后responseCacheAutoExpirationInSeconds(默认180)秒内失效，readOnlyCacheMap则是一个定时任务，每responseCacheUpdateIntervalMs(默认30)秒从readWriterCacheMap获取最新数据 去掉readOnlyCacheMap从CAP理论上看，Eureka是一个AP系统，但是在C层面这么弱，就是因为各种无谓的缓存造成的，看了下readWriterCacheMap去掉比较难，但是readOnlyCacheMap有一个开关useReadOnlyResponseCache，果断关掉！！ Time Lag最后再来看下Eureka wiki中提到的2min time lag问题，其实分多个角度看，不一定是2min 服务正常上线/修改，最大可能会有120s滞后12- 30(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=120- 如果是在Spring Cloud环境下使用这些组件(Eureka, Ribbon)，不会有首次注册30秒延迟的问题，服务启动后会马上注册,所以从注册到发现，最多可能是90s。 服务异常下线：最大可能会有270s滞后1234- 定时清理任务每eureka.server. evictionIntervalTimerInMs(默认60)执行一次清理任务- 每次清理任务会把90秒(3个心跳周期，eureka.instance.leaseExpirationDurationInSeconds)没收到心跳的踢除，但是根据官方的说法 ，因为代码实现的bug，这个时间其实是两倍，即180秒，也就是说如果一个客户端因为网络问题或者主机问题异常下线，可能会在180秒后才剔除- 读取端，因为readOnlyCacheMap以及客户端缓存的存在，可能会在30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=90- 所以极端情况最终可能会是180+90=270 生产环境最佳配置总结前面3点，经过梳理后，推荐的生产环境最佳配置如下：（可用于中小规模环境）： Eureka Server端配置123456789101112131415## 中小规模下，自我保护模式坑比好处多，所以关闭它eureka.server.enableSelfPreservation=false## 心跳阈值计算周期，如果开启自我保护模式，可以改一下这个配置## eureka.server.renewalThresholdUpdateIntervalMs=120000## 主动失效检测间隔,配置成5秒eureka.server.evictionIntervalTimerInMs=5000## 心跳间隔，5秒eureka.instance.leaseRenewalIntervalInSeconds=5## 没有心跳的淘汰时间，10秒eureka.instance.leaseExpirationDurationInSeconds=10## 禁用readOnlyCacheMapeureka.server. useReadOnlyResponseCache=false 服务提供者和client配置123456789## 心跳间隔，5秒eureka.instance.leaseRenewalIntervalInSeconds=5## 没有心跳的淘汰时间，10秒eureka.instance.leaseExpirationDurationInSeconds=10# 定时刷新本地缓存时间eureka.client.registryFetchIntervalSeconds=5# ribbon缓存时间ribbon.ServerListRefreshInterval=2000 改成上面配置后: 正常上线下线客户端最大感知时间：eureka.client.registryFetchIntervalSeconds+ribbon. ServerListRefreshInterval = 7秒 异常下线客户端最大感知时间：2*eureka.instance.leaseExpirationDurationInSeconds+eureka.server.evictionIntervalTimerInMs+eureka.client.registryFetchIntervalSeconds+ribbon. ServerListRefreshInterval = 32]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)Spring Cloud 实践-降级、限流、滚动、灰度、AB、金丝雀等等等等]]></title>
    <url>%2F2018%2F04%2F14%2Fspring-cloud-green-blue%2F</url>
    <content type="text"><![CDATA[spring cloud 实践源码地址：https://github.com/charlesvhe/spring-cloud-practice 项目结构config 配置中心端口：8888，方便起见直接读取配置文件，生产环境可以读取git。application-dev.properties为全局配置。先启动配置中心，所有服务的配置（包括注册中心的地址）均从配置中心读取。 consumer 服务消费者端口：18090，调用服务提供者，为了演示header传递。 core 框架核心包核心jar包，所有微服务均引用该包，使用AutoConfig实现免配置，模拟生产环境下spring-cloud的使用。 eureka 注册中心端口：8761，/metadata端点实现metadata信息配置。 provider 服务提供者端口：18090，服务提供者，无特殊逻辑。 zuul 网关端口：8080，演示解析token获得label并放入header往后传递 实践：降级、限流、滚动、灰度、AB、金丝雀等等等等我本人是从dubbo转过来的，经常看到社区里面拿dubbo和spring-cloud做对比，一对比就提到dubbo所谓的降级、限流功能。spring-cloud默认没有这个能力，让我们来扩展spring-cloud，使她具备比dubbo更牛逼的各种能力。 所谓的降级、限流、滚动、灰度、AB、金丝雀等等等等，在我看来无非就是扩展了服务路由能力而已。这里说的服务降级，说的是服务A部署多个实例，实例级别的降级限流。如果要做整个服务A的降级，直接采用docker自动扩容缩容即可。 我们先来看应用场景： 服务A 发布了1.0版，部署了3个实例A1、A2、A3，现在要对服务A进行升级，由1.0升级到2.0。先将A1服务流量关闭，使A2、A3负担；升级A1代码版本到2.0；将A1流量调整为1%，观察新版本运行情况，如果运行稳定，则逐步提升流量5%、10%直到完全放开流量控制。A2、A3重复上述步骤。 在上述步骤中，我们想让特别的人使用2.0，其他人还是使用1.0版，稳定后再全员开放。 我们想不依赖sleuth做链路跟踪，想自己实现一套基于ELK的链路跟踪。 我们还有各种千奇百怪的想法。。。 实现思路要实现这些想法，我们需要对spring-cloud的各个组件、数据流非常熟悉，这样才能知道该在哪里做扩展。一个典型的调用：外网-》Zuul网关-》服务A-》服务B。。。 spring-cloud跟dubbo一样都是客户端负载均衡，所有调用均由Ribbon来做负载均衡选择服务器，所有调用前后会套一层hystrix做隔离、熔断。服务间调用均用带LoadBalanced注解的RestTemplate发出。RestTemplate-》Ribbon-》hystrix 通过上述分析我们可以看到，我们的扩展点就在Ribbon，Ribbon根据我们的规则，选择正确的服务器即可。 我们先来一个dubbo自带的功能：基于权重的流量控制。dubbo自带的控制台可以设置服务实例粒度的半权，倍权。其实就是在客户端负载均衡时，选择服务器带上权重即可，spring-cloud默认是ZoneAvoidanceRule，优先选择相同Zone下的实例，实例间采用轮询方式做负载均衡。我们的想把基于轮询改为基于权重即可。接下来的问题是，每个实例的权重信息保存在哪里？从哪里取？dubbo放在zookeeper中，spring-cloud放在eureka中。我们只需从eureka拿每个实例的权重信息，然后根据权重来选择服务器即可。具体代码LabelAndWeightMetadataRule（先忽略里面的优先匹配label相关代码）。 放入核心框架LabelAndWeightMetadataRule写好了，那么我们如何使用它，使之生效呢？有3种方式。 1）写个AutoConfig将LabelAndWeightMetadataRule声明成@Bean，用来替换默认的ZoneAvoidanceRule。这种方式在技术验证、开发测试阶段使用短平快。但是这种方式是强制全局设置，无法个性化。 2）由于spring-cloud的Ribbon并没有实现netflix Ribbon的所有配置项。netflix配置全局rule方式为：ribbon.NFLoadBalancerRuleClassName=package.YourRule，spring-cloud并不支持，spring-cloud直接到服务粒度，只支持SERVICE_ID.ribbon.NFLoadBalancerRuleClassName=package.YourRule。我们可以扩展org.springframework.cloud.netflix.ribbon.PropertiesFactory修正spring cloud ribbon未能完全支持netflix ribbon配置的问题。这样我们可以将全局配置写到配置中心的application-dev.properties全局配置中，然后各个微服务还可以根据自身情况做个性化定制。但是PropertiesFactory属性均为私有，应该是spring cloud不建议在此扩展。参见https://github.com/spring-cloud/spring-cloud-netflix/issues/1741。 3）使用spring cloud官方建议的@RibbonClient方式。该方式仅存在于spring-cloud单元测试中（在我提问后，现在还存在于spring-cloud issue list）。具体代码参见DefaultRibbonConfiguration、CoreAutoConfiguration。 实际测试依次开启 config eureka provide（开两个实例，通过启动参数server.port指定不同端口区分） consumer zuul 访问 http://localhost:8761/metadata.html 这是我手写的一个简单的metadata管理界面，分别设置两个provider实例的weight值（设置完需要一段2分钟才能生效），然后访问 http://localhost:8080/provider/user 多刷几次来测试zuul是否按权重发送请求，也可以访问 http://localhost:8080/consumer/test 多刷几次来测试consumer是否按权重来调用provide服务。 进阶，基于标签基于权重的搞定之后，接下来才是重头戏：基于标签的路由。入口请求含有各种标签，然后我们可以根据标签幻化出各种各样的路由规则。例如只有标注为粉丝的用户才使用新版本（灰度、AB、金丝雀），例如标注为中国的用户请求必须发送到中国的服务器（全球部署），例如标注为写的请求必须发送到专门的写服务实例（读写分离），等等等等，唯一限制你的就是你的想象力。 实现思路根据标签的控制，我们当然放到之前写的Ribbon的rule中，每个实例配置的不同规则也是跟之前一样放到注册中心的metadata中，关键是标签数据如何传过来。权重随机的实现思路里面有答案，请求都通过zuul进来，因此我们可以在zuul里面给请求打标签，基于用户，IP或其他看你的需求，然后将标签信息放入ThreadLocal中，然后在Ribbon Rule中从ThreadLocal拿出来使用就可以了。然而，按照这个方式去实验时，发现有问题，拿不到ThreadLocal。原因是有hystrix这个东西，回忆下hystrix的原理，为了做到故障隔离，hystrix启用了自己的线程，不在同一个线程ThreadLocal失效。那么还有什么办法能够将标签信息一传到底呢，想想之前有没有人实现过类似的东西，没错sleuth，他的链路跟踪就能够将spam传递下去，翻翻sleuth源码，找找其他资料，发现可以使用HystrixRequestVariableDefault，这里不建议直接使用HystrixConcurrencyStrategy，会和sleuth的strategy冲突。代码参见CoreHeaderInterceptor。现在可以测试zuul里面的rule，看能否拿到标签内容了。 这里还不是终点，解决了zuul的路由，服务A调服务B这里的路由怎么处理呢？zuul算出来的标签如何往后面依次传递下去呢，我们还是抄sleuth：把标签放入header，服务A调服务B时，将服务A header里面的标签放到服务B的header里，依次传递下去。这里的关键点就是：内部的微服务在接收到发来的请求时（zuul-》A，A-》B都是这种情况）我们将请求放入ThreadLocal，哦，不对，是HystrixRequestVariableDefault，还记得上面说的原因么：）。这个容易处理，写一个spring mvc拦截器即可，代码参见CoreHeaderInterceptor。然后发送请求时自动带上这个里面保存的标签信息，参见RestTemplate的拦截器CoreHttpRequestInterceptor。到此为止，技术上全部走通实现。 总结一下：zuul依据用户或IP等计算标签，并将标签放入header里向后传递，后续的微服务通过拦截器，将header里的标签放入RestTemplate请求的header里继续向后接力传递。标签的内容通过放入类似于ThreadLocal的全局变量（HystrixRequestVariableDefault），使Ribbon Rule可以使用。 测试参见PreFilter源码，模拟了几个用户的标签，参见LabelAndWeightMetadataRule源码，模拟了OR AND两种标签处理策略。依次开启 config eureka provide（开两个实例，通过启动参数server.port指定不同端口区分） consumer zuul 访问 http://localhost:8761/metadata.html 设置第一个provide 实例 orLabel为 CN,Test 发送请求头带入Authorization: emt 访问http://localhost:8080/provider/user 多刷几次，可以看到zuul所有请求均路由给了第一个实例。访问http://localhost:8080/consumer/test 多刷几次，可以看到，consumer调用均路由给了第一个实例。 设置第二个provide 实例 andLabel为 EN,Male 发送请求头带入Authorization: em 访问http://localhost:8080/provider/user 多刷几次，可以看到zuul所有请求均路由给了第二个实例。访问http://localhost:8080/consumer/test 多刷几次，可以看到，consumer调用均路由给了第二个实例。 Authorization头还可以设置为PreFilter里面的模拟token来做测试，至此所有内容讲解完毕，技术路线拉通，剩下的就是根据需求来完善你自己的路由策略啦。 作者：Charles_He链接：https://www.jianshu.com/p/37ee1e84900a來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix流程和原理讲解]]></title>
    <url>%2F2018%2F03%2F26%2Fhystrix02%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 Hystrix流程讲解创建Command一个HystrixCommand或一个HystrixObservableCommand对象，代表了对某个依赖服务发起的一次请求或者调用，构造的时候，可以在构造函数中传入任何需要的参数。HystrixCommand主要用于仅仅会返回一个结果的调用HystrixObservableCommand主要用于可能会返回多条结果的调用 12HystrixCommand command = new HystrixCommand(arg1, arg2);HystrixObservableCommand command = new HystrixObservableCommand(arg1, arg2); 执行Command执行Command就可以发起一次对依赖服务的调用 要执行Command，需要在4个方法中选择其中的一个：execute()，queue()，observe()，toObservable() 其中execute()和queue()仅仅对HystrixCommand适用 execute()：调用后直接block住，属于同步调用，直到依赖服务返回单条结果，或者抛出异常queue()：返回一个Future，属于异步调用，后面可以通过Future获取单条结果observe()：订阅一个Observable对象，Observable代表的是依赖服务返回的结果，获取到一个那个代表结果的Observable对象的拷贝对象toObservable()：返回一个Observable对象，如果我们订阅这个对象，就会执行command并且获取返回结果 1234K value = command.execute();Future&lt;K&gt; fValue = command.queue();Observable&lt;K&gt; ohValue = command.observe(); Observable&lt;K&gt; ocValue = command.toObservable(); execute()实际上会调用queue().get().queue()，接着会调用toObservable().toBlocking().toFuture() 也就是说，无论是哪种执行command的方式，最终都是依赖toObservable()去执行的。 检查是否开启请求缓存如果这个command开启了请求缓存，而且这个调用的结果在缓存中存在，那么直接从缓存中返回结果。 检查是否开启了断路器检查这个command对应的依赖服务是否开启了断路器 如果断路器被打开了，那么hystrix就不会执行这个command，而是直接去执行fallback降级机制。 检查线程池/队列/semaphore是否已经满了如果command对应的线程池/队列/semaphore已经满了，那么也不会执行command，而是直接去调用fallback降级机制。 执行command调用HystrixObservableCommand.construct()或HystrixCommand.run()来实际执行这个command HystrixCommand.run()是返回一个单条结果，或者抛出一个异常HystrixObservableCommand.construct()是返回一个Observable对象，可以获取多条结果 如果HystrixCommand.run()或HystrixObservableCommand.construct()的执行，超过了timeout时长的话，那么command所在的线程就会抛出一个TimeoutException。 如果timeout了，也会去执行fallback降级机制，而且就不会管run()或construct()返回的值了。 断路健康检查Hystrix会将每一个依赖服务的调用成功，失败，拒绝，超时，等事件，都会发送给circuit breaker断路器。断路器就会对调用成功/失败/拒绝/超时等事件的次数进行统计。断路器会根据这些统计次数来决定，是否要进行断路，如果打开了断路器，那么在一段时间内就会直接断路，然后如果在之后第一次检查发现调用成功了，就关闭断路器。 调用fallback降级机制在以下几种情况中，hystrix会调用fallback降级机制：run()或construct()抛出一个异常，断路器打开，线程池/队列/semaphore满了，command执行超时了。 一般在降级机制中，都建议给出一些默认的返回值，比如静态的一些代码逻辑，或者从内存中的缓存中提取一些数据，尽量在这里不要再进行网络请求了 即使在降级中，一定要进行网络调用，也应该将那个调用放在一个HystrixCommand中，进行隔离 在HystrixCommand中，上线getFallback()方法，可以提供降级机制 在HystirxObservableCommand中，实现一个resumeWithFallback()方法，返回一个Observable对象，可以提供降级结果 如果fallback返回了结果，那么hystrix就会返回这个结果 对于HystrixCommand，会返回一个Observable对象，其中会发返回对应的结果对于HystrixObservableCommand，会返回一个原始的Observable对象 如果没有实现fallback，或者是fallback抛出了异常，Hystrix会返回一个Observable，但是不会返回任何数据 不同的command执行方式，其fallback为空或者异常时的返回结果不同 对于execute()，直接抛出异常对于queue()，返回一个Future，调用get()时抛出异常对于observe()，返回一个Observable对象，但是调用subscribe()方法订阅它时，立即抛出调用者的onError方法对于toObservable()，返回一个Observable对象，但是调用subscribe()方法订阅它时，立即抛出调用者的onError方法。 不同执行方式走的流程execute()，获取一个Future.get()，然后拿到单个结果queue()，返回一个Futureobserver()，立即订阅Observable，然后启动8大执行步骤，返回一个拷贝的Observable，订阅时立即回调给你结果toObservable()，返回一个原始的Observable，必须手动订阅才会去执行8大步骤 Request Cache首先有一个概念，叫做reqeust context–请求上下文，一般来说在一个web应用中，会使用hystrix在一个filter里面，对每一个请求都施加一个请求上下文，在tomcat容器内，每一次请求就是一次请求上下文。 然后在这次请求上下文中，我们会去执行N多代码，调用N多依赖服务，有的依赖服务可能还会调用好几次，在一次请求上下文中，如果有多个command，参数都是一样的，调用的接口也是一样的，其实结果可以认为也是一样的。 那么这个时候，我们就可以让第一次command执行返回的结果被缓存在内存中，然后这个请求上下文中后续的其他对这个依赖的调用全部从内存中取用缓存结果就可以了。不用在一次请求上下文中反复多次的执行一样的command，提升整个请求的性能。 HystrixCommand和HystrixObservableCommand都可以指定一个缓存key，然后hystrix会自动进行缓存，接着在同一个request context内，再次访问的时候，就会直接取用缓存。用请求缓存，可以避免重复执行网络请求，多次调用一个command，那么只会执行一次，后面都是直接取缓存。 指定缓存key，只需要实现一个getCacheKey方法： 1234@Overrideprotected String getCacheKey() &#123; return "product_info_" + productId;&#125; 实例对于请求缓存（request caching），请求合并（request collapsing），请求日志（request log），等等技术，都必须自己管理HystrixReuqestContext的生命周期。 在一个请求执行之前，都必须先初始化一个request context 1HystrixRequestContext context = HystrixRequestContext.initializeContext(); 然后在请求结束之后，需要关闭request context 1context.shutdown(); 一般在Java Web应用中，都是通过filter过滤器来实现的。 12345678910111213141516171819202122public class HystrixRequestContextFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HystrixRequestContext context = HystrixRequestContext.initializeContext(); try &#123; chain.doFilter(request, response); &#125; finally &#123; context.shutdown(); &#125; &#125; @Override public void destroy() &#123; &#125;&#125; 注册Filter 123456@Beanpublic FilterRegistrationBean filterRegistrationBean() &#123; FilterRegistrationBean registration = new FilterRegistrationBean&lt;&gt;(new HystrixRequestContextFilter()); registration.addUrlPatterns("/*"); return registration;&#125; 手动清理缓存有时候可能需要手动清理缓存，Hystrix提供了方法。 1234public static void flushCache(Long productId) &#123; HystrixRequestCache.getInstance(KEY, HystrixConcurrencyStrategyDefault.getInstance()).clear(String.valueOf(productId));&#125; FallBack降级hystrix在3种情况下会调用降级方法。 运行的程序报错了，error。 线程池/信号量满了，reject。 超时了，timeout。 如果路器发现异常事件的占比达到了一定的比例，直接开启断路，circuit breaker。降级方法可以返回一个自定义的结果，或者一个过期的数据。 降级是通过实现HystrixCommand.getFallBack()方法来实现的。 12345@Overrideprotected String getFallback() &#123; System.out.println("从本地缓存获取过期的品牌数据，brandId=" + brandId); return BrandCache.getBrandName(brandId);&#125; 断路器工作原理 如果经过短路器的流量超过了一定的阈值，HystrixCommandProperties.circuitBreakerRequestVolumeThreshold() 如果断路器统计到的异常调用的占比超过了一定的阈值，HystrixCommandProperties.circuitBreakerErrorThresholdPercentage() 然后断路器从close状态转换到open状态 断路器打开的时候，所有经过该断路器的请求全部被短路，不调用后端服务，直接走fallback降级 经过了一段时间之后，HystrixCommandProperties.circuitBreakerSleepWindowInMilliseconds()，会half-open，让一条请求经过短路器，看能不能正常调用。如果调用成功了，那么就自动恢复，转到close状态。 断路器配置 circuitBreaker.enabled 控制断路器是否允许工作，包括跟踪依赖服务调用的健康状况，以及对异常情况过多时是否允许触发短路，默认是true。 12HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(boolean value) circuitBreaker.requestVolumeThreshold 设置一个rolling window，滑动窗口中，最少要有多少个请求时，才触发开启短路。举例来说，如果设置为20（默认值），那么在一个sleepWindowInMilliseconds秒的滑动窗口内，如果只有19个请求，即使这19个请求都是异常的，也是不会触发开启短路器的。 12HystrixCommandProperties.Setter() .withCircuitBreakerRequestVolumeThreshold(int value) circuitBreaker.sleepWindowInMilliseconds 设置在断路之后，需要在多长时间内直接reject请求，然后在这段时间之后，再重新到holf-open状态，尝试允许请求通过以及自动恢复，默认值是5000毫秒。这个值也是设置滑动窗口长度的一个值。 12HystrixCommandProperties.Setter() .withCircuitBreakerSleepWindowInMilliseconds(int value) circuitBreaker.errorThresholdPercentage 设置异常请求量的百分比，当异常请求达到这个百分比时，就触发打开短路器，默认是50，也就是50%。 12HystrixCommandProperties.Setter() .withCircuitBreakerErrorThresholdPercentage(int value) circuitBreaker.forceOpen 如果设置为true的话，直接强迫打开短路器，相当于是手动短路了，手动降级，默认false。 12HystrixCommandProperties.Setter() .withCircuitBreakerForceOpen(boolean value) circuitBreaker.forceClosed 如果设置为ture的话，直接强迫关闭短路器，相当于是手动停止短路了，手动升级，默认false。 12HystrixCommandProperties.Setter() .withCircuitBreakerForceClosed(boolean value) 配置实战配置一个断路器，流量要求是20，异常比例是50%，短路时间是5s。在command内加入一个判断，如果是productId=-1，那么就直接报错，触发异常执行。 写一个client测试程序，写入50个请求，前20个是正常的，但是后30个是productId=-1，然后继续请求，会发现。 1234567891011121314151617181920212223242526272829303132333435public class CircuitBreakerTest &#123; @Test public void test() throws InterruptedException &#123; for (int i = 0; i &lt; 15; i++) &#123; String response = HttpUtil.get("http://localhost:8081/getProductInfo?productId=1"); System.out.println("第" + (i + 1) + "次请求，结果为：" + response); &#125; for (int i = 0; i &lt; 25; i++) &#123; String response = HttpUtil.get("http://localhost:8081/getProductInfo?productId=-1"); System.out.println("第" + (i + 1) + "次请求，结果为：" + response); &#125; System.out.println("等待几秒钟，统计到最近30次请求超过40%次，开启断路"); Thread.sleep(3000); // 等待五秒后，时间窗口统计了（withCircuitBreakerSleepWindowInMilliseconds），发现异常比例太多，这个时候才会去开启断路器。直接走断路器 for (int i = 0; i &lt; 10; i++) &#123; String response = HttpUtil.get("http://localhost:8081/getProductInfo?productId=1"); System.out.println("第" + (i + 1) + "次请求，结果为：" + response); &#125; // 断路器有一个时间窗口，我们必须要等到那个个时间窗口过了以后，hystrix才会看一下最近的时间窗口 // 比如说最近的10秒内有多少条数据其中一场的数据有没有到一定的比例，如果到了一定的比例，才会去断路 System.out.println("尝试等待5秒钟，等待恢复"); Thread.sleep(5000); for (int i = 0; i &lt; 10; i++) &#123; String response = HttpUtil.get("http://localhost:8081/getProductInfo?productId=1"); System.out.println("第" + (i + 1) + "次请求，结果为：" + response); &#125; &#125;&#125; 断路器设计原则 每个服务都会调用几十个后端依赖服务，那些后端依赖服务通常是由很多不同的团队开发的 每个后端依赖服务都会提供它自己的client调用库，比如说用thrift的话，就会提供对应的thrift依赖 client调用库随时会变更 client调用库随时可能会增加新的网络请求的逻辑 client调用库可能会包含诸如自动重试，数据解析，内存中缓存等逻辑 client调用库一般都对调用者来说是个黑盒，包括实现细节，网络访问，默认配置，等等 在真实的生产环境中，经常会出现调用者，突然间惊讶的发现，client调用库发生了某些变化 即使client调用库没有改变，依赖服务本身可能有会发生逻辑上的变化 有些依赖的client调用库可能还会拉取其他的依赖库，而且可能那些依赖库配置的不正确 大多数网络请求都是同步调用的 调用失败和延迟，也有可能会发生在client调用库本身的代码中，不一定就是发生在网络请求中 线程池机制的优点如下： 任何一个依赖服务都可以被隔离在自己的线程池内，即使自己的线程池资源填满了，也不会影响任何其他的服务调用 服务可以随时引入一个新的依赖服务，因为即使这个新的依赖服务有问题，也不会影响其他任何服务的调用 当一个故障的依赖服务重新变好的时候，可以通过清理掉线程池，瞬间恢复该服务的调用，而如果是tomcat线程池被占满，再恢复就很麻烦 如果一个client调用库配置有问题，线程池的健康状况随时会报告，比如成功/失败/拒绝/超时的次数统计，然后可以近实时热修改依赖服务的调用配置，而不用停机 如果一个服务本身发生了修改，需要重新调整配置，此时线程池的健康状况也可以随时发现，比如成功/失败/拒绝/超时的次数统计，然后可以近实时热修改依赖服务的调用配置，而不用停机 基于线程池的异步本质，可以在同步的调用之上，构建一层异步调用层 线程池机制的缺点： 线程池机制最大的缺点就是增加了cpu的开销 每个command的执行都依托一个独立的线程，会进行排队，调度，还有上下文切换 Hystrix官方自己做了一个多线程异步带来的额外开销，通过对比多线程异步调用+同步调用得出，Netflix API每天通过hystrix执行10亿次调用，每个服务实例有40个以上的线程池，每个线程池有10个左右的线程 最后，用hystrix的额外开销，就是给请求带来了3ms左右的延时，最多延时在10ms以内，相比于可用性和稳定性的提升，这是可以接受的 限流测试限流的目的是为了保护过多的请求导致服务并发量过高而宕机。 withCoreSize：设置你的线程池的大小withMaxQueueSize：设置的是你的等待队列，缓冲队列的大小withQueueSizeRejectionThreshold：如果withMaxQueueSize&lt;withQueueSizeRejectionThreshold，那么取的是withMaxQueueSize，反之，取得是withQueueSizeRejectionThreshold 1234.andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() .withCoreSize(10) .withMaxQueueSize(12) .withQueueSizeRejectionThreshold(15)) 基于线程池的限流测试代码： 1234567891011121314151617181920212223242526/** * * 限流测试 * * @author yangfan * @date 2018/03/30 */public class RejectTest &#123; public static void main(String[] args) &#123; // GetProductInfoCommand配置线程池大小10，队列长度为12，超过8以后会被拒。 // 先进去线程池的是10个请求，然后有8个请求进入等待队列，线程池里有空闲，等待队列中的请求如果还没有timeout，那么就进去线程池去执行 // withExecutionTimeoutInMilliseconds(20000)：timeout也设置大一些，否则如果请求放等待队列中时间太长了，直接就会timeout，等不到去线程池里执行了 // withFallbackIsolationSemaphoreMaxConcurrentRequests(30)：fallback，sempahore限流，30个，避免太多的请求同时调用fallback被拒绝访问 for (int i = 0; i &lt; 25; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; String response = HttpUtil.get("http://localhost:8081/getProductInfo?productId=-2"); System.out.println("第" + (finalI + 1) + "次请求，结果为：" + response); &#125;).start(); &#125; &#125;&#125; 超时问题我们在调用一些第三方服务或者分布式系统的一些其他服务的时候，如果别的服务不稳定，导致大量超时，我们没有处理好，可能会导致我们自己的服务也会出问题，大量的线程卡死。所以我们必须做超时的控制，给我们的服务提供安全保护的措施。 execution.isolation.thread.timeoutInMilliseconds 手动设置timeout时长，一个command运行超出这个时间，就被认为是timeout，然后将hystrix command标识为timeout，同时执行fallback降级逻辑 默认是1000，也就是1000毫秒 `HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(int value)` execution.timeout.enabled 控制是否要打开timeout机制，默认是true `HystrixCommandProperties.Setter().withExecutionTimeoutEnabled(boolean value)` 总结hystrix的核心知识 hystrix内部工作原理：8大执行步骤和流程 资源隔离：你如果有很多个依赖服务，高可用性，先做资源隔离，任何一个依赖服务的故障不会导致你的服务的资源耗尽，不会崩溃 请求缓存：对于一个request context内的多个相同command，使用request cache，提升性能 熔断：基于短路器，采集各种异常事件，报错，超时，reject，短路，熔断，一定时间范围内就不允许访问了，直接降级，自动恢复的机制 降级：报错，超时，reject，熔断，降级，服务提供容错的机制 限流：在你的服务里面，通过线程池，或者信号量，限制对某个后端的服务或资源的访问量，避免从你的服务这里过去太多的流量，打死某个资源 超时：避免某个依赖服务性能过差，导致大量的线程hang住去调用那个服务，会导致你的服务本身性能也比较差 hystrix的高阶知识 request collapser，请求合并技术 fail-fast和fail-slient，高阶容错模式 static fallback和stubbed fallback，高阶降级模式 嵌套command实现的发送网络请求的降级模式 基于facade command的多级降级模式 request cache的手动清理 生产环境中的线程池大小以及timeout配置优化经验 线程池的自动化动态扩容与缩容技术 hystrix的metric高阶配置 基于hystrix dashboard的可视化分布式系统监控 生产环境中的hystrix工程运维经验]]></content>
      <categories>
        <category>hystrix</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix介绍和简单使用]]></title>
    <url>%2F2018%2F03%2F11%2Fhystrix01%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程学习笔记，记录以后翻看 Hystrix是什么？在分布式系统中，每个服务都可能会调用很多其他服务，被调用的那些服务就是依赖服务，有的时候某些依赖服务出现故障也是很正常的。 Hystrix可以让我们在分布式系统中对服务间的调用进行控制，加入一些调用延迟或者依赖故障的容错机制。 Hystrix通过将依赖服务进行资源隔离，进而组织某个依赖服务出现故障的时候，这种故障在整个系统所有的依赖服务调用中进行蔓延，同时Hystrix还提供故障时的fallback降级机制 总而言之，Hystrix通过这些方法帮助我们提升分布式系统的可用性和稳定性。 Hystrix的历史hystrix就是一种高可用保障的一个框架，预先封装好的为了解决某个特定领域的特定问题的一套代码库。用了框架之后，来解决这个领域的特定的问题，就可以大大减少我们的工作量，提升我们的工作质量和工作效率。 Netflix（可以认为是国外的优酷或者爱奇艺之类的视频网站），API团队从2011年开始做一些提升系统可用性和稳定性的工作，Hystrix就是从那时候开始发展出来的。 在2012年的时候，Hystrix就变得比较成熟和稳定了，Netflix中，除了API团队以外，很多其他的团队都开始使用Hystrix。 时至今日，Netflix中每天都有数十亿次的服务间调用，通过Hystrix框架在进行，而Hystrix也帮助Netflix网站提升了整体的可用性和稳定性 Hystrix的设计原则hystrix为了实现高可用性的架构，它的设计原则: 对依赖服务调用时出现的调用延迟和调用失败进行控制和容错保护 在复杂的分布式系统中，阻止某一个依赖服务的故障在整个系统中蔓延，服务A-&gt;服务B-&gt;服务C，服务C故障了，服务B也故障了，服务A故障了，整套分布式系统全部故障，整体宕机 提供fail-fast（快速失败）和快速恢复的支持 提供fallback优雅降级的支持 支持近实时的监控、报警以及运维操作 Hystrix要解决的问题在复杂的分布式系统架构中，每个服务都有很多的依赖服务，而每个依赖服务都可能会故障。如果服务没有和自己的依赖服务进行隔离，那么可能某一个依赖服务的故障就会拖垮当前这个服务。 举例来说，某个服务有30个依赖服务，每个依赖服务的可用性非常高，已经达到了99.99%的高可用性，那么该服务的可用性就是99.99%的30次方，也就是99.7%的可用性，99.7%的可用性就意味着0.3%的请求可能会失败，因为0.3%的时间内系统可能出现了故障导致系统不可用。对于1亿次访问来说，0.3%的请求失败，也就意味着30万次请求会失败，也意味着每个月有2个小时的时间系统是不可用的。 在真实生产环境中，可能更加糟糕，也就是说，即使你每个依赖服务都是99.99%高可用性，但是一旦你有几十个依赖服务，还是会导致你每个月都有几个小时是不可用的。 Hystrix的更加细节的设计原则 阻止任何一个依赖服务耗尽所有的资源，比如tomcat中的所有线程资源 避免请求排队和积压，采用限流和fail fast来控制故障 提供fallback降级机制来应对故障 使用资源隔离技术，比如bulkhead（舱壁隔离技术），swimlane（泳道技术），circuit breaker（短路技术），来限制任何一个依赖服务的故障的影响 通过近实时的统计/监控/报警功能，来提高故障发现的速度 通过近实时的属性和配置热修改功能，来提高故障处理和恢复的速度 保护依赖服务调用的所有故障情况，而不仅仅只是网络故障情况 Hystrix的实现 通过HystrixCommand或者HystrixObservableCommand来封装对外部依赖的访问请求，这个访问请求一般会运行在独立的线程中，资源隔离 对于超出我们设定阈值的服务调用，直接进行超时，不允许其耗费过长时间阻塞住。这个超时时间默认是99.5%的访问时间，但是一般我们可以自己设置一下 为每一个依赖服务维护一个独立的线程池，或者是semaphore，当线程池已满时，直接拒绝对这个服务的调用 对依赖服务的调用的成功次数，失败次数，拒绝次数，超时次数，进行统计 如果对一个依赖服务的调用失败次数超过了一定的阈值，自动进行熔断，在一定时间内对该服务的调用直接降级，一段时间后再自动尝试恢复 当一个服务调用出现失败，被拒绝，超时，短路等异常情况时，自动调用fallback降级机制 对属性和配置的修改提供近实时的支持 Hystrix项目实战背景商品详情页服务和缓存服务，模拟缓存更新时如何使用hystrix。 缓存服务 https://github.com/sail-y/eshop-cache-ha 商品服务 https://github.com/sail-y/eshop-product-ha 商品服务接口导致缓存服务资源耗尽的问题 基于线程池的资源隔离hystrix进行资源隔离，其实是提供了一个command抽象。把对某一个依赖服务的所有调用请求全部隔离在同一份资源池内，对这个依赖服务的所有调用请求，全部走这个资源池内的资源，不会去用其他的资源了，这个就叫做资源隔离。 hystrix最最基本的资源隔离的技术，线程池隔离技术。对某一个依赖服务，商品服务，所有的调用请求，全部隔离到一个线程池内，对商品服务的每次调用请求都封装在一个command里面。每个command（每次服务调用请求）都是使用线程池内的一个线程去执行的，所以哪怕是对这个依赖服务（商品服务）同时发起的调用量已经到了1000了，但是线程池内就10个线程，最多就只会用这10个线程去执行。 不会出现对商品服务的请求，因为接口调用延迟将tomcat内部所有的线程资源全部耗尽。目的是为了保护不要因为某一个依赖服务的故障，导致耗尽了缓存服务中的所有的线程资源去执行。 HystrixCommand：是用来获取一条数据的123456789101112131415161718public class GetProductInfoCommand extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; public GetProductInfoCommand(Long productId) &#123; super(HystrixCommandGroupKey.Factory.asKey("GetProductInfoCommandGroup")); this.productId = productId; &#125; @Override protected ProductInfo run() &#123; String url = "http://localhost:8082/getProductInfo?productId=" + productId; String response = HttpUtil.get(url); System.out.println(response); return JSON.parseObject(response, ProductInfo.class); &#125;&#125; test: http://localhost:8081/getProductInfo?productId=1 HystrixObservableCommand：是设计用来获取多条数据的controller: 1234567891011121314151617181920212223@RequestMapping("/getProductInfos")public String getProductInfos(String productIds) &#123; HystrixObservableCommand&lt;ProductInfo&gt; getProductInfosCommand = new GetProductInfosCommand(productIds.split(",")); Observable&lt;ProductInfo&gt; observable = getProductInfosCommand.observe(); observable.subscribe(new Observer&lt;ProductInfo&gt;()&#123; @Override public void onCompleted() &#123; log.info("获取完了所有的商品数据"); &#125; @Override public void onError(Throwable throwable) &#123; throwable.printStackTrace(); &#125; @Override public void onNext(ProductInfo productInfo) &#123; log.info(productInfo.toString()); &#125; &#125;); return "success";&#125; command: 1234567891011121314151617181920212223242526272829public class GetProductInfosCommand extends HystrixObservableCommand&lt;ProductInfo&gt; &#123; private String[] productIds; public GetProductInfosCommand(String[] productIds) &#123; super(HystrixCommandGroupKey.Factory.asKey("GetProductInfoCommandGroup")); this.productIds = productIds; &#125; @Override protected Observable&lt;ProductInfo&gt; construct() &#123; return Observable.&lt;ProductInfo&gt;create(observer -&gt; &#123; try &#123; if (!observer.isUnsubscribed()) &#123; for (String productId : productIds) &#123; String url = "http://localhost:8082/getProductInfo?productId=" + productId; String response = HttpUtil.get(url); ProductInfo productInfo = JSON.parseObject(response, ProductInfo.class); observer.onNext(productInfo); &#125; observer.onCompleted(); &#125; &#125; catch (Exception e) &#123; observer.onError(e); &#125; &#125;).subscribeOn(Schedulers.io()); &#125;&#125; test: http://localhost:8081/getProductInfos?productId=1,2,3 command的四种调用方式同步：new CommandHelloWorld(“World”).execute()，new ObservableCommandHelloWorld(“World”).toBlocking().toFuture().get() 异步：new CommandHelloWorld(“World”).queue()，new ObservableCommandHelloWorld(“World”).toBlocking().toFuture() 立即执行： observe()：hot，已经执行过了 订阅： toObservable(): cold，还没执行过 基于信号量的资源隔离 信号量跟线程池两种资源隔离的技术的区别： 线程池隔离技术和信号量隔离技术，分别在什么样的场景下去使用?线程池：适合99%场景，线程池一般处理对依赖服务的网络请求的调用和访问，timeout这种问题。 信号量：适合不是对外部依赖的访问，而是对内部的一些比较复杂的业务逻辑的访问，但是像这种访问系统内部的代码，其实不涉及任何的网络请求。那么只要做信号量的普通限流就可以了，因为不需要去捕获timeout类似的问题，如果算法+数据结构的效率不是太高，并发量突然太高，因为这里稍微耗时一些，导致很多线程卡在这里的话是不太好的。所以进行一个基本的资源隔离和访问，避免内部复杂的低效率的代码，导致大量的线程被hang住。 采用信号量技术进行资源隔离与限流 123super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ExampleGroup")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(ExecutionIsolationStrategy.SEMAPHORE))); 资源隔离策略配置现在我们知道有线程池（THREAD）和信号量（SEMAPHORE）两种隔离方式。除了选择隔离方式，hystrix还支持对隔离策略进行一些细粒度的配置。 默认的策略就是线程池 线程池其实最大的好处就是对于网络访问请求，如果有超时的话，可以避免调用线程阻塞住 而使用信号量的场景，通常是针对超大并发量的场景下，每个服务实例每秒都几百的QPS，那么此时你用线程池的话，线程一般不会太多，可能撑不住那么高的并发，如果要撑住，可能要耗费大量的线程资源，那么就是用信号量，来进行限流保护 一般用信号量常见于那种基于纯内存的一些业务逻辑服务，而不涉及到任何网络访问请求 netflix有100+的command运行在40+的线程池中，只有少数command是不运行在线程池中的，就是从纯内存中获取一些元数据，或者是对多个command包装起来的facacde command，是用信号量限流的 123456// to use thread isolationHystrixCommandProperties.Setter() .withExecutionIsolationStrategy(ExecutionIsolationStrategy.THREAD)// to use semaphore isolationHystrixCommandProperties.Setter() .withExecutionIsolationStrategy(ExecutionIsolationStrategy.SEMAPHORE) command名称和command组每个command都可以设置一个自己的名称，同时可以设置一个自己的组。 12345678private static final Setter cachedSetter = Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ExampleGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("HelloWorld")); public CommandHelloWorld(String name) &#123; super(cachedSetter); this.name = name;&#125; command group，是一个非常重要的概念，默认情况下，因为就是通过command group来定义一个线程池的，而且还会通过command group来聚合一些监控和报警信息。同一个command group中的请求，都会进入同一个线程池中。 command线程池threadpool key代表了一个HystrixThreadPool，用来进行统一监控，统计，缓存。默认的threadpool key就是command group名称。每个command都会跟它的threadpool key对应的thread pool绑定在一起。如果不想直接用command group，也可以手动设置thread pool name。 123456public CommandHelloWorld(String name) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ExampleGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("HelloWorld")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey("HelloWorldPool"))); this.name = name;&#125; command threadpool -&gt; command group -&gt; command key command key：代表了一类command，一般来说代表了底层的依赖服务的一个接口。 command group：代表了某一个底层的依赖服务，一个依赖服务可能会暴露出来多个接口，每个接口就是一个command key。在逻辑上去组织起来一堆command key的调用，统计信息、成功次数、timeout超时次数、失败次数等等，可以看到某一个服务整体的一些访问情况。一般推荐是根据一个服务去划分出一个线程池，command key默认都是属于同一个线程池的。 比如以一个服务为粒度，估算出来这个服务每秒的所有接口加起来的整体QPS在100左右。调用目标服务的当前服务部署了10个服务实例，每个服务实例上给一个线程池，线程数量大概在10个左右，就可以满足对目标服务的整体的访问QPS大概在每秒100左右需求了。 还有一种场景，就是command group对应的服务的接口访问量差别很大。然后就希望做一些细粒度的资源隔离，针对同一个服务的不同接口，使用不同的线程池。 之前的模式是： command key -&gt; command group 我们可以针对每个command单独设置threadpool key：command key -&gt; 自己的threadpool key 这样从逻辑上来说多个command key是属于一个command group的，在做统计的时候会放在一起统计。但是每个command key有自己的线程池，每个接口有自己的线程池去做资源隔离和限流。 设置线程池大小Hystrix默认的线程池大小是10，可以通过下面的代码进行设置。 12HystrixThreadPoolProperties.Setter() .withCoreSize(int value) 一般来说默认的10个已经够了。 queueSizeRejectionThreshold线程池是10个，如果还有请求过来，默认可以排队的线程是5个。超过5个以后多余的请求进来，就会被线程池拒绝掉，抛出异常。 默认值是5，可以通过下面代码修改： 12HystrixThreadPoolProperties.Setter() .withQueueSizeRejectionThreshold(int value) execution.isolation.semaphore.maxConcurrentRequests设置使用SEMAPHORE隔离策略的时候，允许访问的最大并发量，超过这个最大并发量，请求直接被reject。这个并发量的设置跟线程池大小的设置应该是类似的，但是基于信号量的话性能会好很多，而且hystrix框架本身的开销会小很多。 默认值是10，不能设置得太大，因为信号量是基于调用线程去执行command的，而且不能从timeout中抽离，因此一旦设置的太大，而且有延时发生，可能瞬间导致tomcat本身的线程资源本占满。 12HystrixCommandProperties.Setter() .withExecutionIsolationSemaphoreMaxConcurrentRequests(int value) Hystrix的基本使用已经差不多是这样了，后面再有一篇文章，分析hystrix的流程和原理。]]></content>
      <categories>
        <category>hystrix</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用缓存架构实战5-缓存预热]]></title>
    <url>%2F2018%2F02%2F22%2Fcache05%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程笔记，记录以供以后翻看 缓存预热系统刚上线的时候，redis里面是没有数据的，如果这个时候高并发的流量过来全部跑到mysql，那么mysql肯定就挂掉了。所以我们需要缓存预热 提前给redis中灌入部分数据，再提供服务 肯定不可能将所有数据都写入redis，因为数据量太大了，第一耗费的时间太长了，第二根本redis容纳不下所有的数据 需要根据当天的具体访问情况，实时统计出访问频率较高的热数据 然后将访问频率较高的热数据写入redis中，肯定是热数据也比较多，我们也得多个服务并行读取数据去写，并行的分布式的缓存预热 然后将灌入了热数据的redis对外提供服务，这样就不至于冷启动，直接让数据库裸奔了 缓存预热的方案和流程： 1、 nginx+lua将访问流量上报到kafka中 要统计出来当前最新的实时的热数据是哪些，我们就得将商品详情页访问的请求对应的流量，日志，实时上报到kafka中。 2、 storm从kafka中消费数据，实时统计出每个商品的访问次数，访问次数基于LRU内存数据结构的存储方案 如何使用storm？ 优先用内存中的一个LRUMap去存放，这样做性能高，而且没有外部依赖。否则依赖redis的话，我们本就是要防止redis挂掉数据丢失的情况，就不合适了; 用mysql，扛不住高并发读写; 用hbase，hadoop生态系统，维护麻烦，太重了。其实我们只要统计出最近一段时间访问最频繁的商品，然后对它们进行访问计数，同时维护出一个前N个访问最多的商品list即可。计算好每个task大致要存放的商品访问次数的数量，计算出大小。然后构建一个LRUMap，apache commons collections有开源的实现，设定好map的最大大小，就会自动根据LRU算法去剔除多余的数据，保证内存使用限制。即使有部分数据被干掉了，然后下次来重新开始计数，也没关系，因为如果它被LRU算法干掉，那么它就不是热数据，说明最近一段时间都很少访问了。 3、每个storm task启动的时候，基于zk分布式锁，将自己的id写入zk同一个节点中 4、每个storm task负责完成自己这里的热数据的统计，每隔一段时间，就遍历一下这个map，然后维护一个前3个商品的list，更新这个list 5、写一个后台线程，每隔一段时间，比如1分钟，都将排名前3的热数据list，同步到zk中去，存储到这个storm task对应的一个znode中去 6、我们需要一个服务，比如说，代码可以跟缓存数据生产服务放一起，但是也可以放单独的服务，这个服务可能部署了很多个实例。每次服务启动的时候，就会去拿到一个storm task的列表，然后根据taskid，一个一个的去尝试获取taskid对应的znode的zk分布式锁。如果能获取到分布式锁的话，那么就将那个storm task对应的热数据的list取出来，然后将数据从mysql中查询出来，写入缓存中，进行缓存的预热。因为是多个服务实例，分布式的并行的去做，都基于zk分布式锁做了协调（没有并发冲突问题），分布式并行缓存的预热，效率很高。 基于nginx+lua完成商品详情页访问流量实时上报kafka的开发在nginx这一层，接收到访问请求的时候，就把请求的流量上报发送给kafka。这样的话，storm才能去消费kafka中的实时的访问日志，然后去进行缓存热数据的统计。用的技术方案非常简单，从lua脚本直接创建一个kafka producer，发送数据到kafka。 12345cd /usr/localwget https://github.com/doujiang24/lua-resty-kafka/archive/master.zipyum install -y unzipunzip master.zipcp -rf /usr/local/lua-resty-kafka-master/lib/resty /usr/hello/lualib 接着修改脚本，开始写记录日志并发送到kafka： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788local cjson = require("cjson")local producer = require("resty.kafka.producer")local broker_list = &#123; &#123;host="192.168.2.201", port=9092&#125;, &#123;host="192.168.2.202", port=9092&#125;, &#123;host="192.168.2.203", port=9092&#125;&#125;local log_json = &#123;&#125;-- 商品详情页log_json["request_module"] = "product_detail_info"log_json["headers"] = ngx.req.get_headers()log_json["uri_args"] = ngx.req.get_uri_args()log_json["body"] = ngx.req.read_body() log_json["http_version"] = ngx.req.http_version() log_json["method"] =ngx.req.get_method() log_json["raw_reader"] = ngx.req.raw_header() log_json["body_data"] = ngx.req.get_body_data() local message = cjson.encode(log_json); local uri_args = ngx.req.get_uri_args()local productId = uri_args["productId"]local shopId = uri_args["shopId"]-- 向kafka发送请求的记录local async_producer = producer:new(broker_list, &#123;producer_type="async"&#125;)local ok, err = async_producer:send("access-log", productId, message)local cache_ngx = ngx.shared.my_cache-- 拼接商品和缓存的keylocal productCacheKey = "product_info_"..productIdlocal shopCacheKey = "shop_info_"..shopId-- 获取cachelocal productCache = cache_ngx:get(productCacheKey)local shopCache = cache_ngx:get(shopCacheKey)-- 没有就从后端查询if productCache == "" or productCache == nil then local http = require("resty.http") local httpc = http.new() local resp, err = httpc:request_uri("http://192.168.2.171:8080",&#123; method = "GET", path = "/getProductInfo?productId="..productId &#125;) productCache = resp.body cache_ngx:set(productCacheKey, productCache, 10 * 60)end if shopCache == "" or shopCache == nil then local http = require("resty.http") local httpc = http.new() local resp, err = httpc:request_uri("http://192.168.2.171:8080",&#123; method = "GET", path = "/getShopInfo?shopId="..shopId &#125;) shopCache = resp.body cache_ngx:set(shopCacheKey, shopCache, 10 * 60)endlocal productCacheJSON = cjson.decode(productCache)local shopCacheJSON = cjson.decode(shopCache)local context = &#123; productId = productCacheJSON.id, productName = productCacheJSON.name, productPrice = productCacheJSON.price, productPictureList = productCacheJSON.pictureList, productSpecification = productCacheJSON.specification, productService = productCacheJSON.service, productColor = productCacheJSON.color, productSize = productCacheJSON.size, shopId = shopCacheJSON.id, shopName = shopCacheJSON.name, shopLevel = shopCacheJSON.level, shopGoodCommentRate = shopCacheJSON.goodCommentRate&#125;-- 渲染模板local template = require("resty.template")template.render("product.html", context) 两台机器上都这样做，才能统一上报流量到kafka 修改配置： 在nginx.conf中，http部分，加入resolver 8.8.8.8; 在/usr/local/kafka/config/server.properties中加入advertised.host.name = 192.168.2.201(各kafka实例的ip)，杀掉并重启三个kafka进程。nohup bin/kafka-server-start.sh config/server.properties &amp; 启动eshop-cache缓存服务，因为nginx中的本地缓存可能不在了 下面试试消息是否上报成功，先创建kafka topic。 1bin/kafka-topics.sh --zookeeper 192.168.2.201:2181,192.168.2.202:2181,192.168.2.203:2181 --topic access-log --replication-factor 1 --partitions 1 --create 访问：http://192.168.2.203/product?productId=1&amp;requestPath=product&amp;shopId=1 启动consumer，订阅access-log主题可以看到消息已经发送过来了。 1bin/kafka-console-consumer.sh --zookeeper 192.168.2.201:2181,192.168.2.202:2181,192.168.2.203:2181 --topic access-log --from-beginning 基于Storm统计热数据 用Spout从kafka读取消息 Bolt提取productId发射到下一个Bolt 基于LRUMap统计热点访问的productId 将热点数据存入zookeeper https://github.com/sail-y/eshop-storm 预热逻辑： 服务启动的时候，进行缓存预热 从zk中读取taskid列表 依次遍历每个taskid，尝试获取分布式锁，如果获取不到，快速报错，不要等待，因为说明已经有其他服务实例在预热了 直接尝试获取下一个taskid的分布式锁 即使获取到了分布式锁，也要检查一下这个taskid的预热状态，如果已经被预热过了，就不再预热了 执行预热操作，遍历productid列表，查询数据，然后写ehcache和redis 预热完成后，设置taskid对应的预热状态 https://github.com/sail-y/eshop-cache 基于nginx+lua+storm的热点缓存的流量分发策略自动降级解决方案如果因为秒杀等或者抢购等原因，某一个商品访问量瞬间飙升，就算做了流量分发和缓存，因为hash策略所以同一个productId会被分发到同一个nginx服务器中。那么这就可能会导致nginx服务挂掉，这一台挂掉后别的服务上，然后也陆陆续续挂掉，导致整个系统不可用。 解决办法： 在storm中实时的计算出瞬间出现的热点 我们可以基于storm来计算热点数据，比如我们将访问的次数排序，将后面95%的数据访问量取一个平均值。这个时候要设定一个阈值，如果超出95%平均值的n倍，例如5倍，我们就认为是瞬间出现的热点数据，判断其可能在短时间内继续扩大的访问量，甚至达到平均值几十倍，或者几百倍，当发现第一个商品的访问次数，小于平均值的5倍，就安全了，就break掉这个循环。 流量分发nginx的分发策略降级 流量分发nginx加一个逻辑：每次访问一个商品详情页的时候，如果发现它是个热点，那么立即做流量分发策略的降级。降级成对这个热点商品，流量分发采取随机负载均衡发送到所有的后端应用nginx服务器上去。瞬间将热点缓存数据的访问从hash分发全部到一台nginx，变成了负载均衡发送到多台nginx上。 storm还需要保存下来上次识别出来的热点list 保存上次的热点数据，跟这次计算出的热点数据进行比较，那么就需要对某些数据进行热点取消，删除nginx本地缓存。 代码实战https://github.com/sail-y/eshop-storm HotProductFindThread.java 新增的逻辑： 将LRUMap中的数据按照访问次数进行全局的排序 计算95%的商品访问次数的平均值 遍历排序后的商品访问次数，降序 如果某个商品是平均访问量的10倍以上，就认为是缓存的热点 将缓存热点数据推送到流量分发的nginx中 将获取到的换成你数据推送到nginx服务上 lua接口开发将热点数据进行标记接口开发 之前流量分发的nginx服务是部署在192.168.2.203上面的，所以vi /usr/hello/hello.conf 12345678910111213141516171819server &#123; listen 80; server_name _; location /hello &#123; default_type 'text/html'; content_by_lua_file /usr/hello/lua/hello.lua; &#125; location /product &#123; default_type 'text/html'; content_by_lua_file /usr/hello/lua/distribute.lua; &#125; location /hot &#123; default_type 'text/html'; content_by_lua_file /usr/hello/lua/hot.lua; &#125;&#125; 123456789101112vi /usr/hello/lua/hot.lualocal uri_args = ngx.req.get_uri_args()local product_id = uri_args["productId"]local cache_ngx = ngx.shared.my_cachelocal hot_product_cache_key = "hot_product_"..product_idcache_ngx:set(hot_product_cache_key, "true", 60*60)/usr/servers/nginx/sbin/nginx -s reload 设置缓存数据接口开发 分别在201和201的nginx应用增加一个hot的配置 12345678910111213141516vi /usr/hello/hello.conflocation /hot &#123; default_type 'text/html'; content_by_lua_file /usr/hello/lua/hot.lua;&#125;vi /usr/hello/lua/hot.lualocal uri_args = ngx.req.get_uri_args()local product_id = uri_args["productId"]local product_info = uri_args["productInfo"]local product_cache_key = "product_info_"..product_idlocal cache_ngx = ngx.shared.my_cachecache_ngx:set(product_cache_key, product_info, 60*60) 自动降级代码开发下面在distribute.lua里面开发自动降级的逻辑 取出之前在hot.lua中缓存的hot_product_cache_key 如果为true就不走hash算法进行流量分发了，走随机负载均衡算法。 123456789101112131415local hosts = &#123;"192.168.2.201", "192.168.2.202"&#125;local backend = ""local hot_product_key = "hot_product_"..productIdlocal cache_ngx = ngx.shared.my_cachelocal hot_product_flag = cache_ngx:get(hot_product_key)if hot_product_flag == "true" then math.randomseed(tostring(os.time()):reverse():sub(1, 7)) local index = math.random(1, 2) backend = "http://"..hosts[index]else local hash = ngx.crc32_long(productId) local index = (hash % 2) + 1 backend = "http://"..hosts[index]end 热点缓存消失自动识别和感知逻辑开发https://github.com/sail-y/eshop-cache HotProductFindThread.java 123// 缓存热点消失，发送一个一个http请求到nginx取消热点缓存的标识String url = &quot;http://192.168.2.203/cancelHot?productId=&quot; + productId;HttpClientUtils.sendGetRequest(url); 203上nginx的lua接口开发 123456789101112131415161718vi hello.conf location /cancelHot &#123; default_type 'text/html'; content_by_lua_file /usr/hello/lua/cancelHot.lua;&#125;cp lua/hot.lua lua/cancelHot.luavi lua/cancelHot.lualocal uri_args = ngx.req.get_uri_args()local product_id = uri_args["productId"]local cache_ngx = ngx.shared.my_cachelocal hot_product_cache_key = "hot_product_"..product_idcache_ngx:set(hot_product_cache_key, "false", 60) 测试手动将某个热点设置为热点： 1http://192.168.2.203/hot?productId=1 然后访问： 1http://192.168.2.203/product?productId=1&amp;requestPath=product&amp;shopId=1 可以看到当缓存变成热点以后，访问的服务是随机变化的。 因为代码里用了缓存，所以记得在203的上面vi conf/nginx.conf，在http模块下新增lua_shared_dict my_cache 128m;。 多访问几次不同的商品ID 用storm运行topology，观察日志是否正确 观察zookeeper里面的数据是否正确 123zkCli.shget /task-hot-product-list-4[5,1,3] storm日志： 123456782018-03-03 21:12:56.627 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【HotProductFindThread计算出一份排序后的商品访问次数列表】 productCountListJSON=[&#123;1:11&#125;,&#123;3:1&#125;,&#123;5:1&#125;,&#123;9:1&#125;,&#123;7:1&#125;]2018-03-03 21:12:56.637 c.r.e.s.b.ProductCountBolt Thread-16 [INFO] 【ProductCountThread计算出一份top3热门商品列表】zk path=/task-hot-product-list-4, topnProductListJSON=[1,3,5]2018-03-03 21:12:56.677 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【HotProductFindThread】计算出后95%访问次数的平均值 avgCount=1.02018-03-03 21:12:56.821 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【HotProductFindThread】 发现一个新的热点 productId=12018-03-03 21:12:58.754 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] http://192.168.2.201/hot?productId=1&amp;productInfo=%7B%22id%22%3A1%2C%22name%22%3A%22iphone7%E6%89%8B%E6%9C%BA%22%2C%22price%22%3A5599.0%2C%22pictureList%22%3A%22a.jpg%2Cb.jpg%22%2C%22specification%22%3A%22iphone7%E7%9A%84%E8%A7%84%E6%A0%BC%22%2C%22service%22%3A%22iphone7%E7%9A%84%E5%94%AE%E5%90%8E%E6%9C%8D%E5%8A%A1%22%2C%22color%22%3A%22%E7%BA%A2%E8%89%B2%2C%E7%99%BD%E8%89%B2%2C%E9%BB%91%E8%89%B2%22%2C%22size%22%3A%225.5%22%2C%22shopId%22%3A1%2C%22modifiedTime%22%3A%222017-01-01+12%3A00%3A00%22%7D%0A2018-03-03 21:12:58.787 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] http://192.168.2.202/hot?productId=1&amp;productInfo=%7B%22id%22%3A1%2C%22name%22%3A%22iphone7%E6%89%8B%E6%9C%BA%22%2C%22price%22%3A5599.0%2C%22pictureList%22%3A%22a.jpg%2Cb.jpg%22%2C%22specification%22%3A%22iphone7%E7%9A%84%E8%A7%84%E6%A0%BC%22%2C%22service%22%3A%22iphone7%E7%9A%84%E5%94%AE%E5%90%8E%E6%9C%8D%E5%8A%A1%22%2C%22color%22%3A%22%E7%BA%A2%E8%89%B2%2C%E7%99%BD%E8%89%B2%2C%E9%BB%91%E8%89%B2%22%2C%22size%22%3A%225.5%22%2C%22shopId%22%3A1%2C%22modifiedTime%22%3A%222017-01-01+12%3A00%3A00%22%7D%0A2018-03-03 21:12:58.807 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【HotProductFindThread】 保存上次热点数据 lastTimeHotProductList=[1]2018-03-03 21:13:01.637 c.r.e.s.b.ProductCountBolt Thread-16 [INFO] 【ProductCountThread打印productCountMap的长度】size=5 然后访问几次别的商品，把平均数提高。 12345672018-03-03 21:15:01.948 c.r.e.s.b.ProductCountBolt Thread-16 [INFO] 【ProductCountThread计算出一份top3热门商品列表】zk path=/task-hot-product-list-4, topnProductListJSON=[1,3,5]2018-03-03 21:15:03.839 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【ProductCountThread打印productCountMap的长度】size=52018-03-03 21:15:03.839 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【HotProductFindThread计算出一份排序后的商品访问次数列表】 productCountListJSON=[&#123;1:12&#125;,&#123;3:3&#125;,&#123;5:1&#125;,&#123;9:1&#125;,&#123;7:1&#125;]2018-03-03 21:15:03.839 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【HotProductFindThread】计算出后95%访问次数的平均值 avgCount=1.52018-03-03 21:15:03.866 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【HotProductFindThread】 发现一个热点消失了 productId=12018-03-03 21:15:03.866 c.r.e.s.b.ProductCountBolt Thread-17 [INFO] 【HotProductFindThread】 保存上次热点数据 lastTimeHotProductList=[]2018-03-03 21:15:06.949 c.r.e.s.b.ProductCountBolt Thread-16 [INFO] 【ProductCountThread打印productCountMap的长度】size=5]]></content>
      <categories>
        <category>高可用缓存架构实战</category>
      </categories>
      <tags>
        <tag>多级缓存架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm介绍和集群部署、WordCount演示]]></title>
    <url>%2F2018%2F02%2F22%2Fcache04-3%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程笔记，记录以供以后翻看 java系统跟大数据技术的关系 大数据不仅仅只是大数据工程师要关注的东西 大数据也是Java程序员在构建各类系统的时候一种全新的思维，以及架构理念，比如Storm，Hive，Spark，ZooKeeper，HBase，Elasticsearch，等等 Storm：实时缓存热点数据统计-&gt;缓存预热-&gt;缓存热点数据自动降级 Hive：Hadoop生态栈里面，做数据仓库的一个系统，高并发访问下，海量请求日志的批量统计分析，日报周报月报，接口调用情况，业务使用情况，等等 Spark：离线批量数据处理，比如从DB中一次性批量处理几亿数据，清洗和处理后写入Redis中供后续的系统使用，大型互联网公司的用户相关数据 ZooKeeper：分布式系统的协调，分布式锁，分布式选举-&gt;高可用HA架构，轻量级元数据存储 HBase：海量数据的在线存储和简单查询，替代MySQL分库分表，提供更好的伸缩性 Elasticsearch：海量数据的复杂检索以及搜索引擎的构建，支撑有大量数据的各种企业信息化系统的搜索引擎，电商/新闻等网站的搜索引擎，等等 Apache Storm简介Storm是一个分布式的，可靠的，容错的数据流处理系统。是非常流行的实时计算框架，也非常成熟。 支撑各种实时类的项目场景：实时处理消息以及更新数据库，基于最基础的实时计算语义和API（实时数据处理领域）；对实时的数据流持续的进行查询或计算，同时将最新的计算结果持续的推送给客户端展示，同样基于最基础的实时计算语义和API（实时数据分析领域）；对耗时的查询进行并行化，基于DRPC，即分布式RPC调用，单表30天数据，并行化，每个进程查询一天数据，最后组装结果 高度的可伸缩性：如果要扩容，直接加机器，调整storm计算作业的并行度就可以了，storm会自动部署更多的进程和线程到其他的机器上去，无缝快速扩容 数据不丢失的保证：storm的消息可靠机制开启后，可以保证一条数据都不丢 超强的健壮性：从历史经验来看，storm比hadoop、spark等大数据类系统，健壮的多的多，因为元数据全部放zookeeper，不在内存中，随便挂都不要紧 使用的便捷性：核心语义非常的简单，开发起来效率很高 Storm的集群架构以及核心概念Nimbus，Supervisor，ZooKeeper，Worker，Executor，Task Topology，Spout，Bolt，Tuple，Stream 拓扑：务虚的一个概念 Spout：数据源的一个代码组件，就是我们可以实现一个spout接口，写一个java类，在这个spout代码中，我们可以自己尝试去数据源获取数据，比如说从kafka中消费数据 bolt：一个业务处理的代码组件，spout会将数据传送给bolt，各种bolt还可以串联成一个计算链条，java类实现了一个bolt接口 一堆spout+bolt，就会组成一个topology，就是一个拓扑，实时计算作业，spout+bolt，一个拓扑涵盖数据源获取/生产+数据处理的所有的代码逻辑，topology tuple：就是一条数据，每条数据都会被封装在tuple中，在多个spout和bolt之间传递 stream：就是一个流，务虚的一个概念，抽象的概念，源源不断过来的tuple，就组成了一条数据流 Spout和bolt组件会向Nimbus请求资源，通过Supervisor分配到不同的worker，然后开启多个task执行任务。 Storm组件 在Storm集群中，有两类节点：主节点master node和工作节点worker nodes。主节点运行Nimbus守护进程，这个守护进程负责在集群中分发代码，为工作节点分配任务，并监控故障。Supervisor守护进程作为拓扑的一部分运行在工作节点上。一个Storm拓扑结构在不同的机器上运行着众多的工作节点。每个工作节点都是topology中一个子集的实现。而Nimbus和Supervisor之间的协调则通过Zookeeper系统或者集群。 Zookeeper Zookeeper是完成Supervisor和Nimbus之间协调的服务。而应用程序实现实时的逻辑则被封装进Storm中的“topology”。topology则是一组由Spouts（数据源）和Bolts（数据操作）通过Stream Groupings进行连接的图。 Spout Spout从来源处读取数据并放入topology。Spout分成可靠和不可靠两种；当Storm接收失败时，可靠的Spout会对tuple（元组，数据项组成的列表）进行重发；而不可靠的Spout不会考虑接收成功与否只发射一次。而Spout中最主要的方法就是nextTuple（），该方法会发射一个新的tuple到topology，如果没有新tuple发射则会简单的返回。 Bolt Topology中所有的处理都由Bolt完成。Bolt从Spout中接收数据并进行处理，如果遇到复杂流的处理也可能将tuple发送给另一个Bolt进行处理。而Bolt中最重要的方法是execute（），以新的tuple作为参数接收。不管是Spout还是Bolt，如果将tuple发射成多个流，这些流都可以通过declareStream（）来声明。 Storm的并行度以及流分组并行度：Worker-&gt;Executor-&gt;Task，没错，是Task 流分组：Task与Task之间的数据流向关系 Stream Grouping定义了一个流在Bolt任务中如何被切分。 Shuffle grouping：随机分发tuple到Bolt的任务，保证每个任务获得相等数量的tuple。 Fields grouping：根据指定字段分割数据流，并分组。例如，根据“user-id”字段，相同“user-id”的元组总是分发到同一个任务，不同“user-id”的元组可能分发到不同的任务。 Partial Key grouping：根据指定字段分割数据流，并分组。类似Fields grouping。 All grouping：tuple被复制到bolt的所有任务。这种类型需要谨慎使用。 Global grouping：全部流都分配到bolt的同一个任务。明确地说，是分配给ID最小的那个task。 None grouping：无需关心流是如何分组。目前，无分组等效于随机分组。但最终，Storm将把无分组的Bolts放到Bolts或Spouts订阅它们的同一线程去执行（如果可能）。 Direct grouping：这是一个特别的分组类型。元组生产者决定tuple由哪个元组处理者任务接收。 Local or shuffle grouping：如果目标bolt有一个或多个任务在同一工作进程，tuples 会打乱这些进程内的任务。否则,这就像一个正常的 Shuffle grouping。 一般只用Shuffle grouping和Fields grouping。 helloworldhttps://github.com/sail-y/storm-helloworld 集群部署拷贝apache-storm-1.1.0.tar.gz到/usr/local目录下，解压。 12/usr/localmv apache-storm-1.1.0 storm 配置环境变量： 1234vi ~/.bashrcexport STORM_HOME=/usr/local/stormexport PATH=$PATH:$STORM_HOME/binsource ~/.bashrc 修改storm配置文件： 1234567891011121314151617181920mkdir /var/stormvi /usr/local/storm/conf/storm.yamlstorm.zookeeper.servers: - "192.168.2.201" - "192.168.2.202" - "192.168.2.203"nimbus.seeds: ["192.168.2.201"]storm.local.dir: "/var/storm"# slots.ports，指定每个机器上可以启动多少个worker，一个端口号代表一个workersupervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 把另外两台机器也部署上： 12scp ~/.bashrc root@192.168.2.202:~/scp -r /usr/local/storm root@192.168.2.202:/usr/local 记得source一下和创建/var/storm的目录。 在201上的nimbus：storm nimbus &gt;/dev/null 2&gt;&amp;1 &amp; 3个节点都启动supervisor ：storm supervisor &gt;/dev/null 2&gt;&amp;1 &amp; 201启动storm ui：storm ui &gt;/dev/null 2&gt;&amp;1 &amp; 3个节点都启动logviewer：storm logviewer &gt;/dev/null 2&gt;&amp;1 &amp; 用jps检查是否已经启动。 1234567jps1697 Kafka1316 QuorumPeerMain11269 Supervisor11369 core11148 nimbus11486 Jps 访问Storm UI查看集群状态：http://192.168.2.201:8080/index.html 提交作业到storm集群来运行先将上面的项目打包 mvn clean package 将打包好的storm-helloworld-1.0-SNAPSHOT.jar上传到201的/usr/local目录下。 然后执行命令提交作业到storm集群。 1storm jar /usr/local/storm-helloworld-1.0-SNAPSHOT.jar com.roncoo.eshop.storm.WorkCountTopology WorkCountTopology 如何kill掉一个topology： 1storm kill WorkCountTopology]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper+kafka集群的安装部署]]></title>
    <url>%2F2018%2F02%2F20%2Fcache04-2%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程笔记，记录以供以后翻看 zookeeper集群搭建将zookeeper-3.4.5.tar.gz拷贝到/usr/local目录下。对zookeeper-3.4.5.tar.gz进行解压缩： tar -zxvf zookeeper-3.4.5.tar.gz 对zookeeper目录进行重命名： mv zookeeper-3.4.5 zk 配置zookeeper相关的环境变量 12345678910111213141516171819vi ~/.bashrcexport ZOOKEEPER_HOME=/usr/local/zkexport PATH=$ZOOKEEPER_HOME/binsource ~/.bashrccd zk/confcp zoo_sample.cfg zoo.cfgvi zoo.cfg#修改：dataDir=/usr/local/zk/data#新增：server.0=eshop-cache01:2888:3888 server.1=eshop-cache02:2888:3888server.2=eshop-cache03:2888:3888cd zkmkdir datacd dataecho 0 &gt;&gt; myid 这是eshop-cache01节点的搭建，另外2个节点，一样的步骤去搭建。 在另外两个节点上按照上述步骤配置ZooKeeper，使用scp将zk和.bashrc拷贝到eshop-cache02和eshop-cache03上即可。唯一的区别是myid标识号分别设置为1和2。 12scp ~/.bashrc root@eshop-cache02:~/scp -r /usr/local/zk root@eshop-cache02:/usr/local/ 分别在三台机器上执行： zkServer.sh start 检查ZooKeeper状态： zkServer.sh status 应该是一个leader，两个follower jps：检查三个节点是否都有QuromPeerMain进程 kafka集群搭建将kafka_2.11-1.0.0.tgz拷贝到/usr/local目录下。 对kafka_2.9.2-0.8.1.tgz进行解压缩： tar -zxvf kafka_2.11-1.0.0.tgz 对kafka目录进行改名： mv kafka_2.11-1.0.0 kafka 配置kafka 123vi /usr/local/kafka/config/server.properties# broker.id：依次增长的整数，0、1、2，集群中Broker的唯一idzookeeper.connect=192.168.2.201:2181,192.168.2.202:2181,192.168.2.203:2181 按照上述步骤在另外两台机器分别安装kafka。用scp把kafka拷贝到其他机器即可。唯一区别的，就是server.properties中的broker.id，要设置为1和2 在三台机器上的kafka目录下，分别执行以下命令： nohup bin/kafka-server-start.sh config/server.properties &amp; 使用jps检查启动是否成功: 1234jps1697 Kafka1316 QuorumPeerMain1997 Jps 使用基本命令检查kafka是否搭建成功: 12345bin/kafka-topics.sh --zookeeper 192.168.2.201:2181,192.168.2.202:2181,192.168.2.203:2181 --topic test --replication-factor 1 --partitions 1 --createbin/kafka-console-producer.sh --broker-list 192.168.2.201:9092,192.168.2.202:9092,192.168.2.203:9092 --topic testbin/kafka-console-consumer.sh --zookeeper 192.168.2.201:2181,192.168.2.202:2181,192.168.2.203:2181 --topic test --from-beginning]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用缓存架构实战4-多级缓存架构.高并发读写方案.nginx流量分发方案.ZK分布式锁解决并发冲突方案]]></title>
    <url>%2F2018%2F02%2F18%2Fcache04%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程笔记，记录以供以后翻看 上亿流量的商品详情页系统的多级缓存架构很多人以为做个缓存其实就是用一下redis访问一下就可以了，这只是简单的缓存使用方式。做复杂的缓存，支撑电商等复杂的场景下的高并发的缓存，遇到的问题非常非常之多，绝对不是说简单的访问一下redis就可以了。通常采用三级缓存：nginx本地缓存+redis分布式缓存+tomcat堆缓存的多级缓存架构 时效性要求非常高的数据：库存 一般来说，显示的库存都是时效性要求会相对高一些，因为随着商品的不断的交易，库存会不断的变化。当然，我们就希望当库存变化的时候，尽可能更快将库存显示到页面上去，而不是说等了很长时间，库存才反应到页面上去。 时效性要求不高的数据：商品的基本信息（名称. 颜色. 版本. 规格参数，等等），时效性要求不高的数据，就还好，比如说你现在改变了商品的名称，稍微晚个几分钟反应到商品页面上，也是可以接受的。 商品价格/库存等时效性要求高的数据，这种数据不多，相关的服务系统每次发生了变更的时候，直接采取数据库和redis缓存双写的方案，这样缓存的时效性最高。 商品基本信息等时效性不高的数据，而且种类繁多，来自多种不同的系统，采取MQ异步通知的方式，写一个数据生产服务，监听MQ消息，然后异步拉取服务的数据，更新tomcat jvm缓存+redis缓存。 流程：nginx+lua脚本做页面动态生成的工作，每次请求过来，优先从nginx本地缓存中提取各种数据，结合页面模板，生成需要的页面。如果nginx本地缓存过期了，那么就从nginx到redis中去拉取数据，更新到nginx本地。如果redis中也被LRU算法清理掉了，那么就从nginx走http接口到后端的服务中拉取数据，数据生产服务中，先在本地tomcat里的jvm堆缓存（ehcache）中找，如果也被LRU清理掉了，那么就重新发送请求到源头的服务中去拉取数据，然后再次更新tomcat堆内存缓存+redis缓存，并返回数据给nginx，nginx缓存到本地。 多级缓存架构中每一层的意义nginx本地缓存，抗的是热数据的高并发访问，一般来说，商品的购买总是有热点的，比如每天购买iphone. nike. 海尔等知名品牌的东西的人，总是比较多的。这些热数据，由于经常被访问，利用nginx本地缓存，所以可以被锁定在nginx的本地缓存内。大量的热数据的访问，就会被保留在nginx本地缓存内，那么对这些热数据的大量访问，就直接走nginx就可以了。那么大量的访问，直接就可以走到nginx就行了，不需要走后续的各种网络开销了。 redis分布式大规模缓存，抗的是很高的离散访问，支撑海量的数据，高并发的访问，高可用的服务。redis缓存最大量最完整的数据，可能1T+数据; 支撑高并发的访问，QPS最高到几十万; 可用性需要非常好，提供非常稳定的服务。nginx本地内存有限，也就能cache住部分热数据，除了各种iphone. nike等热数据，其他相对不那么热的数据，可能流量会经常走到redis那里。利用redis cluster的多master写入，横向扩容，1T+以上海量数据支持，几十万的读写QPS，99.99%高可用性都没有问题，那么就可以抗住大量的离散访问请求。 tomcat jvm堆内存缓存，主要是抗redis大规模灾难的情况，如果redis出现了大规模的宕机，导致nginx大量流量直接涌入数据生产服务，那么最后的tomcat堆内存缓存至少可以再抗一下，不至于让数据库直接裸奔。同时tomcat jvm堆内存缓存，也可以抗住redis没有cache住的最后那少量的部分缓存。 多级缓存架构项目实例源码时效性要求不高的数据，采取的是异步更新缓存的策略。缓存数据生产服务，监听一个消息队列，然后数据源服务（商品信息管理服务）发生了数据变更之后，就将数据变更的消息推送到消息队列中。缓存数据生产服务可以去消费这个数据变更的消息，然后根据消息的指示提取一些参数，然后调用对应的数据源服务的接口拉取数据，这个时候一般是从mysql库中拉取的。 这里消息中间件，我们使用kafka，zookeeper+kafka集群的安装部署 项目源码地址：https://github.com/sail-y/eshop-cache Nginx层缓存前面三层缓存架构中的本地堆缓存+redis分布式缓存都做好了，接下来就要来做三级缓存中的nginx那一层的缓存了。一般默认会部署多个nginx，在里面都会放一些缓存，默认情况下，此时缓存命中率是比较低的，因为流量会均分。 如何提升缓存命中率？ 分发层+应用层，双层nginx。 分发层nginx，负责流量分发的逻辑和策略，它可以用lua脚本开发一些规则，比如根据productId去进行hash，然后对后端的nginx数量取模，将某一个商品的访问的请求，就固定路由到一个nginx后端服务器上去，保证只会从redis中获取一次缓存数据，再次请求全都是走nginx本地缓存了。 后端的nginx服务器，就称之为应用服务器; 最前端的nginx服务器，被称之为分发服务器。看似很简单，其实很有用，在实际的生产环境中，可以大幅度提升你的nginx本地缓存这一层的命中率，大幅度减少redis后端的压力，提升性能。 这里会采用OpenResty的方式去部署nginx，而且会写一个nginx+lua开发的一个hello world。 部署第一个nginx，作为应用层nginx我这里部署到我机器上192.168.2.201，教程参考http://jinnianshilongnian.iteye.com/blog/2186270 部署openresty1234567891011121314151617181920212223242526272829303132yum install -y readline-devel pcre-devel openssl-devel gccmkdir -p /usr/servers cd /usr/servers/wget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz tar -xzvf ngx_openresty-1.7.7.2.tar.gz cd /usr/servers/ngx_openresty-1.7.7.2/cd bundle/LuaJIT-2.1-20150120/ make clean &amp;&amp; make &amp;&amp; make install ln -sf luajit-2.1.0-alpha /usr/local/bin/luajitcd bundle wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz tar -xvf 2.3.tar.gz cd bundle wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz tar -xvf v0.3.0.tar.gz cd /usr/servers/ngx_openresty-1.7.7.2 ./configure --prefix=/usr/servers --with-http_realip_module --with-pcre --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2 make &amp;&amp; make install cd /usr/servers/ ll/usr/servers/luajit/usr/servers/lualib/usr/servers/nginx/usr/servers/nginx/sbin/nginx -v 启动nginx: /usr/servers/nginx/sbin/nginx nginx+lua开发的hello worldvi /usr/servers/nginx/conf/nginx.conf 在http部分添加： 12lua_package_path &quot;/usr/servers/lualib/?.lua;;&quot;; lua_package_cpath &quot;/usr/servers/lualib/?.so;;&quot;; /usr/servers/nginx/conf下，创建一个lua.conf 1234server &#123; listen 80; server_name _; &#125; 在nginx.conf的http部分添加： include lua.conf; 验证配置是否正确： /usr/servers/nginx/sbin/nginx -t 在lua.conf的server部分添加： 1234location /lua &#123; default_type &apos;text/html&apos;; content_by_lua &apos;ngx.say(&quot;hello world&quot;)&apos;; &#125; 验证配置是否正确： /usr/servers/nginx/sbin/nginx -t 重新nginx加载配置 /usr/servers/nginx/sbin/nginx -s reload 访问： http://192.168.2.201/lua 成功输出 hello world 接下来替换成lua脚本文件执行 123mkdir /usr/servers/nginx/conf/lua/vi /usr/servers/nginx/conf/lua/test.luangx.say("hello world"); 修改lua.conf 1234location /lua &#123; default_type &apos;text/html&apos;; content_by_lua_file conf/lua/test.lua; &#125; 验证配置是否正确： /usr/servers/nginx/sbin/nginx -t 重新nginx加载配置 /usr/servers/nginx/sbin/nginx -s reload 查看异常日志 tail -f /usr/servers/nginx/logs/error.log 工程化的nginx+lua项目结构刚才只是写了一个hello world，在正式的项目中，脚本的目录结构一般是下面这样： 1234567hello hello.conf lua hello.lua lualib *.lua *.so 放在/usr/hello目录下，不会放在nginx所在文件夹里。 配置如下： 123456789101112131415mkdir -p /usr/hello/luaecho 'ngx.say("hello world");' &gt;&gt; /usr/hello/lua/hello.luacp -r /usr/servers/lualib /usr/hellovi /usr/hello/hello.confserver &#123; listen 80; server_name _; location /hello &#123; default_type 'text/html'; content_by_lua_file /usr/hello/lua/hello.lua; &#125; &#125; 修改nginx.conf 123456789101112131415161718vi /usr/servers/nginx/conf/nginx.confworker_processes 2; error_log logs/error.log; events &#123; worker_connections 1024; &#125; http &#123; include mime.types; default_type text/html; lua_package_path "/usr/hello/lualib/?.lua;;"; lua_package_cpath "/usr/hello/lualib/?.so;;"; include /usr/hello/hello.conf; &#125; 验证配置是否正确： /usr/servers/nginx/sbin/nginx -t 重新nginx加载配置 /usr/servers/nginx/sbin/nginx -s reload 如法炮制，在另外一个机器上，也用OpenResty部署一个nginx。 开发和部署流量分发层我在eshop-cache02和eshop-cache03上都部署好了openresty。现在用eshop-cache01和eshop-cache02作为应用层nginx服务器，用eshop-cache03作为分发层nginx。 现在在eshop-cache03，也就是分发层nginx中，编写lua脚本，完成基于商品id的流量分发策略： 获取请求参数，比如productId 对productId进行hash hash值对应用服务器数量取模，获取到一个应用服务器 利用http发送请求到应用层nginx 获取响应后返回 这个就是基于商品id的定向流量分发的策略，lua脚本来编写和实现。作为一个流量分发的nginx，会发送http请求到后端的应用nginx上面去，所以要先引入lua http lib包。 123cd /usr/hello/lualib/resty/ wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua 然后我们编辑流量分发的代码： 123456789101112131415161718192021222324252627local uri_args = ngx.req.get_uri_args()local productId = uri_args["productId"]local hosts = &#123;"192.168.2.201", "192.168.2.202"&#125;local hash = ngx.crc32_long(productId)local index = (hash % 2) + 1backend = "http://"..hosts[index]local requestPath = uri_args["requestPath"]requestPath = "/"..requestPath.."?productId="..productIdlocal http = require("resty.http")local httpc = http.new()local resp, err = httpc:request_uri(backend, &#123; method = "GET", path = requestPath&#125;)if not resp then ngx.say("request error :", err) returnendngx.say(resp.body)httpc:close() 访问测试： 123/usr/servers/nginx/sbin/nginx -s reloadhttp://192.168.2.203/hello?productId=5&amp;requestPath=hellohello world. this is eshop-cache01 基于商品id的定向流量分发策略的lua脚本就开发完了。经过测试可以看到，如果请求的是固定的某一个商品，那么就一定会将流量分到固定的一个应用nginx上面去。 基于nginx+lua+java完成多级缓存架构的核心业务逻辑上面做了流量分发的demo测试，Java层级的缓存开发，接下来把他们对接起来，正式编写分发层和应用层的脚本。 eshop-cache03机器上修改流量分发层配置和lua脚本： 123456789101112131415vi hello.conf# 添加以下内容location /product &#123; default_type 'text/html'; content_by_lua_file /usr/hello/lua/distribute.lua;&#125;cd luacp hello.lua distribute.luavi distribute.lua# 添加shopIdlocal shopId = uri_args["shopId"]requestPath = "/"..requestPath.."?productId="..productId.."&amp;shopId="..shopId/usr/servers/nginx/sbin/nginx -s reload 修改应用层配置和lua脚本： 应用nginx的lua脚本接收到请求 获取请求参数中的商品id，以及商品店铺id 根据商品id和商品店铺id，在nginx本地缓存中尝试获取数据 如果在nginx本地缓存中没有获取到数据，那么就到redis分布式缓存中获取数据，如果获取到了数据，还要设置到nginx本地缓存中 这里有个问题，建议不要用nginx+lua直接去获取redis数据。因为openresty没有太好的redis cluster的支持包，所以建议是发送http请求到缓存数据生产服务，由该服务提供一个http接口。缓存数生产服务可以基于redis cluster api从redis中直接获取数据，并返回给nginx。 如果缓存数据生产服务没有在redis分布式缓存中没有获取到数据，那么就在自己本地ehcache中获取数据，返回数据给nginx，也要设置到nginx本地缓存中 如果ehcache本地缓存都没有数据，那么就需要去原始的服务中拉去数据，该服务会从mysql中查询，拉去到数据之后，返回给nginx，并重新设置到ehcache和redis中 nginx最终利用获取到的数据，动态渲染网页模板 因为应用层也要访问http接口，所以也需要部署http依赖和模板的依赖 12345678cd /usr/hello/lualib/resty/ wget https://raw.githubusercontent.com/pintsized/lua-resty-http/ master/lib/resty/http_headers.lua wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua cd /usr/hello/lualib/resty/wget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template.luamkdir /usr/hello/lualib/resty/htmlcd /usr/hello/lualib/resty/htmlwget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template/html.lua 在hello.conf的server中配置模板位置 12set $template_location "/templates"; set $template_root "/usr/hello/templates"; 编辑要显示的模板： 1234567891011121314151617181920212223mkdir /usr/hello/templatescd /usr/hello/templatesvi product.html&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;title&gt;商品详情页&lt;/title&gt; &lt;/head&gt;&lt;body&gt;商品id: &#123;* productId *&#125;&lt;br/&gt;商品名称: &#123;* productName *&#125;&lt;br/&gt;商品图片列表: &#123;* productPictureList *&#125;&lt;br/&gt;商品规格: &#123;* productSpecification *&#125;&lt;br/&gt;商品售后服务: &#123;* productService *&#125;&lt;br/&gt;商品颜色: &#123;* productColor *&#125;&lt;br/&gt;商品大小: &#123;* productSize *&#125;&lt;br/&gt;店铺id: &#123;* shopId *&#125;&lt;br/&gt;店铺名称: &#123;* shopName *&#125;&lt;br/&gt;店铺评级: &#123;* shopLevel *&#125;&lt;br/&gt;店铺好评率: &#123;* shopGoodCommentRate *&#125;&lt;br/&gt;&lt;/body&gt;&lt;/html&gt; 将渲染后的网页模板作为http响应，返回给分发层nginx 在nginx.conf的http模块里添加： 12# 添加nginx本地缓存的支持lua_shared_dict my_cache 128m; 修改hello.conf： 12345# 添加路由location /product &#123; default_type &apos;text/html&apos;; content_by_lua_file /usr/hello/lua/product.lua;&#125; 修改lua/product.lua脚本，注意192.168.2.171是我本机的ip，启动了Java项目： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960vi lua/product.lualocal uri_args = ngx.req.get_uri_args()local productId = uri_args["productId"]local shopId = uri_args["shopId"]local cache_ngx = ngx.shared.my_cachelocal productCacheKey = "product_info_"..productIdlocal shopCacheKey = "shop_info_"..shopIdlocal productCache = cache_ngx:get(productCacheKey)local shopCache = cache_ngx:get(shopCacheKey)if productCache == "" or productCache == nil then local http = require("resty.http") local httpc = http.new() local resp, err = httpc:request_uri("http://192.168.2.171:8080",&#123; method = "GET", path = "/getProductInfo?productId="..productId &#125;) productCache = resp.body cache_ngx:set(productCacheKey, productCache, 10 * 60)endif shopCache == "" or shopCache == nil then local http = require("resty.http") local httpc = http.new() local resp, err = httpc:request_uri("http://192.168.2.171:8080",&#123; method = "GET", path = "/getShopInfo?shopId="..shopId &#125;) shopCache = resp.body cache_ngx:set(shopCacheKey, shopCache, 10 * 60)endlocal cjson = require("cjson")local productCacheJSON = cjson.decode(productCache)local shopCacheJSON = cjson.decode(shopCache)local context = &#123; productId = productCacheJSON.id, productName = productCacheJSON.name, productPrice = productCacheJSON.price, productPictureList = productCacheJSON.pictureList, productSpecification = productCacheJSON.specification, productService = productCacheJSON.service, productColor = productCacheJSON.color, productSize = productCacheJSON.size, shopId = shopCacheJSON.id, shopName = shopCacheJSON.name, shopLevel = shopCacheJSON.level, shopGoodCommentRate = shopCacheJSON.goodCommentRate&#125;local template = require("resty.template")template.render("product.html", context) 写到这里，应该去把java项目里的2个接口给补充一下。还是之前的项目：https://github.com/sail-y/eshop-cache 分布式重建缓存的并发冲突问题之前在Java代码里会先去redis里面取数据，如果redis取不到，就会去ehcache里面取，如果还是取不到，就需要重建缓存了。 但是重建缓存有一个问题，因为我们的服务可能是多实例的，虽然在nginx层我们通过流量分发将请求通过id分发到了不同的nginx应用层上。那么到了接口服务层，可能多次请求访问的是不同的实例，那么可能会导致多个机器去重建读取相同的数据，然后写入缓存中，这就有了分布式重建缓存的并发冲突问题。 问题就是可能2个实例获取到的数据快照不一样，但是新数据先写入缓存，如果这个时候另外一个实例的缓存后写入，就有问题了。 这个问题有好几种解决方案： 一样的在应用层对id进行取模然后固定分发到不同的服务实例上 将更新缓存的请求发送到同一个分区的kafka消息中 一般来讲，每个服务实例都会监听kafka一个topic的某一个分区，所以具体去哪一个分区也得取模，保证是同一个实例消费到更新的请求。但问题是在nginx算出来的hash取模可能与kafka生产者的hash策略算出来的分区可能并不一致，还是可能有并发冲突问题。 基于zookeeper分布式锁的解决方案 分布式锁，如果你有多个机器在访问同一个共享资源，加个锁让多个分布式的机器在访问共享资源的时候串行起来。 zk分布式锁的解决并发冲突的方案 变更缓存重建以及空缓存请求重建，更新redis之前，都需要先获取对应商品id的分布式锁 拿到分布式锁之后，需要根据时间版本去比较一下，如果自己的版本新于redis中的版本，那么就更新，否则就不更新 如果拿不到分布式锁，那么就等待，不断轮询等待，直到自己获取到分布式的锁 方案源码实现：https://github.com/sail-y/eshop-cache ZooKeeperSession.java 经典的缓存+数据库读写的模式（cache aside pattern） 读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应 更新的时候，先删除缓存，然后再更新数据库 为什么是删除缓存，而不是更新缓存呢？ 原因很简单，很多时候，复杂点的缓存的场景，因为缓存有的时候，不简单是数据库中直接取出来的值。比如商品详情页的系统，修改库存，只是修改了某个表的某些字段，但是要真正把这个影响的最终的库存计算出来，可能还需要从其他表查询一些数据，然后进行一些复杂的运算，才能最终计算出现在最新的库存是多少，然后才能将库存更新到缓存中去。 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据，并进行运算，才能计算出缓存最新的值的，更新缓存的代价是很高的。还有一个问题就是，是不是每次修改数据库的时候，都一定要将其对应的缓存去跟新一份？也许有的场景是这样的，但是对于比较复杂的缓存数据计算的场景，就不是这样了。举个例子，一个缓存涉及的表的字段，在1分钟内就修改了20次，或者是100次，那么缓存更新20次，100次; 但是这个缓存在1分钟内就被读取了1次，系统有大量的冷数据，28法则，20%的数据，占用了80%的访问量。实际上，如果你只是删除缓存的话，那么1分钟内，访问的时候再计算，这个缓存不过就重新计算一次而已，开销大幅度降低。每次数据过来，就只是删除缓存，然后修改数据库，如果这个缓存，在1分钟内只是被访问了1次，那么只有那1次，缓存是要被重新计算的，用缓存才去算缓存。其实删除缓存，而不是更新缓存，就是一个lazy计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。 缓存+数据库双写不一致问题分析最初级的缓存不一致问题以及解决方案问题：先修改数据库，再删除缓存，如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据出现不一致 解决思路： 先删除缓存，再修改数据库，如果删除缓存成功了，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致，因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中。 比较复杂的数据不一致问题分析数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中，数据变更的程序完成了数据库的修改，这个时候数据库和缓存中的数据不一样了。 为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就1万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况，高并发了以后，问题是很多的。 解决方案： 数据库与缓存更新与读取操作进行异步串行化。更新数据的时候，根据数据的唯一标识（例如hash值取模），将操作路由之后，发送到一个jvm内部的队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中。一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。 高并发的场景下，该解决方案要注意的问题 读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。该解决方案，最大的风险点在于可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库，所以务必通过一些模拟真实的测试，看看更新数据的频繁是怎样的。 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压100个商品的库存修改操作，每个库存修改操作要耗费10ms去完成，那么最后一个商品的读请求，可能等待10 * 100 = 1000ms = 1s后，才能得到数据，这个时候就导致读请求的长时阻塞。 一定要做根据实际业务系统的运行情况，去进行一些压力测试和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求会hang多少时间，如果你计算过后，哪怕是最繁忙的时候，积压10个更新操作，最多等待200ms，那还可以。如果一个内存队列可能积压的更新操作特别多，那么就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。 一般来说数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的，针对读高并发，读缓存架构的项目，一般写请求相对读来说，是非常非常少的，每秒的QPS能到几百就不错了。比如500/s的写操作，拆成5份，每200ms就是100个写操作，单机器一般20个内存队列，每个内存队列，可能就积压5个写操作，每个写操作性能测试后，一般在20ms左右就完成。如果写QPS扩大10倍，但是经过刚才的测算，就知道，单机支撑写QPS几百没问题，那么就扩容机器，扩容10倍的机器，10台机器，每个机器20个队列，200个队列。 大部分的情况下，应该是大量的读请求过来，都是直接走缓存取到数据的。少量情况下，可能遇到读跟数据更新冲突的情况，如上所述，那么此时更新操作如果先入队列，之后可能会瞬间来了对这个数据大量的读请求，但是因为做了去重的优化，所以也就一个更新缓存的操作跟在它后面，等数据更新完了，读请求触发的缓存更新操作也完成，然后临时等待的读请求全部可以读到缓存中的数据。 读请求并发量过高 还必须做好压力测试，确保恰巧碰上上述情况的时候，还有另一个风险，就是突然间大量读请求会在几十毫秒的延时hang在服务上，看服务能不能抗的住，需要多少机器才能抗住最大的极限情况的峰值。但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。按1:99的比例计算读和写的请求，每秒5万的读QPS，可能只有500次更新操作，如果一秒有500的写QPS，那么要测算好，可能写操作影响的数据有500条，这500条数据在缓存中失效后可能导致多少读请求发送读请求到库存服务来要求更新缓存。一般来说这个比例在1:1，1:2，1:3之内，例如500条缓存数据失效导致每秒钟有1000个读请求会hang在库存服务上，每个读请求最多hang200ms就会返回。在同一时间最多hang住的可能也就是单机200个读请求，单机hang200个读请求，还是ok的。 多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证，执行数据更新操作，以及执行缓存更新操作的请求，都通过nginx服务器路由到相同的服务实例上。 热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能造成某台机器的压力过大。因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。 复杂的数据库+缓存双写一致保障方案项目实例源码https://github.com/sail-y/eshop-inventory]]></content>
      <categories>
        <category>高可用缓存架构实战</category>
      </categories>
      <tags>
        <tag>多级缓存架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用缓存架构实战3-Redis高可用集群实战]]></title>
    <url>%2F2018%2F02%2F15%2Fcache03%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程笔记，记录以供以后翻看 Redis高可用集群实战如何做到99.99%高可用性 什么叫99.99%的高可用性？ 在365天 * 99.99%的时间内，你的系统都是可以对外提供服务的，那就是高可用性，99.99%。 redis不可用是什么？单实例不可用？主从架构不可用？不可用的后果是什么？ 如果是master进程被杀了，或者系统宕机了，那就无法提供服务了。但是如果是集群中某一个slave挂掉了，没问题，还有其他的slave可以提供服务。 Redis怎么才能做到高可用？ 如果master挂了怎么办？Redis有个故障转移功能，如果master node故障时，自动检测，并且将某个slave node自动切换为master node，也可以叫做主备切换，这实现了redis主从架构下的高可用性，这其中会用到Redis的哨兵架构（它会去检测）。 一旦master故障，在很短的时间内，就会切换到另外一个master上去，可能就几分钟，或者几秒钟是不可用的。 Redis哨兵架构介绍Sentinel（哨兵）是Redis 的高可用性解决方案：由一个或多个Sentinel 实例 组成的Sentinel 系统可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器。 哨兵是redis集群架构中非常重要的一个组件，主要功能如下： 集群监控，负责监控redis master和slave进程是否正常工作。 消息通知，如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移，如果master node挂掉了，会自动转移到slave node上。 配置中心，如果故障转移发生了，通知client客户端新的master地址。 哨兵本身也是分布式的，作为一个哨兵集群去运行，互相协同工作： 执行故障转移时，判断一个master node是宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就坑爹了。 哨兵的核心知识 哨兵至少需要3个实例，来保证自己的健壮性 哨兵 + redis主从的部署架构，是不会保证数据零丢失的，只能保证redis集群的高可用性 对于哨兵 + redis主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练 为什么redis哨兵集群只有2个节点无法正常工作？哨兵集群必须部署2个以上节点，如果哨兵集群仅仅部署了个2个哨兵实例 1234+----+ +----+| M1 |---------| R1 || S1 | | S2 |+----+ +----+ Configuration: quorum = 1（如果有quorum个哨兵投票选举，就认为master宕机，进行切换） 上图中2个哨兵，master宕机，s1和s2中只要有1个哨兵认为master宕机就可以进行切换，同时s1和s2中会选举出一个哨兵来执行故障转移。这个时候，它需要大多数(majority)哨兵都是运行的，2个哨兵的majority就是2（2的majority=2，3的majority=2，5的majority=3，4的majority=2），如果2个哨兵都运行着，就可以允许执行故障转移。但是，如果整个M1和S1运行的机器宕机了，那么哨兵只有1个了，此时就没有majority(大多数的哨兵)来允许执行故障转移，虽然另外一台机器还有一个R1，但是故障转移不会执行。 经典的3节点哨兵集群123456789 +----+ | M1 | | S1 | +----+ |+----+ | +----+| R2 |----+----| R3 || S2 | | S3 |+----+ +----+ Configuration: quorum = 2，majority 如果M1所在机器宕机了，那么三个哨兵还剩下2个，S2和S3可以一致认为master宕机，然后选举出一个来执行故障转移。 数据丢失问题主备切换的过程，可能会导致数据丢失 异步复制导致的数据丢失 因为master -&gt; slave的复制是异步的，所以可能有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了。 脑裂导致的数据丢失 脑裂，也就是说，某个master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着，此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master，这个时候，集群里就会有两个master，也就是所谓的脑裂。此时虽然某个slave被切换成了master，但是可能client还没来得及切换到新的master，还继续写向旧master的数据可能也丢失了，因此旧master再次恢复的时候，会被作为一个slave挂到新的master上去，自己的数据会清空，重新从新的master复制数据。 解决异步复制和脑裂导致的数据丢失有2个参数： 12min-slaves-to-write 1min-slaves-max-lag 10 要求至少有1个slave，数据复制和同步的延迟不能超过10秒，如果说一旦所有的slave，数据复制和同步的延迟都超过了10秒钟，那么这个时候，master就不会再接收任何请求了，上面两个配置可以减少异步复制和脑裂导致的数据丢失。 减少异步复制的数据丢失 有了min-slaves-max-lag这个配置，就可以确保一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就拒绝写请求，这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低的可控范围内。 减少脑裂的数据丢失 如果一个master出现了脑裂，跟其他slave丢了连接，上面的配置就确保了如果跟任何一个slave丢了连接，在10秒后发现没有slave给自己ack，那么就拒绝客户端新的写请求，因此在脑裂场景下，最多就丢失10秒的数据。 哨兵原理详解sdown和odown转换机制sdown和odown是两种失败状态。sdown是主观宕机，就是一个哨兵如果自己觉得一个master宕机了，那么就是主观宕机。 odown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，那么就是客观宕机。 sdown达成的条件很简单，如果一个哨兵ping一个master，超过了is-master-down-after-milliseconds参数指定的毫秒数之后，就主观认为master宕机。 sdown到odown转换的条件很简单，如果一个哨兵在指定时间内，收到了quorum指定数量的其他哨兵也认为那个master是sdown了，那么就认为是odown了，客观认为master宕机。 哨兵集群的自动发现机制哨兵互相之间的发现，是通过redis的pub/sub系统实现的，每个哨兵都会往__sentinel__:hello这个channel里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在，每隔两秒钟，每个哨兵都会往自己监控的某个master+slaves对应的__sentinel__:hello channel里发送一个消息，内容是自己的host、ip和runid还有对这个master的监控配置。 每个哨兵也会去监听自己监控的每个master+slaves对应的__sentinel__:hellochannel，然后去感知到同样在监听这个master+slaves的其他哨兵的存在。每个哨兵还会跟其他哨兵交换对master的监控配置，互相进行监控配置的同步。 slave配置的自动纠正哨兵会负责自动纠正slave的一些配置，比如slave如果要成为潜在的master候选人，哨兵会确保slave在复制现有master的数据; 如果slave连接到了一个错误的master上，比如故障转移之后，哨兵会确保它们连接到正确的master上。 slave-&gt;master选举算法如果一个master被认为odown了，而且majority（大多数）哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个slave。 选举会考虑slave的一些因素： 跟master断开连接的时长 slave优先级 复制offset run id 如果一个slave跟master断开连接已经超过了down-after-milliseconds的10倍，外加master宕机的时长，那么slave就被认为不适合选举为master。 接下来会对slave进行排序： 按照slave优先级进行排序，slave priority越低，优先级就越高。 如果slave priority相同，那么看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高。 如果上面两个条件都相同，那么选择一个run id比较小的那个slave。 quorum和majority每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还得得到majority哨兵的授权，才能正式执行切换。 如果quorum &lt; majority，比如5个哨兵，majority就是3，quorum设置为2，那么就3个哨兵授权就可以执行切换。 但是如果quorum &gt;= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换。 configuration epoch哨兵会对一套redis master+slave进行监控，有相应的监控的配置，要执行切换的那个哨兵，会从要切换到的新master（salve-&gt;master）节点那里得到一个configuration epoch，这就是一个version号，每次切换的version号都必须是唯一的。 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待failover-timeout时间，然后接替继续执行切换，此时会重新获取一个新的configuration epoch，作为新的version号。 configuraiton传播哨兵完成切换之后，会在自己本地更新生成最新的master配置，然后同步给其他的哨兵，就是通过之前说的pub/sub消息机制，这里之前的version号就很重要了，因为各种消息都是通过一个channel去发布和监听的，所以一个哨兵完成一次新的切换之后，新的master配置是跟着新的version号的，其他的哨兵都是根据版本号的大小来更新自己的master配置。 哨兵集群实战动手实操，练习如何操作部署哨兵集群，如何基于哨兵进行故障转移，还有一些企业级的配置方案。 哨兵的配置文件每一个哨兵都可以去监控多个maser-slaves的主从架构，相同的一套哨兵集群，可以去监控不同的多个redis主从集群，只需要给每个redis主从集群分配一个逻辑的名称。 sentinel.conf 12345678910111213141516# 指定对一个master的监控，给监控的master指定的一个名称，后面分布式集群架构里会讲到，可以配置多个master做数据拆分。sentinel monitor mymaster 127.0.0.1 6379 2# 超过多少毫秒跟一个redis实例断了连接，哨兵就可能认为这个redis实例挂了sentinel down-after-milliseconds mymaster 60000# 执行故障转移的timeout超时时长sentinel failover-timeout mymaster 180000# 新的master切换之后，同时有多少个slave被切换到去连接新master，重新做同步，数字越低，花费的时间越多sentinel parallel-syncs mymaster 1# 上面的三个配置，都是针对某个监控的master配置的，给其指定上面分配的名称即可sentinel monitor resque 192.168.1.3 6380 4sentinel down-after-milliseconds resque 10000sentinel failover-timeout resque 180000sentinel parallel-syncs resque 5sentinel monitor mymaster 127.0.0.1 6379 上面这段配置，就监控了两个master node。这是最小的哨兵配置，如果发生了master-slave故障转移，或者新的哨兵进程加入哨兵集群，那么哨兵会自动更新自己的配置文件。 sentinel monitor master-group-name hostname port quorum quorum的解释如下： 至少多少个哨兵要一致同意，master进程挂掉了，或者slave进程挂掉了，或者要启动一个故障转移操作 quorum是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作 假设有5个哨兵，quorum设置了2，那么如果5个哨兵中的2个都认为master挂掉了; 2个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移; 如果5个哨兵中有3个哨兵都是运行的，那么故障转移就会被允许执行 假设你的redis是1个master，4个slave，然后master宕机了，4个slave中有1个切换成了master，剩下3个slave就要挂到新的master上面去，这个时候，如果parallel-syncs是1，那么3个slave，一个一个地挂接到新的master上面去，1个挂接完，而且从新的master sync完数据之后，再挂接下一个。如果parallel-syncs是3，那么一次性就会把所有slave挂接到新的master上去。 在eshop-cache03上再部署一个Redis eshop-cache03是我本机安装的又一台虚拟机。 先安装好Redis，但是不用启动，接下来做哨兵的配置。 1234567891011wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gztar -xzvf tcl8.6.1-src.tar.gzcd /usr/local/tcl8.6.1/unix/./configure make &amp;&amp; make install使用redis-3.2.8.tar.gztar -zxvf redis-3.2.8.tar.gzcd redis-3.2.8make &amp;&amp; make testmake install 配置哨兵123mkdir /etc/sentinelmkdir -p /var/sentinel/5000vi /etc/sentinel/5000.conf 5000.conf 1234567891011121314151617181920212223port 5000bind 192.168.2.201dir /var/sentinel/5000sentinel monitor mymaster 192.168.2.201 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1port 5000bind 192.168.2.202dir /var/sentinel/5000sentinel monitor mymaster 192.168.2.201 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1port 5000bind 192.168.2.203dir /var/sentinel/5000sentinel monitor mymaster 192.168.2.201 6379 2sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 注意这是3段哨兵的配置，分别在我的3台虚拟机上进行配置。哨兵默认用26379端口，默认不能跟其他机器在指定端口连通，只能在本地访问，所以要改一下bind配置，把三台redis实例的ip都加上。 启动哨兵进程在eshop-cache01、eshop-cache02、eshop-cache03三台机器上，分别启动三个哨兵进程，组成一个集群，观察一下日志的输出。 12redis-sentinel /etc/sentinel/5000.confredis-server /etc/sentinel/5000.conf --sentinel 日志输出： 12341318:X 16 Feb 18:59:15.097 # +monitor master mymaster 192.168.2.201 6379 quorum 21318:X 16 Feb 18:59:15.099 * +slave slave 192.168.2.202:6379 192.168.2.202 6379 @ mymaster 192.168.2.201 63791318:X 16 Feb 18:59:15.177 * +sentinel sentinel 6f6009aac859757a296467f11f68af7284e4c9ff 192.168.2.202 5000 @ mymaster 192.168.2.201 63791318:X 16 Feb 18:59:16.861 * +sentinel sentinel 4fbf75c6fcbfdd09fe8460b6e12006561567f24d 192.168.2.203 5000 @ mymaster 192.168.2.201 6379 日志里会显示出来，每个哨兵都能去监控到对应的redis master，并能够自动发现对应的slave。 哨兵之间，互相会自动进行发现，用的就是之前说的pub/sub，消息发布和订阅channel消息系统和机制。 检查哨兵状态redis-cli -h 192.168.2.201 -p 5000 sentinel master mymasterSENTINEL slaves mymasterSENTINEL sentinels mymaster SENTINEL get-master-addr-by-name mymaster 哨兵管理和容灾演练哨兵节点的增加和删除如果是增加sentinel，会自动发现。 删除sentinel的步骤： 停止sentinel进程 SENTINEL RESET *，在所有sentinel上执行，清理所有的master状态 SENTINEL MASTER mastername，在所有sentinel上执行，查看所有sentinel对数量是否达成了一致 slave的永久下线让master摘除某个已经下线的slave：SENTINEL RESET mastername，在所有的哨兵上面执行. slave切换为Master的优先级slave-&gt;master选举优先级：slave-priority，值越小优先级越高 基于哨兵集群架构下的安全认证每个slave都有可能切换成master，所以每个实例都要配置两个指令 master上启用安全认证，requirepassmaster连接口令，masterauth sentinel配置：sentinel auth-pass &lt;master-group-name&gt; &lt;pass&gt; 容灾演练通过哨兵看一下当前的master：SENTINEL get-master-addr-by-name mymaster。 把master节点kill -9掉，pid文件也删除掉。 日志： 12345678910111336:X 16 Feb 22:05:18.458 # -sdown master mymaster 192.168.2.201 63791336:X 16 Feb 22:05:18.458 # -odown master mymaster 192.168.2.201 63791336:X 16 Feb 22:05:18.458 # +selected-slave slave 192.168.2.202:6379 192.168.2.202 6379 @ mymaster 192.168.2.201 63791336:X 16 Feb 22:05:18.458 * +failover-state-send-slaveof-noone slave 192.168.2.202:6379 192.168.2.202 6379 @ mymaster 192.168.2.201 63791336:X 16 Feb 22:05:18.559 * +failover-state-wait-promotion slave 192.168.2.202:6379 192.168.2.202 6379 @ mymaster 192.168.2.201 63791336:X 16 Feb 22:05:19.417 # +promoted-slave slave 192.168.2.202:6379 192.168.2.202 6379 @ mymaster 192.168.2.201 63791336:X 16 Feb 22:05:19.417 # +failover-state-reconf-slaves master mymaster 192.168.2.201 63791336:X 16 Feb 22:05:19.486 # +failover-end master mymaster 192.168.2.201 63791336:X 16 Feb 22:05:19.486 # +switch-master mymaster 192.168.2.201 6379 192.168.2.202 63791336:X 16 Feb 22:05:19.486 * +slave slave 192.168.2.201:6379 192.168.2.201 6379 @ mymaster 192.168.2.202 63791336:X 16 Feb 22:05:29.593 * +convert-to-slave slave 192.168.2.201:6379 192.168.2.201 6379 @ mymaster 192.168.2.202 6379 查看sentinel的日志，是否出现+sdown字样，识别出了master的宕机问题; 然后出现+odown字样，就是指定的quorum哨兵数量，都认为master宕机了。 三个哨兵进程都认为master是sdown了 超过quorum指定的哨兵进程都认为sdown之后，就变为odown 哨兵1是被选举为要执行后续的主备切换的那个哨兵 哨兵1去新的master（slave）获取了一个新的config version 尝试执行failover 投票选举出一个slave去切换成master，每个哨兵都会执行一次投票 failover-state-send-slaveof-noone，不让它去做任何节点的slave了; 把slave提拔成master; 旧的master认为不再是master了 哨兵就自动认为之前的201:6379变成了slave了，202:6379变成了master了 哨兵去探查了一下201:6379这个salve的状态，认为它sdown了 所有哨兵选举出了一个实例，来执行主备切换操作，可以看到投票的日志xxx voted for xxxx。如果majority的哨兵都存活着，那么就会执行主备切换操作，刚才日志里也看到了，+switch-master mymaster 192.168.2.201 6379 192.168.2.202 6379。 再通过哨兵看一下master：SENTINEL get-master-addr-by-name mymaster 1234[root@eshop-cache01 ~]# redis-cli -h 192.168.2.201 -p 5000192.168.2.201:5000&gt; SENTINEL get-master-addr-by-name mymaster1) "192.168.2.202"2) "6379" 可以看到master已经变成192.168.2.202:6379了，接下来我们试试故障恢复，再将旧的master重新启动，查看是否被哨兵自动切换成slave节点。 1[root@eshop-cache01 ~]# /etc/init.d/redis_6379 start 重新看一下202上的info replication： 12345192.168.2.202:6379&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=192.168.2.201,port=6379,state=online,offset=219972,lag=1 发现201它变成了一个slave了。 所以容灾的演练的步骤是： 手动杀掉master 哨兵能否执行主备切换，将slave切换为master 哨兵完成主备切换后，新的master能否使用 故障恢复，将旧的master重新启动 哨兵能否自动将旧的master变为slave，挂接到新的master上面去，而且也是可以使用的 哨兵的生产环境部署配置文件改成后台运行，然后把日志路径配置上。 daemonize yes logfile /var/log/sentinel/5000.log mkdir -p /var/log/sentinel 如何让Redis支持1T以上大数据单Master的redis在海量数据面前的瓶颈Master节点的数据和slave节点的数据是一样的，master最大能容纳多大的数据量，那么slave也就只能容纳多大的数据量。 Redis的缓存清理算法，将旧的很少使用的数据，给清除出内存，然后保证内存中，就只有固定大小的内存，不可能超过master内存的物理上线。 但是如果要让Redis保存1T以上的数据在缓存里，供系统高性能的查询和运行，在单机Master的情况下，目前几乎是不可能达到的。 怎么才能够突破单机瓶颈，让redis支撑海量数据？如果要支撑更大数据量的缓存，那就横向扩容更多的master节点，每个master节点就能存放更多的数据了，单台服务器是32GB，30台左右就可以支撑1T的数据量了。 Redis集群架构Redis集群架构支持N个master node，每个master node都可以挂载多个slave node，依然是读写分离的架构，对于每个master来说，写就写到master，然后读就从master对应的slave去读。 集群高可用：因为每个master都有slave节点，那么如果master挂掉，redis cluster的的机制，就会自动将某个slave切换成master。 redis cluster = 多master + 读写分离 + 高可用。 所以只需要基于redis cluster去搭建redis集群即可，不需要手工去搭建replication复制+主从架构+读写分离+哨兵集群+高可用。 redis cluster vs. replication + sentinel如果数据量很少，主要是为了承载高并发高性能的场景，比如你的缓存一般就几个G，单机足够了。 如果是replication架构，一个mater，多个slave，需要几个slave跟要求的读吞吐量有关系，然后搭建一个sentinel集群，去保证redis主从架构的高可用性，就能满足需求了。redis cluster主要是针对海量数据+高并发+高可用的场景，如果数据量很大，那么建议就用redis cluster。 分布式数据存储的核心算法随着技术的进步，算法的进阶： hash算法 -&gt; 一致性hash算法（memcached） -&gt; redis cluster，hash slot算法 用不同的算法，就决定了在多个master节点的时候，数据如何分布到这些节点上去。 hash算法 先是通过对key计算hash值，然后对节点数量（3）取模，取模结果一定是0~2之间，小于节点数量，然后根据索引去对应节点删取数据。如果某一个master宕机了，所有请求过来都会重新基于新的节点数量（2）去取模，此时所有数据都无法获取到，大量的流量会涌入到数据库中，几乎100%的缓存都可能失效了。 一致性hash算法 同样是先是通过对key计算hash值，然后用hash值落在圆环上的某个点，然后顺时针去寻找最近的一个节点。这个算法保证了如果某一台master宕机，只有之前那台master上的数据会受到影响，因为顺时针会找到下一个节点，还是找不到数据，此时只有1/3的数据找不到，流量会涌入到数据库中，重新查询一次。但是一致性hash算法也有一个问题，那就是换缓存热点数据问题，可能集中在某个hash区间的值特别多，会导致大量数据都涌入同一个master内，造成master的热点问题，性能出现瓶颈。 优化一致性hash算法 为了解决缓存热点数据问题，增加了虚拟节点的概念，如上图的黑色圆圈。给每个master都做了均匀分布的虚拟节点。这样的话，在每个区间内，大量的数据都会均分到不同的节点上，而不是按照顺时针的顺序去涌入同一个master内。 hash slot算法 Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽,举个例子,比如当前集群有3个节点,那么: * 节点 A 包含 0 到 5500号哈希槽 * 节点 B 包含5501 到 11000 号哈希槽 * 节点 C 包含11001 到 16384号哈希槽 这种结构很容易添加或者删除节点. 比如如果我想新添加个节点D, 我需要从节点 A, B, C中得部分槽到D上. 如果我像移除节点A,需要将A中得槽移到B和C节点上,然后将没有任何槽的A节点从集群中移除即可. 由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态. Redis Cluster介绍Redis 集群是一个提供在多个Redis间节点间共享数据的程序集。 Redis 集群的优势: 自动将数据进行分片，每个master上放一部分数据。 提供内置的高可用支持，部分master不可用时，还是可以继续工作。 Redis Cluster实战部署Redis Cluster会自动去做master+slave架构的复制和读写分离，以及master+slave的高可用+主备切换，支持多个master的hash slot分布式数据存储，所以我们之前的redis主从，哨兵集群，全部都不需要了。 Redis Cluster的重要配置12345cluster-enabled &lt;yes/no&gt;cluster-config-file &lt;filename&gt;：这是指定一个文件，供cluster模式下的redis实例保存集群状态，包括集群中其他机器的信息，比如节点的上线和下线，故障转移，不是我们去维护的，给它指定一个文件，让redis自己去维护。cluster-node-timeout &lt;milliseconds&gt;：节点存活超时时长，超过一定时长，认为节点宕机，master宕机的话就会触发主备切换，slave宕机就不会提供服务。 在三台机器上启动6个redis实例redis cluster集群，要求至少3个master去组成一个高可用，健壮的分布式的集群，每个master都建议至少给一个slave，所以3个master，3个slave，这是最少的要求。如果是正式环境下，建议在6台机器上去搭建，是为了保证每个master都跟自己的slave不在同一台机器上，自然是6台自然更好，否则机器挂了，一个master+一个slave就死了，集群也就不可用了。 我的虚拟机为了方便测试，使用3台机器去搭建6个redis实例的redis cluster。 先创建目录： 12mkdir -p /etc/redis-clustermkdir -p /var/log/redis 201： 12mkdir -p /var/redis/7001mkdir -p /var/redis/7002 202： 12mkdir -p /var/redis/7003mkdir -p /var/redis/7004 203： 12mkdir -p /var/redis/7005mkdir -p /var/redis/7006 写六份配置文件分别对应7001~7006，/etc/redis/7001.conf，每台机器上2个实例： 12345678910port 7001cluster-enabled yescluster-config-file /etc/redis-cluster/node-7001.confcluster-node-timeout 15000daemonize yes pidfile /var/run/redis_7001.pid dir /var/redis/7001 logfile /var/log/redis/7001.logbind 192.168.2.201appendonly yes 将上面的配置文件，在/etc/redis下放6个，分别为: 7001.conf，7002.conf，7003.conf，7004.conf，7005.conf，7006.conf，至少要用3个master节点启动，每个master加一个slave节点，先选择6个节点，启动6个实例。 准备生产环境的启动脚本在/etc/init.d下，放6个启动脚本，分别为: redis_7001, redis_7002, redis_7003, redis_7004, redis_7005, redis_7006。 123cd /etc/init.d/cp redis_6379 redis_7001vi redis_7001 将REDISPORT修改为7001~7006对应的端口号。 检查一下3台机器上的配置文件，目录是否都已经准备好，然后分别在3台机器上，启动6个redis实例。 123456[root@eshop-cache01 redis]# /etc/init.d/redis_7001 start[root@eshop-cache01 redis]# /etc/init.d/redis_7002 start[root@eshop-cache02 init.d]# /etc/init.d/redis_7003 start[root@eshop-cache02 init.d]# /etc/init.d/redis_7004 start[root@eshop-cache03 init.d]# /etc/init.d/redis_7005 start[root@eshop-cache03 init.d]# /etc/init.d/redis_7006 start 创建Redis集群创建集群的工具是用的redis-trib，它是用ruby写的，所以我们得先安装ruby环境。 1234567891011yum install –y gcc* openssl* wgetwget https://cache.ruby-lang.org/pub/ruby/2.3/ruby-2.3.1.tar.gztar -zxvf ruby-2.3.1.tar.gzcd ruby-2.3.1./configure -prefix=/usr/local/rubymake &amp;&amp; make installln -sf /usr/local/ruby/bin/* /usr/bin/wget http://rubygems.org/downloads/redis-3.3.0.gemgem install -l ./redis-3.3.0.gemgem list --check redis gem 我在安装的时候遇到一个错误 12345gem install -l ./redis-3.3.0.gemERROR: Loading command: install (LoadError) cannot load such file -- zlibERROR: While executing gem ... (NoMethodError) undefined method `invoke_with_build_args' for nil:NilClass 解决方法： 1234yum install zlib-develcd ruby-2.3.1/ext/zlib ruby ./extconf.rb make &amp;&amp; make install 然后安装： 123gem install -l ./redis-3.3.0.gemcp /usr/local/redis-4.0.8/src/redis-trib.rb /usr/local/binredis-trib.rb create --replicas 1 192.168.2.201:7001 192.168.2.201:7002 192.168.2.202:7003 192.168.2.202:7004 192.168.2.203:7005 192.168.2.203:7006 –replicas: 每个master有几个slave 6台机器，3个master，3个slave，尽量自己让master和slave不在一台机器上。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@eshop-cache01 local]# redis-trib.rb create --replicas 1 192.168.2.201:7001 192.168.2.201:7002 192.168.2.202:7003 192.168.2.202:7004 192.168.2.203:7005 192.168.2.203:7006&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:192.168.2.201:7001192.168.2.202:7003192.168.2.203:7005Adding replica 192.168.2.202:7004 to 192.168.2.201:7001Adding replica 192.168.2.203:7006 to 192.168.2.202:7003Adding replica 192.168.2.201:7002 to 192.168.2.203:7005M: 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972 192.168.2.201:7001 slots:0-5460 (5461 slots) masterS: 8861dda48f95e748bc0e7df2757cdc723c897f28 192.168.2.201:7002 replicates 5183cdee2295a07af3e98226887da2a645d979d1M: a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43 192.168.2.202:7003 slots:5461-10922 (5462 slots) masterS: cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.202:7004 replicates 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972M: 5183cdee2295a07af3e98226887da2a645d979d1 192.168.2.203:7005 slots:10923-16383 (5461 slots) masterS: 19f6027db2837cc56dd581a3c826a687d096207a 192.168.2.203:7006 replicates a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43Can I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join......&gt;&gt;&gt; Performing Cluster Check (using node 192.168.2.201:7001)M: 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972 192.168.2.201:7001 slots:0-5460 (5461 slots) master 1 additional replica(s)S: 8861dda48f95e748bc0e7df2757cdc723c897f28 192.168.2.201:7002 slots: (0 slots) slave replicates 5183cdee2295a07af3e98226887da2a645d979d1M: 5183cdee2295a07af3e98226887da2a645d979d1 192.168.2.203:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43 192.168.2.202:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.202:7004 slots: (0 slots) slave replicates 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972S: 19f6027db2837cc56dd581a3c826a687d096207a 192.168.2.203:7006 slots: (0 slots) slave replicates a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 可以检查一下集群的状态： 1redis-trib.rb check 192.168.2.201:7001 Redis Cluster测试接下来对刚才搭建的集群做一些测试，Redis Cluster提供了多个master，数据可以分布式存储在多个master上; 每个master都带着slave，自动就做读写分离; 某个master如果故障，那么就会自动将slave切换成master，从而达到高可用。 实验多master写入 -&gt; 海量数据的分布式存储1234567[root@eshop-cache01 local]# redis-cli -h 192.168.2.201 -p 7001192.168.2.201:7001&gt; set mykey1 v1OK192.168.2.201:7001&gt; set mykey2 v2(error) MOVED 14119 192.168.2.203:7005192.168.2.201:7001&gt; set mykey3 v3(error) MOVED 9990 192.168.2.202:7003 我们在redis cluster写入数据的时候，其实是可以将请求发送到任意一个master上去执行的。但是，每个master都会计算这个key对应的CRC16值，然后对16384个hash slot取模，找到key对应的hash slot，找到hash slot对应的master。如果对应的master就在自己本地的话，set mykey1 v1，mykey1这个key对应的hashslot就在自己本地，那么自己就处理掉了。但是如果计算出来的hashslot在其他master上，那么就会给客户端返回一个moved error，告诉你，你得到哪个master上去执行这条写入的命令。什么叫做多master的写入，就是每条数据只能存在于一个master上，不同的master负责存储不同的数据，分布式的数据存储。100w条数据，5个master，每个master就负责存储20w条数据，分布式数据存储。 所以我们需要去7005和7003实例上执行后面2条语句。 123456redis-cli -h 192.168.2.202 -p 7003192.168.2.202:7003&gt; set mykey3 v3OKredis-cli -h 192.168.2.203 -p 7005192.168.2.203:7005&gt; set mykey2 v2OK 实验不同master各自的slave读取 -&gt; 读写分离刚才是写入数据，现在我们去各自的从节点试试取数据，根据之前是日志分析，我们知道每台master的从节点信息如下： 123Adding replica 192.168.2.202:7004 to 192.168.2.201:7001Adding replica 192.168.2.203:7006 to 192.168.2.202:7003Adding replica 192.168.2.201:7002 to 192.168.2.203:7005 试试看，发现读不到，原来在redis cluster中，如果你要在slave读取数据，那么需要带先执行readonly指令，再get mykey1。 1234567redis-cli -h 192.168.2.202 -p 7004192.168.2.202:7004&gt; get mykey1(error) MOVED 1860 192.168.2.201:7001192.168.2.202:7004&gt; readonlyOK192.168.2.202:7004&gt; get mykey1"v1" 实际上Redis的客户端是可以帮我们自动路由的，只需要在连接的时候加上-c的参数。 1234567redis-cli -h 192.168.2.201 -p 7001 -c192.168.2.201:7001&gt; set mykey2 v2-&gt; Redirected to slot [14119] located at 192.168.2.203:7005OK192.168.2.203:7005&gt; get mykey1-&gt; Redirected to slot [1860] located at 192.168.2.201:7001"v1" 现在我们发现实验redis cluster的读写分离的时候，会发现有一定的限制性，因为默认情况下，redis cluster的核心的理念，主要是用slave做高可用的，每个master挂一两个slave，主要是做数据的热备，还有master故障时的主备切换，它的侧重点在高可用，而不是读写分离。 redis cluster默认是不支持slave节点读或者写的，跟我们手动基于replication搭建的主从架构不一样。想要在从节点上读取数据，必须要先执行readonly指令。 虽然Redis Cluster的主从架构出来了，但是要做读写分离，就复杂了一点，jedis客户端，对redis cluster的读写分离支持不太好。默认是读和写都到master上去执行，如果你要让最流行的jedis做redis cluster的读写分离的访问，那可能还得自己修改一点jedis的源码，成本比较高。要不然你就是自己基于jedis，封装一下，自己做一个redis cluster的读写分离的访问api。 核心的思路是这样：redis cluster就没有所谓的读写分离的概念了。读写分离是为了要建立一主多从的架构，才能横向任意扩展slave node去支撑更大的读吞吐量。redis cluster的架构下，实际上本身master就是可以任意扩展的，你如果要支撑更大的读吞吐量，或者写吞吐量，或者数据量，直接对master进行横向扩展就可以了，也能实现支撑更高的读吞吐的效果。 实验自动故障切换 -&gt; 高可用性我现在把201上的7001给杀掉，看202的7004是否会接替它的位置。 123456789101112131415161718192021redis-trib.rb check 192.168.2.201:7002&gt;&gt;&gt; Performing Cluster Check (using node 192.168.2.201:7002)S: 8861dda48f95e748bc0e7df2757cdc723c897f28 192.168.2.201:7002 slots: (0 slots) slave replicates 5183cdee2295a07af3e98226887da2a645d979d1M: a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43 192.168.2.202:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.202:7004 slots:0-5460 (5461 slots) master 0 additional replica(s)S: 19f6027db2837cc56dd581a3c826a687d096207a 192.168.2.203:7006 slots: (0 slots) slave replicates a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43M: 5183cdee2295a07af3e98226887da2a645d979d1 192.168.2.203:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 可以看到，202:7004已经变成了master，现在去7004上获取mykey1的值，看看是否能获取到。 123redis-cli -h 192.168.2.202 -p 7004192.168.2.202:7004&gt; get mykey1"v1" 再试着把201:7001给重新启动，它将自动作为slave挂载到了202:7004上面去。 1234567891011121314151617181920212223242526/etc/init.d/redis_7001 startStarting Redis server...redis-trib.rb check 192.168.2.201:7002&gt;&gt;&gt; Performing Cluster Check (using node 192.168.2.201:7002)S: 8861dda48f95e748bc0e7df2757cdc723c897f28 192.168.2.201:7002 slots: (0 slots) slave replicates 5183cdee2295a07af3e98226887da2a645d979d1M: a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43 192.168.2.202:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.202:7004 slots:0-5460 (5461 slots) master 1 additional replica(s)S: 19f6027db2837cc56dd581a3c826a687d096207a 192.168.2.203:7006 slots: (0 slots) slave replicates a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43S: 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972 192.168.2.201:7001 slots: (0 slots) slave replicates cc8a78087798e148b257d2ae33815a25715109e8M: 5183cdee2295a07af3e98226887da2a645d979d1 192.168.2.203:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 进一步验证，slave0:ip=192.168.2.201,port=7001,state=online,offset=4565,lag=1： 1234567891011121314redis-cli -h 192.168.2.202 -p 7004192.168.2.202:7004&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=192.168.2.201,port=7001,state=online,offset=4565,lag=1master_replid:38c489e10e3ede8290476aefec3e0ca9822f056emaster_replid2:6451a5d9f0c94fd5191b94898181424c39a24528master_repl_offset:4565second_repl_offset:4300repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:4565 Redis Cluster水平扩容之前说了不建议在Redis Cluster上做读写分离，建议直接对master进行水平扩容来横向扩展读写吞吐量，还有支撑海量数据。 假设redis单机，读吞吐是5w/s，写吞吐2w/s。扩展redis到5台master，读吞吐可以达到总量25w/s QPS，写可以达到10w/s QPS。扩容到5台master，能支撑的总的缓存数据量就是30G，40G，如果是100台，那就是600G，800G，甚至1T+的海量数据。 单机Redis的内存一般就6G、8G，如果内存太大，fork类操作的时候很耗时，会导致请求延时的问题。 Redis扩容方法： 加入新master203上执行： 12345678910111213141516171819202122mkdir -p /var/redis/7007cd /etc/rediscp 7006.conf 7007.confvi 7007.conf# 改一下里面的配置port 7007cluster-enabled yescluster-config-file /etc/redis-cluster/node-7007.confcluster-node-timeout 15000daemonize yes pidfile /var/run/redis_7007.pid dir /var/redis/7007 logfile /var/log/redis/7007.logbind 192.168.2.203appendonly yes cd /etc/init.d/cp redis_7006 redis_7007vi redis_7007# REDISPORT=7007 /etc/init.d/redis_7007 start 启动完成后，加入master，在201上执行： 123456789101112131415161718192021222324252627redis-trib.rb add-node 192.168.2.203:7007 192.168.2.201:7001&gt;&gt;&gt; Adding node 192.168.2.203:7007 to cluster 192.168.2.201:7001&gt;&gt;&gt; Performing Cluster Check (using node 192.168.2.201:7001)S: 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972 192.168.2.201:7001 slots: (0 slots) slave replicates cc8a78087798e148b257d2ae33815a25715109e8S: 19f6027db2837cc56dd581a3c826a687d096207a 192.168.2.203:7006 slots: (0 slots) slave replicates a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43M: cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.202:7004 slots:0-5460 (5461 slots) master 1 additional replica(s)M: 5183cdee2295a07af3e98226887da2a645d979d1 192.168.2.203:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 8861dda48f95e748bc0e7df2757cdc723c897f28 192.168.2.201:7002 slots: (0 slots) slave replicates 5183cdee2295a07af3e98226887da2a645d979d1M: a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43 192.168.2.202:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 192.168.2.203:7007 to make it join the cluster.[OK] New node added correctly. 确认一下，发现新加入的7007的master没有被分配任何的slot，所以还要需要处理： 123456789101112131415161718192021222324252627redis-trib.rb check 192.168.2.201:7001&gt;&gt;&gt; Performing Cluster Check (using node 192.168.2.201:7001)S: 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972 192.168.2.201:7001 slots: (0 slots) slave replicates cc8a78087798e148b257d2ae33815a25715109e8M: 5fe91cff7ab6c20b2e2ccc0815b0a7227119f52e 192.168.2.203:7007 slots: (0 slots) master 0 additional replica(s)S: 19f6027db2837cc56dd581a3c826a687d096207a 192.168.2.203:7006 slots: (0 slots) slave replicates a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43M: cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.202:7004 slots:0-5460 (5461 slots) master 1 additional replica(s)M: 5183cdee2295a07af3e98226887da2a645d979d1 192.168.2.203:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 8861dda48f95e748bc0e7df2757cdc723c897f28 192.168.2.201:7002 slots: (0 slots) slave replicates 5183cdee2295a07af3e98226887da2a645d979d1M: a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43 192.168.2.202:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. reshard一些数据过去resharding的意思就是把一部分hash slot从一些node上迁移到另外一些node上。 12345678910redis-trib.rb reshard 192.168.2.201:7001How many slots do you want to move (from 1 to 16384)? 4096What is the receiving node ID? 5fe91cff7ab6c20b2e2ccc0815b0a7227119f52ePlease enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs.Source node #1:cc8a78087798e148b257d2ae33815a25715109e8Source node #2:5183cdee2295a07af3e98226887da2a645d979d1Source node #3:a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43Source node #4:done 要把之前3个master算上，总共4096个hashslot迁移到新的第四个master上去， 添加node作为slave203执行： 12345678910111213141516171819202122mkdir -p /var/redis/7008cd /etc/rediscp 7006.conf 7008.confvi 7008.conf# 改一下里面的配置port 7008cluster-enabled yescluster-config-file /etc/redis-cluster/node-7008.confcluster-node-timeout 15000daemonize yes pidfile /var/run/redis_7008.pid dir /var/redis/7008 logfile /var/log/redis/7008.logbind 192.168.2.203appendonly yes cd /etc/init.d/cp redis_7006 redis_7008vi redis_7008# REDISPORT=7008 /etc/init.d/redis_7008 start 201执行，将新的节点挂载到7004cc8a78087798e148b257d2ae33815a25715109e8上面去： 1redis-trib.rb add-node --slave --master-id cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.203:7008 192.168.2.201:7001 删除node先用resharding将数据都移除到其他节点，确保node为空之后，才能执行remove操作，之前7007上是4096个slot，所以要移动3次，分别是移动1365个slot到7003，1365个slot到7004，1366个slot到7005上。 12345redis-trib.rb reshard 192.168.2.201:7001redis-trib.rb del-node 192.168.2.201:7001 5fe91cff7ab6c20b2e2ccc0815b0a7227119f52e&gt;&gt;&gt; Removing node 5fe91cff7ab6c20b2e2ccc0815b0a7227119f52e from cluster 192.168.2.201:7001&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node. 当你清空了一个master的hashslot时，redis cluster就会自动将其slave挂载到其他master上去，这个时候就只要删除掉master就可以了。 Redis Cluster的Slave自动迁移比如现在有10个master，每个有1个对应的slave，然后现在新增了3个slave作为冗余，有的master就有2个slave了，出现了salve冗余。这个时候如果某个master的slave挂了，那么redis cluster会自动迁移一个冗余的slave给那个挂掉slave的master。 它可以避免这样一个场景：如果你每个master只有一个slave，万一某一个slave死了，然后很快，master也死了，那可用性就降低了。但是如果你给整个集群挂载了一些冗余slave，那么某个master的slave死了，冗余的slave会被自动迁移过去，作为master的新slave，此时即使那个master也死了，还是有一个slave会切换成master的。 上面的实验中有一个master是有冗余slave的，直接让其他master其中的一个slave死掉，然后看有冗余slave会不会自动挂载到那个master，203:7005的master，冗余了一个slave。 123456789101112131415161718192021222324252627redis-trib.rb check 192.168.2.201:7001&gt;&gt;&gt; Performing Cluster Check (using node 192.168.2.201:7001)S: 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972 192.168.2.201:7001 slots: (0 slots) slave replicates 5183cdee2295a07af3e98226887da2a645d979d1S: 19f6027db2837cc56dd581a3c826a687d096207a 192.168.2.203:7006 slots: (0 slots) slave replicates a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43M: cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.202:7004 slots:1365-6825 (5461 slots) master 1 additional replica(s)S: 77aa78066b1a542e501bd9a0691f5f923529c482 192.168.2.203:7008 slots: (0 slots) slave replicates cc8a78087798e148b257d2ae33815a25715109e8M: 5183cdee2295a07af3e98226887da2a645d979d1 192.168.2.203:7005 slots:6826,10923-16383 (5462 slots) master 2 additional replica(s)S: 8861dda48f95e748bc0e7df2757cdc723c897f28 192.168.2.201:7002 slots: (0 slots) slave replicates 5183cdee2295a07af3e98226887da2a645d979d1M: a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43 192.168.2.202:7003 slots:0-1364,6827-10922 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 现在把203:7008给kill掉，并删除pidfile，这样202:7004就没有slave了，看看Redis Cluster会做些什么。 123456789101112131415161718192021222324redis-trib.rb check 192.168.2.201:7001&gt;&gt;&gt; Performing Cluster Check (using node 192.168.2.201:7001)S: 158414bbcaa2cf0b9b30a81d2e31fb35ba5b4972 192.168.2.201:7001 slots: (0 slots) slave replicates cc8a78087798e148b257d2ae33815a25715109e8S: 19f6027db2837cc56dd581a3c826a687d096207a 192.168.2.203:7006 slots: (0 slots) slave replicates a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43M: cc8a78087798e148b257d2ae33815a25715109e8 192.168.2.202:7004 slots:1365-6825 (5461 slots) master 1 additional replica(s)M: 5183cdee2295a07af3e98226887da2a645d979d1 192.168.2.203:7005 slots:6826,10923-16383 (5462 slots) master 1 additional replica(s)S: 8861dda48f95e748bc0e7df2757cdc723c897f28 192.168.2.201:7002 slots: (0 slots) slave replicates 5183cdee2295a07af3e98226887da2a645d979d1M: a7d09608d3669b0bff9152dc4c62fc2f8e5c2e43 192.168.2.202:7003 slots:0-1364,6827-10922 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 可以看到，之前203:7005是有2个slave的，Redis Cluster在7008挂掉以后，自动将201:7001作为slave挂载到了202:7004下面。 Redis Cluster核心原理节点间的内部通信机制gossip协议redis cluster节点间采取gossip协议进行通信，集群本身有很多元数据，比如hashslot和节点之间的映射，master和slave之间的关系，故障信息等等。 像集中式的存储，例如采用zookeeper集中式的维护和存储元数据。举个例子，集群元数据集中式存储的一个典型的代表，就是大数据领域里面的storm（分布式的大数据实时计算引擎），集中式的元数据存储架构底层是基于zookeeper（分布式协调中间件）的集群，这样来维护所有集群的元数据。 gossip跟集中式不同，不是将集群元数据（节点信息，故障，等等）集中存储在某个节点上，每个节点都持有一份元数据，互相之间不断通信，保持整个集群所有节点的数据是完整的。 集中式：好处在于，元数据的更新和读取，时效性非常好，一旦元数据出现了变更，立即就更新到集中式的存储中，其他节点读取的时候立即就可以感知到; 不好在于，所有的元数据的跟新压力全部集中在一个地方，可能会导致元数据的存储有压力。 gossip：好处在于元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续发到所有节点上去更新，有一定的延时，降低了压力; 缺点，元数据更新有延时，可能导致集群的一些操作会有一些滞后。 10000端口每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如7001，那么用于节点间通信的就是17001端口。每个节点每隔一段时间都会往另外几个节点发送ping消息，同时其他节点接收到ping之后返回pong。 交换的信息节点之间相互交换信息包括故障信息，节点的增加和移除，hash slot信息，等等。 gossip协议gossip协议包含多种消息，包括ping，pong，meet，fail，等等。 ping: 每个节点都会频繁给其他节点发送ping，其中包含自己的状态还有自己维护的集群元数据，互相通过ping交换元数据。 meet: 某个节点发送meet给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点进行通信。 redis-trib.rb add-node其实内部就是发送了一个gossip meet消息，给新加入的节点，通知那个节点去加入我们的集群。 pong: 返回ping和meet，包含自己的状态和其他信息，也可以用于信息广播和更新。 fail: 某个节点判断另一个节点fail之后，就发送fail给其他节点，通知其他节点，指定的节点宕机了。 ping很频繁，而且要携带一些元数据，所以可能会加重网络负担，每个节点每秒会执行10次ping，每次会选择5个最久没有通信的其他节点。当然如果发现某个节点通信延时达到了cluster_node_timeout / 2，那么立即发送ping，避免数据交换延时过长，落后的时间太长了，比如说，两个节点之间都10分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。cluster_node_timeout可以调整，如果值比较大，那么会降低发送的频率，每次ping的目的一个是带上自己节点的信息，还有就是带上1/10其他节点的信息一起发送出去，跟其他节点进行数据交换。每次至少发给3个其他节点，最多发送总节点-2个其他节点。 面向集群的jedis内部实现原理jedis cluster api与redis cluster集群交互的一些基本原理。 基于重定向的客户端 请求重定向 客户端可能会挑选任意一个redis实例去发送命令，每个redis实例接收到命令，都会计算key对应的hash slot，如果在本地就在本地处理，否则返回moved给客户端，让客户端进行重定向。cluster keyslot mykey，可以查看一个key对应的hash slot是什么。用redis-cli的时候，可以加入-c参数，支持自动的请求重定向，redis-cli接收到moved之后，会自动重定向到对应的节点执行命令。 计算hash slot 计算hash slot的算法，就是根据key计算CRC16值，然后对16384取模，拿到对应的hash slot。用hash tag可以手动指定key对应的slot，同一个hash tag下的key，都会在一个hash slot中，比如set mykey1:{100}和set mykey2:{100}。 hash slot查找 节点间通过gossip协议进行数据交换，这样就知道每个hash slot在哪个节点上。 smart jedis 什么是smart jedis 基于重定向的客户端，很消耗网络IO，因为大部分情况下，可能都会出现一次请求重定向，才能找到正确的节点。所以大部分的客户端，比如java redis客户端，就是jedis，都是smart的，本地维护一份hashslot -&gt; node的映射表，大部分情况下，直接走本地缓存就可以找到hashslot -&gt; node，不需要通过节点进行moved重定向。 JedisCluster的工作原理 在JedisCluster初始化的时候，就会随机选择一个node，初始化hashslot -&gt; node映射表，同时为每个节点创建一个JedisPool连接池。每次基于JedisCluster执行操作，首先JedisCluster都会在本地计算key的hashslot，然后在本地映射表找到对应的节点。如果那个node正好还是持有那个hashslot，那么就ok; 如果进行了reshard这样的操作，可能hashslot已经不在那个node上了，就会返回moved，那么利用该节点的元数据，更新本地的hashslot -&gt; node映射表缓存。重复上面几个步骤，直到找到对应的节点，如果重试超过5次，那么就报错，JedisClusterMaxRedirectionException。jedis老版本，可能会出现在集群某个节点故障还没完成自动切换恢复时，频繁更新hash slot，频繁ping节点检查活跃，导致大量网络IO开销，jedis最新版本，对于这些过度的hash slot更新和ping，都进行了优化，避免了类似问题。 hashslot迁移和ask重定向 如果hash slot正在迁移，那么会返回ask重定向给jedis，jedis接收到ask重定向之后，会重新定位到目标节点去执行，但是因为ask发生在hash slot迁移过程中，所以JedisCluster API收到ask是不会更新hashslot本地缓存。如果确定hashslot已经迁移完了，moved还是会更新本地hashslot-&gt;node映射表缓存的。 Redis Cluster高可用性与主备切换原理redis cluster的高可用的原理，几乎跟哨兵是类似的 判断节点宕机 如果一个节点认为另外一个节点宕机，那么就是pfail，主观宕机。 如果多个节点都认为另外一个节点宕机了，那么就是fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。 在cluster-node-timeout内，某个节点一直没有返回pong，那么就被认为pfail。 如果一个节点认为某个节点pfail了，那么会在gossip ping消息中，ping给其他节点，如果超过半数的节点都认为pfail了，那么就会变成fail。 从节点过滤 对宕机的master node，从其所有的slave node中，选择一个切换成master node。检查每个slave node与master node断开连接的时间，如果超过了cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成master，这个从节点超时过滤的步骤也是跟哨兵是一样。 从节点选举 哨兵：对所有从节点进行排序，先排slave priority，然后offset，最后是run id 每个从节点，都根据自己对master复制数据的offset，来设置一个选举时间，offset越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举 所有的master node开始slave选举投票，给要进行选举的slave进行投票，如果大部分master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成master 从节点执行主备切换，从节点切换为主节点 与哨兵比较 整个流程跟哨兵相比，非常类似，所以说redis cluster功能强大，直接集成了replication和sentinal的功能。 Redis在实践中的常见问题以及优化思路fork耗时导致高并发请求延时RDB和AOF的时候，其实生成RDB快照，AOF rewrite会有耗费磁盘IO的过程，主进程fork子进程。fork的时候，子进程是需要拷贝父进程的空间内存页表的，也是会耗费一定的时间的，一般来说，如果父进程内存有1个G的数据，那么fork可能会耗费在20ms左右，如果是10G~30G，那么就会耗费20 10，甚至20 30，也就是几百毫秒的时间。 info stats中的latest_fork_usec，可以看到最近一次fork的时长。redis单机QPS一般在几万，fork可能一下子就会拖慢几万条操作的请求时长，从几毫秒变成1秒。 优化思路： fork耗时跟redis主进程的内存有关系，一般控制redis的内存在10GB以内，slave -&gt; master，全量复制很耗时。 AOF的阻塞问题redis将数据写入AOF缓冲区，单独开一个线程做fsync操作，每秒一次。但是redis主线程会检查两次fsync的时间，如果距离上次fsync时间超过了2秒，那么数据写请求就会阻塞。everysec，最多丢失2秒的数据，一旦fsync超过2秒的延时，整个redis就被拖慢。 优化思路： 优化硬盘写入速度，建议采用SSD，不要用普通的机械硬盘，SSD，大幅度提升磁盘读写的速度。 主从复制延迟问题主从复制可能会超时严重，这个时候需要良好的监控和报警机制。在info replication中，可以看到master和slave复制的offset，做一个差值就可以看到对应的延迟量，如果延迟过多，那么就进行报警。这个问题主要是做好监控。 主从复制风暴问题如果一下子让多个slave从master去执行全量复制，一份大的rdb同时发送到多个slave，会导致网络带宽被严重占用。如果一个master真的要挂载多个slave，那尽量用树状结构，不要用星型结构。 vm.overcommit_memory0: 检查有没有足够内存，没有的话申请内存失败1: 允许使用内存直到用完为止2: 内存地址空间不能超过swap + 50% 如果是0的话，可能导致类似fork等操作执行失败，申请不到足够的内存空间 cat /proc/sys/vm/overcommit_memory echo &quot;vm.overcommit_memory=1&quot; &gt;&gt; /etc/sysctl.conf sysctl vm.overcommit_memory=1 swapinesscat /proc/version，查看linux内核版本 如果linux内核版本&lt;3.5，那么swapiness设置为0，这样系统宁愿swap也不会oom killer（杀掉进程）如果linux内核版本&gt;=3.5，那么swapiness设置为1，这样系统宁愿swap也不会oom killer 这样可以保证redis不会被杀掉 echo 0 &gt; /proc/sys/vm/swappiness echo vm.swapiness=0 &gt;&gt; /etc/sysctl.conf 最大打开文件句柄ulimit -n 10032 10032 去上网搜一下，不同的操作系统，版本，设置的方式都不太一样 tcp backlogcat /proc/sys/net/core/somaxconn echo 511 &gt; /proc/sys/net/core/somaxconn Redis总结如果你的数据量不大，单master就可以容纳，一般来说你的缓存的总量在10G以内就可以，那么建议按照以下架构去部署redis。 redis持久化+备份方案+容灾方案+replication（主从+读写分离）+sentinal（哨兵集群，3个节点，高可用性），可以支撑的数据量在10G以内，可以支撑的写QPS在几万左右，可以支撑的读QPS可以上10万以上（随你的需求，水平扩容slave节点就可以），可用性在99.99%。 如果你的数据量很大，比如（国内排名前三的大电商网站，x宝，x东，x宁易购），数据量是很大的，redis cluster多master分布式存储数据，可以水平扩容。如果要支撑更多的数据量，1T+以上没问题，只要扩容master即可，读写QPS分别都达到几十万都没问题，只要扩容master，redis cluster对读写分离支持不太好，需要执行readonly才能去slave上读。 Redis Cluster支撑99.99%可用性也没问题，slave -&gt; master的主备切换，冗余slave去进一步提升可用性的方案（每个master挂一个slave，但是整个集群再加个3个slave冗余一下）。]]></content>
      <categories>
        <category>高可用缓存架构实战</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用缓存架构实战2-Redis企业级应用实战]]></title>
    <url>%2F2018%2F02%2F12%2Fcache02%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程笔记，记录以供以后翻看 Redis企业应用实战上一篇说了redis的持久化的原理和操作，但是在企业中，持久化到底是怎么去用的呢？企业级的数据备份和各种灾难下的数据恢复，是怎么做的呢？ 企业级的持久化的配置策略在企业中，RDB的生成策略，用默认的配置也差不多。 save 60 10000：如果你希望尽可能确保，RDB最多丢1分钟的数据，那么尽量就是每隔1分钟都生成一个快照，但是低峰期数据量很少，也没必要这样设置。 1分内10000个key发生变更-&gt;生成RDB，1分内1000-&gt;RDB，这个根据应用和业务的数据量，自己去决定。 AOF一定要打开，fsync配置everysec auto-aof-rewrite-percentage 100: 就是当前AOF大小膨胀到超过上次100%，上次的两倍就重写。auto-aof-rewrite-min-size 64mb: 至少64m才重写，根据你的数据量来定，可改成16mb，32mb等等。 企业级的数据备份方案RDB非常适合做冷备，每次生成之后，就不会再有修改了 数据备份方案： 写crontab定时调度脚本去做数据备份 每小时都copy一份rdb的备份，到一个目录中去，仅仅保留最近48小时的备份 每天都保留一份当日的rdb的备份，到一个目录中去，仅仅保留最近1个月的备份 每次copy备份的时候，都把太旧的备份给删了 每天晚上将当前服务器上所有的数据备份，发送一份到远程的云服务上去 创建一个备份目录：/usr/local/redis，下面的脚本基于这个目录做备份。 每小时copy一次备份，删除48小时前的数据redis_rdb_copy_hourly.sh 123456789#!/bin/sh cur_date=`date +%Y%m%d%H`rm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_datedel_date=`date -d -48hour +%Y%m%d%H`rm -rf /usr/local/redis/snapshotting/$del_date 设置定时任务 123chmod 777 redis_rdb_copy_hourly.shcrontab -e0 * * * * sh /usr/local/redis/copy/redis_rdb_copy_hourly.sh 每天copy一次备份redis_rdb_copy_daily.sh 123456789#!/bin/sh cur_date=`date +%Y%m%d`rm -rf /usr/local/redis/snapshotting/$cur_datemkdir /usr/local/redis/snapshotting/$cur_datecp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_datedel_date=`date -d -1month +%Y%m%d`rm -rf /usr/local/redis/snapshotting/$del_date 设置定时任务 123chmod 777 redis_rdb_copy_daily.shcrontab -e0 0 * * * sh /usr/local/redis/copy/redis_rdb_copy_daily.sh 数据恢复方案 如果是redis进程挂掉，那么重启redis进程即可，直接基于AOF日志文件恢复数据，上一篇文章在AOF数据恢复那一块说过了，fsync everysec，最多就丢一秒的数据。 如果是redis进程所在机器挂掉，那么重启机器后，尝试重启redis进程，尝试直接基于AOF日志文件进行数据恢复 如果AOF没有破损，可以直接基于AOF恢复的。 AOF append-only，表示是顺序写入的，如果AOF文件破损，那么用redis-check-aof fix命令修复 如果redis当前最新的AOF和RDB文件出现了丢失/损坏，那么可以尝试基于该机器上当前的某个最新的RDB数据副本进行数据恢复，当前最新的AOF和RDB文件都出现了丢失/损坏到无法恢复，一般不是机器的故障，那可能是人为（不小心被删除）。 找到RDB最新的一份备份，小时级的备份可以了，小时级的肯定是最新的，copy到redis里面去，就可以恢复到某一个小时的数据（注意如果存在appendonly.aof文件，会优先读取appendonly.aof文件） 虽然删除了appendonly.aof，但是因为打开了aof持久化，redis就一定会优先基于aof去恢复，即使文件不在，那就会创建一个新的空的aof文件。正确的操作如下：停止redis，关闭aof，拷贝rdb备份，重启redis，确认数据恢复，直接在命令行热修改redis配置，打开aof，这个时候redis就会将内存中的数据对应的日志，写入aof文件中。此时aof和rdb两份数据文件的数据就同步了，在redis-cli中执行config set appendonly yes热修改配置参数，但是这个时候配置文件中的实际的参数没有被修改，再次停止redis，手动修改配置文件，打开aof的命令，再次重启redis。 如果当前机器上的所有RDB文件全部损坏，那么从远程的云服务上拉取最新的RDB快照回来恢复数据 如果是发现有重大的数据错误，比如某个小时上线的程序一下子将数据全部污染了，数据全错了，那么可以选择某个更早的时间点，对数据进行恢复。 如何通过读写分离来承载读请求QPS超过10万+？redis高并发跟整个系统的高并发之间的关系做高并发的话，不可避免的是需要把底层的缓存做好。如果是用mysql来做高并发，即使做到了，那么也是通过一系列复杂的分库分表，而且mysql高并发一般用在订单系统，事务要求比较高的地方，QPS能到几万，就已经比较高了。 但是如果要做一些电商的商品详情页，真正的超高并发，QPS上十万，甚至是百万，一秒钟百万的请求量，光是redis是不够的，但是redis是整个大型的缓存架构、支撑高并发的架构中，非常重要的一个环节。 首先，底层的缓存中间件，缓存系统，必须能够支撑的起我们说的那种高并发，其次，再经过良好的整体的缓存架构的设计（多级缓存架构、热点缓存），支撑真正的上十万，甚至上百万的高并发。 redis不能支撑高并发的瓶颈在哪里？单机是不能支撑高并发的，单机能够承载的QPS大概在上万或者几万不等，根据业务复杂度决定。 如果redis要支撑超过10万+的并发，那应该怎么做？单机的redis几乎不太可能说QPS超过10万+，除非一些特殊情况，比如你的机器性能特别好，配置特别高，物理机，维护做的特别好，而且你的整体的操作不是太复杂。 读写分离，一般都是用来支撑读高并发的，写的请求是比较少的，可能写请求也就一秒钟几千，一两千，大量的请求都是读，比如一秒钟二十万次读。 读写分离： 主从架构 -&gt; 读写分离 -&gt; 支撑10万+读QPS的架构 架构做成主从架构，一主多从，主负责写，并且将数据同步复制到其他的slave节点，从节点负责读，所有的读请求全部走从节点。 Redis Replication Redis 支持简单且易用的主从复制（master-slave replication）功能，当主Redis服务器更新数据时能将数据同步到从Redis服务器。 Redis Replication的核心机制 redis采用异步方式复制数据到slave节点，不过redis 2.8开始，slave node会周期性地确认自己每次复制的数据量 一个master node是可以配置多个slave node的 slave node也可以连接其他的slave node slave node做复制的时候，是不会block master node的正常工作的 slave node在做复制的时候，也不会block对自己的查询操作，它会用旧的数据集来提供服务; 但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了 slave node主要用来进行横向扩容，做读写分离，扩容的slave node可以提高读的吞吐量 master持久化对于主从架构的安全保障的意义如果采用了主从架构，那么必须开启master node的持久化。不建议用slave node作为master node的数据热备，因为那样的话，如果你关掉master的持久化，可能在master宕机重启的时候数据是空的，然后可能一经过复制，salve node数据也丢了。 master -&gt; RDB和AOF都关闭了 -&gt; 全部在内存中 master宕机，重启，是没有本地数据可以恢复的，然后就会直接认为自己的数据是空的，master就会将空的数据集同步到slave上去，所有slave的数据全部清空，100%的数据丢失，所以master节点必须要使用持久化机制。 主从架构的核心原理 当启动一个slave node的时候，它会发送一个PSYNC命令给master node 如果这时slave node重新连接master node，那么master node仅仅会复制给slave部分缺少的数据; 否则如果是slave node第一次连接master node，那么会触发一次full resynchronization 开始full resynchronization的时候，master会启动一个后台线程，开始生成一份RDB快照文件，同时还会将从客户端收到的所有写命令缓存在内存中。RDB文件生成完毕之后，master会将这个RDB发送给slave，slave会先写入本地磁盘，然后再从本地磁盘加载到内存中。然后master会将内存中缓存的写命令发送给slave，slave也会同步这些数据。 slave node如果跟master node有网络故障，断开了连接，会自动重连。master如果发现有多个slave node都来重新连接，仅仅会启动一个rdb save操作，用一份数据服务所有slave node。 主从复制的断点续传从redis 2.8开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。 master node会在内存中常见一个backlog，master和slave都会保存一个replica offset还有一个master id，offset就是保存在backlog中的。如果master和slave网络连接断掉了，slave会让master从上次的replica offset开始继续复制 但是如果没有找到对应的offset，那么就会执行一次full resynchronization 无磁盘化复制repl-diskless-sync no master在内存中直接创建rdb，然后发送给slave，不会在自己本地落地磁盘了 repl-diskless-sync-delay 5 会等待一定时长（上面是5秒）再开始复制，因为要等更多slave重新连接过来。 过期key处理slave不会过期key，只会等待master过期key。如果master过期了一个key，或者通过LRU淘汰了一个key，那么会模拟一条del命令发送给slave。 深入理解复制机制再次深入了解一下Redis的配置和复制机制。 复制的完整流程 slave node启动，仅仅保存master node的信息，包括master node的host和ip，但是复制流程没开始。 master的host和ip信息配置：redis.conf -&gt; slaveof配置。 slave node内部有个定时任务，每秒检查是否有新的master node要连接和复制，如果发现，就跟master node建立socket网络连接。 slave node发送ping命令给master node。 口令认证，如果master设置了requirepass，那么salve node必须发送masterauth的口令过去进行认证。 master node第一次执行全量复制，将所有数据发给slave node。 master node后续持续将写命令，异步复制给slave node 数据同步相关的核心机制指的就是第一次slave连接master的时候，执行的全量复制过程里面的一些细节的机制。 master和slave都会维护一个offset master会在自身不断累加offset，slave也会在自身不断累加offset，slave每秒都会上报自己的offset给master，同时master也会保存每个slave的offset。这个倒不是说特定就用在全量复制，主要是master和slave都要知道各自的数据的offset，才能知道互相之间的数据不一致的情况。 backlog backlog主要是用来做全量复制中断后的增量复制的，master node有一个backlog，默认是1MB大小，master node给slave node复制数据时，也会将数据在backlog中同步写一份。 master run_id介绍 info server可以看到master run_id，如果是根据host+ip定位master node，是不靠谱的，如果master node重启或者数据出现了变化，那么slave node应该根据不同的run_id区分，run_id不同就做全量复制，如果需要不更改run_id重启redis，可以使用redis-cli debug reload命令。 psync 从节点使用psync从master node进行复制，psync runid offset，master node会根据自身的情况返回响应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制。 全量复制rdb生成、rdb通过网络拷贝、slave旧数据的清理、slave aof rewrite，其实都很耗费时间，如果复制的数据量在4G~6G之间，那么很可能全量复制时间消耗到1分半到2分钟。 master执行bgsave，在本地生成一份rdb快照文件 master node将rdb快照文件发送给salve node，如果rdb复制时间超过60秒（repl-timeout），那么slave node就会认为复制失败，可以适当调节大这个参数。 对于千兆网卡的机器，一般每秒传输100MB，6G文件，很可能超过60s。 master node在生成rdb时，会将所有新的写命令缓存在内存中，在salve node保存了rdb之后，再将新的写命令复制给salve node。 配置解释：client-output-buffer-limit slave 256MB 64MB 60，如果在复制期间，内存缓冲区持续消耗超过64MB，或者一次性超过256MB，那么停止复制，复制失败。 slave node接收到rdb之后，清空自己的旧数据，然后重新加载rdb到自己的内存中，同时基于旧的数据版本对外提供服务。 如果slave node开启了AOF，那么会立即执行BGREWRITEAOF，重写AOF。 增量复制 如果全量复制过程中，master-slave网络连接断掉，那么slave重新连接master时，会触发增量复制。 master直接从自己的backlog中获取部分丢失的数据，发送给slave node，默认backlog就是1MB。 msater就是根据slave发送的psync中的offset来从backlog中获取数据的。 heartbeat主从节点互相都会发送heartbeat信息，master默认每隔10秒发送一次heartbeat，salve node每隔1秒发送一个heartbeat。 异步复制master每次接收到写命令之后，现在内部写入数据，然后异步发送给slave node。 Redis读写分离架构实战上面的内容都是在铺垫各种redis replication的原理和知识，主从架构，读写分离。接下来就跟随文章进行项目应用实战，搭建一个一主一从的架构，主节点去写，从节点去读。 配置从节点，需要在从节点的redis.conf配置文件中修改slaveof ip port。 我这里配置成slaveof eshop-cache01 6379。 强制读写分离配置：slave节点只读是默认开启的，slave-read-only yes，开启了只读的redis slave node，会拒绝所有的写操作，这样可以强制搭建成读写分离的架构。 如果master有密码，还需要配置masterauth，来授权从节点访问。 读写分离架构的测试先启动主节点，eshop-cache01上的redis实例，再启动从节点，eshop-cache02上的redis实例。（eshop-cache01和eshop-cache02是我配置的2台虚拟机。） !!千万要注意一点，redis的默认配置中，是处于开发模式的，从别的机器是无法访问的，所以要修改bind配置，把它先注释掉吧。 bind 127.0.0.1 然后用redis-cli连接从节点，info replication可以看到当前节点集群的配置信息。 1234567891011121314151617181920127.0.0.1:6379&gt; info replication# Replicationrole:slavemaster_host:eshop-cache01master_port:6379master_link_status:upmaster_last_io_seconds_ago:6master_sync_in_progress:0slave_repl_offset:84slave_priority:100slave_read_only:1connected_slaves:0master_replid:081336e1355373b31908914d2a9ac195b0ca2622master_replid2:0000000000000000000000000000000000000000master_repl_offset:84second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:84 连接主节点： 12345678910111213127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=192.168.2.202,port=6379,state=online,offset=714,lag=1master_replid:081336e1355373b31908914d2a9ac195b0ca2622master_replid2:0000000000000000000000000000000000000000master_repl_offset:714second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:714 在主节点set k1 v2，从节点get k1，马上就能看到结果，数据已经同步过去了。 QPS测试如果想要对刚才搭建的Redis集群做一个基准的测试，测一下性能和QPS(query per second)。Redis自己提供了redis-benchmark压测工具，这是最快捷最方便的，当然了，这个工具比较简单，只能用一些简单的操作和场景去压测。 对redis读写分离架构进行压测，单实例写QPS+单实例读QPS(我是1核1G的虚拟机) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274redis-4.0.8/src/redis-benchmark -h localhost-c &lt;clients&gt; Number of parallel connections (default 50)-n &lt;requests&gt; Total number of requests (default 100000)-d &lt;size&gt; Data size of SET/GET value in bytes (default 2)====== PING_INLINE ====== 100000 requests completed in 1.05 seconds 50 parallel clients 3 bytes payload keep alive: 199.81% &lt;= 1 milliseconds99.93% &lt;= 2 milliseconds100.00% &lt;= 2 milliseconds95602.30 requests per second====== PING_BULK ====== 100000 requests completed in 1.14 seconds 50 parallel clients 3 bytes payload keep alive: 199.57% &lt;= 1 milliseconds99.87% &lt;= 2 milliseconds99.99% &lt;= 3 milliseconds100.00% &lt;= 3 milliseconds87489.06 requests per second====== SET ====== 100000 requests completed in 1.23 seconds 50 parallel clients 3 bytes payload keep alive: 198.93% &lt;= 1 milliseconds99.75% &lt;= 2 milliseconds100.00% &lt;= 3 milliseconds81499.59 requests per second====== GET ====== 100000 requests completed in 1.30 seconds 50 parallel clients 3 bytes payload keep alive: 196.70% &lt;= 1 milliseconds99.20% &lt;= 2 milliseconds99.78% &lt;= 3 milliseconds99.97% &lt;= 4 milliseconds100.00% &lt;= 5 milliseconds100.00% &lt;= 5 milliseconds76863.95 requests per second====== INCR ====== 100000 requests completed in 1.51 seconds 50 parallel clients 3 bytes payload keep alive: 195.39% &lt;= 1 milliseconds98.90% &lt;= 2 milliseconds99.73% &lt;= 3 milliseconds99.93% &lt;= 4 milliseconds100.00% &lt;= 4 milliseconds66093.85 requests per second====== LPUSH ====== 100000 requests completed in 1.36 seconds 50 parallel clients 3 bytes payload keep alive: 197.80% &lt;= 1 milliseconds99.54% &lt;= 2 milliseconds99.81% &lt;= 3 milliseconds99.86% &lt;= 4 milliseconds99.89% &lt;= 5 milliseconds99.90% &lt;= 6 milliseconds99.90% &lt;= 8 milliseconds99.93% &lt;= 9 milliseconds99.95% &lt;= 10 milliseconds99.97% &lt;= 11 milliseconds99.98% &lt;= 12 milliseconds100.00% &lt;= 12 milliseconds73260.07 requests per second====== RPUSH ====== 100000 requests completed in 1.21 seconds 50 parallel clients 3 bytes payload keep alive: 199.79% &lt;= 1 milliseconds100.00% &lt;= 1 milliseconds82781.45 requests per second====== LPOP ====== 100000 requests completed in 1.24 seconds 50 parallel clients 3 bytes payload keep alive: 199.19% &lt;= 1 milliseconds99.73% &lt;= 2 milliseconds99.92% &lt;= 3 milliseconds99.95% &lt;= 4 milliseconds99.99% &lt;= 5 milliseconds100.00% &lt;= 5 milliseconds80645.16 requests per second====== RPOP ====== 100000 requests completed in 1.15 seconds 50 parallel clients 3 bytes payload keep alive: 199.67% &lt;= 1 milliseconds99.84% &lt;= 2 milliseconds99.91% &lt;= 3 milliseconds99.95% &lt;= 4 milliseconds99.97% &lt;= 5 milliseconds100.00% &lt;= 5 milliseconds86730.27 requests per second====== SADD ====== 100000 requests completed in 1.72 seconds 50 parallel clients 3 bytes payload keep alive: 194.97% &lt;= 1 milliseconds98.39% &lt;= 2 milliseconds99.20% &lt;= 3 milliseconds99.50% &lt;= 4 milliseconds99.58% &lt;= 5 milliseconds99.59% &lt;= 6 milliseconds99.63% &lt;= 7 milliseconds99.67% &lt;= 8 milliseconds99.68% &lt;= 9 milliseconds99.74% &lt;= 10 milliseconds99.75% &lt;= 11 milliseconds99.75% &lt;= 12 milliseconds99.76% &lt;= 13 milliseconds99.78% &lt;= 14 milliseconds99.83% &lt;= 15 milliseconds99.83% &lt;= 16 milliseconds99.86% &lt;= 17 milliseconds99.89% &lt;= 23 milliseconds99.90% &lt;= 24 milliseconds99.93% &lt;= 25 milliseconds99.93% &lt;= 27 milliseconds99.93% &lt;= 31 milliseconds99.94% &lt;= 33 milliseconds99.95% &lt;= 41 milliseconds99.95% &lt;= 42 milliseconds99.95% &lt;= 43 milliseconds99.96% &lt;= 44 milliseconds99.99% &lt;= 46 milliseconds99.99% &lt;= 49 milliseconds100.00% &lt;= 52 milliseconds100.00% &lt;= 55 milliseconds100.00% &lt;= 56 milliseconds58038.30 requests per second====== HSET ====== 100000 requests completed in 1.26 seconds 50 parallel clients 3 bytes payload keep alive: 199.09% &lt;= 1 milliseconds99.97% &lt;= 2 milliseconds100.00% &lt;= 2 milliseconds79617.84 requests per second====== SPOP ====== 100000 requests completed in 1.25 seconds 50 parallel clients 3 bytes payload keep alive: 199.30% &lt;= 1 milliseconds99.97% &lt;= 2 milliseconds100.00% &lt;= 2 milliseconds80000.00 requests per second====== LPUSH (needed to benchmark LRANGE) ====== 100000 requests completed in 1.27 seconds 50 parallel clients 3 bytes payload keep alive: 198.56% &lt;= 1 milliseconds99.89% &lt;= 2 milliseconds99.97% &lt;= 3 milliseconds100.00% &lt;= 4 milliseconds78616.35 requests per second====== LRANGE_100 (first 100 elements) ====== 100000 requests completed in 1.17 seconds 50 parallel clients 3 bytes payload keep alive: 198.44% &lt;= 1 milliseconds99.83% &lt;= 2 milliseconds99.96% &lt;= 3 milliseconds99.99% &lt;= 4 milliseconds100.00% &lt;= 4 milliseconds85616.44 requests per second====== LRANGE_300 (first 300 elements) ====== 100000 requests completed in 1.24 seconds 50 parallel clients 3 bytes payload keep alive: 197.47% &lt;= 1 milliseconds99.55% &lt;= 2 milliseconds99.76% &lt;= 3 milliseconds99.89% &lt;= 4 milliseconds99.95% &lt;= 5 milliseconds99.95% &lt;= 15 milliseconds99.95% &lt;= 16 milliseconds99.98% &lt;= 17 milliseconds100.00% &lt;= 18 milliseconds100.00% &lt;= 28 milliseconds100.00% &lt;= 28 milliseconds80906.15 requests per second====== LRANGE_500 (first 450 elements) ====== 100000 requests completed in 1.16 seconds 50 parallel clients 3 bytes payload keep alive: 199.01% &lt;= 1 milliseconds99.78% &lt;= 2 milliseconds99.88% &lt;= 3 milliseconds99.99% &lt;= 4 milliseconds100.00% &lt;= 4 milliseconds86355.79 requests per second====== LRANGE_600 (first 600 elements) ====== 100000 requests completed in 1.08 seconds 50 parallel clients 3 bytes payload keep alive: 199.17% &lt;= 1 milliseconds99.82% &lt;= 2 milliseconds99.90% &lt;= 3 milliseconds99.96% &lt;= 4 milliseconds99.98% &lt;= 5 milliseconds100.00% &lt;= 8 milliseconds100.00% &lt;= 8 milliseconds92250.92 requests per second====== MSET (10 keys) ====== 100000 requests completed in 1.28 seconds 50 parallel clients 3 bytes payload keep alive: 196.11% &lt;= 1 milliseconds98.95% &lt;= 2 milliseconds99.49% &lt;= 3 milliseconds99.70% &lt;= 4 milliseconds99.76% &lt;= 5 milliseconds99.86% &lt;= 6 milliseconds99.94% &lt;= 7 milliseconds99.96% &lt;= 8 milliseconds99.98% &lt;= 10 milliseconds100.00% &lt;= 10 milliseconds78369.91 requests per second 到了环境中，QPS就不一定能有这么高了，因为网络本身就有开销，还有QPS两大杀手，一个是lrange操作，还有就是如果value很大，也比较费时。 水平扩容redis读节点，提升度吞吐量 再在其他服务器上搭建redis从节点，单个从节点读请QPS在5万左右，两个redis从节点，所有的读请求到两台机器上去，承载整个集群读QPS在10万+。]]></content>
      <categories>
        <category>高可用缓存架构实战</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用缓存架构实战1-Redis配置和持久化]]></title>
    <url>%2F2018%2F02%2F12%2Fcache01%2F</url>
    <content type="text"><![CDATA[此为龙果学院课程笔记，记录以供以后翻看 RedisRedis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 Redis与其他key-value缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 Redis持久化的意义Redis的安装：centos7安装redis4.0.10并进行生产环境部署。 都知道redis数据可以做持久化，但是有个问题，redis的持久化意义是什么？redis的持久化方式RDB，AOF的区别，各自的特点是什么，适合什么场景，redis的企业级的持久化方案是什么，是用来跟哪些企业级的场景结合起来使用的？ Redis持久化的意义在于故障恢复，比如你部署了一个redis作为cache缓存，当然也可以保存一些较为重要的数据，如果没有持久化的话，redis遇到灾难性故障的时候，就会丢失所有的数据，如果通过持久化将数据存一份在磁盘上，然后定期同步和备份到云存储服务上去，那么就可以保证数据不丢失全部，可以恢复一部分数据回来。 持久化和高可用的关系企业级redis集群架构：海量数据、高并发、高可用。 所以对于一个企业级的redis架构来说，持久化是必不可少的。持久化主要是做灾难恢复，数据恢复，也可以归类到高可用的一个环节里面去。 如果redis整个挂了，redis就不可用了，你要做的事情是让redis变得可用，尽快变得可用。重启redis，尽快让它对外提供服务，如果你没做数据备份，这个时候redis启动了，也不可用，数据都没了。 很可能大量的请求过来，缓存全部无法命中，在redis里根本找不到数据，这个时候就死定了，缓存雪崩，所有请求都没有在redis命中，就会去mysql数据库这种数据源头中去找，一下子mysql承接高并发，然后就挂了。如果mysql再挂掉，你都没法去找数据恢复到redis里面去，因为redis的数据从哪来？从mysql来的。 如果你把redis的持久化做好，备份和恢复方案做到企业级的程度，那么即使你的redis故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务。 所以说Redis的持久化跟高可用，是密切相关的。 Redis持久化：RDB和AOF两种持久化机制的介绍 RDB持久化机制，对redis中的数据执行周期性的持久化。 AOF机制对每条写入命令作为日志，以append-only的模式写入一个日志文件中，在redis重启的时候，可以通过回放AOF日志中的写入指令来重新构建整个数据集。 如果我们想要redis仅仅作为纯内存的缓存来用，那么可以禁止RDB和AOF所有的持久化机制。 通过RDB或AOF，都可以将redis内存中的数据给持久化到磁盘，然后可以将这些数据备份到别的地方去，比如阿里云或者其他云服务。 如果redis挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动redis，redis就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务. 如果同时使用RDB和AOF两种持久化机制，那么在redis重启的时候，会使用AOF来重新构建数据，因为AOF中的数据更加完整 RDB持久化机制的优点1.RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说Amazon的S3云服务上去，在国内可以是阿里云的ODPS分布式存储上，以预定好的备份策略来定期备份redis中的数据。 RDB可以做冷备，生成多个文件，每个文件都代表了某一个时刻的完整的数据快照。 AOF也可以做冷备，只有一个文件，但是可以每隔一定时间复制一份这个文件出来。 RDB做冷备，优势在哪儿呢？由redis去控制固定时长生成快照文件的事情，比较方便; AOF的话还需要自己写一些各种定时脚本去做这个事情。RDB数据做冷备，在最坏的情况下，提供数据恢复的时候，速度比AOF快。 2.RDB对redis对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可 RDB：每次写都是直接写redis内存，只是在一定的时候，才会将数据写入磁盘中。 AOF：每次都是要写文件中，虽然可以快速写入os cache中，但是还是有一定的时间开销，速度肯定比RDB略慢一些。 3.相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis进程，更加快速 AOF：存放的指令日志，做数据恢复的时候，其实是要回放和执行所有的指令日志，来恢复出来内存中的所有数据。 RDB：就是一份数据文件，恢复的时候，直接加载到内存中。 结合上述优点，RDB特别适合做冷备。 RDB持久化机制的缺点 如果想要在redis故障时，尽可能少的丢失数据，那么RDB没有AOF好。一般来说，RDB数据快照文件，都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据。 RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF持久化机制的优点 AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据。 AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite log的时候，会对其进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的merge后的日志文件ready的时候，再交换新老日志文件即可。 AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据。 AOF持久化机制的缺点 对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大。 AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的，如果你要保证一条数据都不丢，也是可以的，AOF的fsync设置成每写入一条数据，fsync一次，那就完蛋了，redis的QPS大降。 以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似AOF这种较为复杂的基于命令日志/merge/回放的方式，比基于RDB每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有bug。不过AOF就是为了避免rewrite过程导致的bug，因此每次rewrite并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 唯一的比较大的缺点，其实就是做数据恢复的时候，会比较慢，还有做冷备，定期的备份，不太方便，可能要自己手写复杂的脚本去做，做冷备不太合适 RDB和AOF到底该如何选择 不要仅仅使用RDB，因为那样会导致你丢失很多数据。 也不要仅仅使用AOF，因为那样有两个问题，第一，通过AOF做冷备，没有RDB做冷备，来的恢复速度更快; 第二，RDB每次简单粗暴生成数据快照，更加健壮，可以避免AOF这种复杂的备份和恢复机制的bug。 综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择; 用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复。 持久化配置和数据恢复实验如何配置RDB持久化机制在redis的配置文件里，save配置项用来配置RDB。 redis.conf文件，也就是/etc/redis/6379.conf，去配置持久化 这是默认的配置： 123save 900 1save 300 10save 60 10000 每隔60s，如果有超过10000个key发生了变更，那么就生成一个新的dump.rdb文件，就是当前redis内存中完整的数据快照，这个操作也被称之为snapshotting，快照。 也可以手动调用save或者bgsave命令，同步或异步执行rdb快照生成。 save可以设置多个，就是多个snapshotting检查点，每到一个检查点，就会去check一下，是否有指定的key数量发生了变更，如果有，就生成一个新的dump.rdb文件。 RDB持久化机制的工作流程 redis根据配置自己尝试去生成rdb快照文件 fork一个子进程出来 子进程尝试将数据dump到临时的rdb快照文件中 完成rdb快照文件的生成之后，就替换之前的旧的快照文件 dump.rdb，每次生成一个新的快照，都会覆盖之前的老快照 基于RDB持久化机制的数据恢复实验 在redis中保存几条数据，立即用命令停掉redis，然后重启redis，看看刚才插入的数据还在不在 经过测试发现数据还在，为什么？通过redis-cli SHUTDOWN这种方式去停掉redis，其实是一种安全退出的模式，redis在退出的时候会将内存中的数据立即生成一份完整的rdb快照。 在redis中再保存几条新的数据，用kill -9粗暴杀死redis进程，模拟redis故障异常退出，导致内存数据丢失的场景。 这次就发现，redis进程异常被杀掉，数据没有进dump文件，几条最新的数据就丢失了 手动设置一个save检查点，save 5 1，5秒检查一次，有一条数据就写入磁盘，写入几条数据，等待5秒钟，会发现自动进行了一次dump rdb快照，在dump.rdb中发现了数据。异常停掉redis进程，再重新启动redis，看刚才插入的数据还在. AOF持久化的配置AOF持久化，默认是关闭的，默认是打开RDB持久化。 appendonly yes属性，可以打开AOF持久化机制，在生产环境里面，一般来说AOF都是要打开的，除非你说随便丢个几分钟的数据也无所谓，打开AOF持久化机制之后，redis每次接收到一条写命令，就会写入日志文件中，当然是先写入os cache的，然后每隔一定时间再fsync一下。 而且即使AOF和RDB都开启了，redis重启的时候，也是优先通过AOF进行数据恢复的，因为aof数据比较完整。 可以配置AOF的fsync策略，有三种策略可以选择:一种是每次写入一条数据就执行一次fsync; 一种是每隔一秒执行一次fsync; 一种是不主动执行fsync。 always: 每次写入一条数据，立即将这个数据对应的写日志fsync到磁盘上去，性能非常非常差，吞吐量很低; 如果要确保redis里的数据一条都不丢，那就只能这样了。 为什么说性能差，这里用mysql和redis比较： 12mysql -&gt; 内存策略，大量磁盘操作，QPS最多到1、2k。(QPS，每秒钟的请求数量)redis -&gt; 内存，磁盘持久化，QPS一般来说上万QPS没问题。 everysec: 每秒将os cache中的数据fsync到磁盘，这个是最常用的，生产环境一般都这么配置，性能很高，QPS还是可以上万的。 no: 仅仅redis负责将数据写入os cache就撒手不管了，然后后面os自己会时不时有自己的策略将数据刷入磁盘，不可控。 AOF持久化的数据恢复实验 先仅仅打开RDB，写入一些数据，然后kill -9杀掉redis进程，接着重启redis，发现数据没了，因为RDB快照还没生成 打开AOF的开关，启用AOF持久化 写入一些数据，观察AOF文件中的日志内容 其实在appendonly.aof文件中，可以看到刚写的日志，它们其实就是先写入os cache的，然后1秒后才fsync到磁盘中，只有fsync到磁盘中了，才是安全的，要不然光是在os cache中，机器只要重启，就什么都没了。 kill -9杀掉redis进程，重新启动redis进程，发现数据被恢复回来了，就是从AOF文件中恢复回来的。 Redis进程启动的时候，直接就会从appendonly.aof中加载所有的日志，把内存中的数据恢复回来。 AOF rewriteredis中的数据其实有限的，很多数据可能会自动过期，可能会被用户删除，可能会被redis用缓存清除的算法清理掉。redis中的数据会不断淘汰掉旧的，就一部分常用的数据会被自动保留在redis内存中。所以可能很多之前的已经被清理掉的数据，对应的写日志还停留在AOF中，AOF日志文件就一个，会不断的膨胀，到很大很大。 所以AOF会自动在后台每隔一定时间做rewrite操作，比如日志里已经存放了针对100w数据的写日志了; redis内存只剩下10万; 基于内存中当前的10万数据构建一套最新的日志，到AOF中; 覆盖之前的老日志; 确保AOF日志文件不会过大，保持跟redis内存数据量一致。 redis2.4之前，还需要手动开发一些脚本、crontab，通过BGREWRITEAOF命令去执行AOF rewrite，但是redis2.4之后，会自动进行rewrite操作。 在redis.conf中，可以配置rewrite策略： 12auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 比如说上一次AOFrewrite之后，是128mb，然后就会接着128mb继续写AOF的日志，如果发现增长的比例超过了之前的100%，有256mb了，就可能会去触发一次rewrite。 但是此时还要去跟auto-aof-rewrite-min-size，也就是64mb去比较，256mb &gt; 64mb，才会去触发rewrite。 rewrite过程： redis fork一个子进程 子进程基于当前内存中的数据，构建日志，开始往一个新的临时的AOF文件中写入日志 redis主进程，接收到client新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的AOF文件 子进程写完新的日志文件之后，redis主进程将内存中的新日志再次追加到新的AOF文件中 用新的日志文件替换掉旧的日志文件 AOF破损文件的修复如果redis在append数据到AOF文件时，机器宕机了，可能会导致AOF文件破损 用redis-check-aof --fix命令来修复破损的AOF文件 AOF和RDB同时工作 如果RDB在执行snapshotting操作，那么redis不会执行AOF rewrite; 如果redis再执行AOF rewrite，那么就不会执行RDB snapshotting 如果RDB在执行snapshotting，此时用户执行BGREWRITEAOF命令，那么等RDB快照生成之后，才会去执行AOF rewrite 同时有RDB snapshot文件和AOF日志文件，那么redis重启的时候，会优先使用AOF进行数据恢复，因为其中的日志更完整 小实验 在有rdb的dump和aof的appendonly的同时，rdb里也有部分数据，aof里也有部分数据，这个时候其实会发现，rdb的数据不会恢复到内存中 我们模拟让aof破损，然后fix，有一条数据会被fix删除 再次用fix的aof文件去重启redis，发现数据只剩下一条了 数据恢复完全是依赖于底层的磁盘的持久化的，主要rdb和aof上都没有数据，那就没了]]></content>
      <categories>
        <category>高可用缓存架构实战</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB6-分片]]></title>
    <url>%2F2018%2F01%2F17%2Fmongo6%2F</url>
    <content type="text"><![CDATA[分片在Mongodb里面存在另一种集群，就是分片技术,可以满足MongoDB数据量大量增长的需求。 当MongoDB存储海量的数据时，一台机器可能不足以存储数据，也可能不足以提供可接受的读写吞吐量。这时，我们就可以通过在多台机器上分割数据，使得数据库系统能存储和处理更多的数据。 为什么使用分片 复制所有的写入操作到主节点 延迟的敏感数据会在主节点查询 单个副本集限制在12个节点 当请求量巨大时会出现内存不足。 本地磁盘不足 垂直扩展价格昂贵 分片组件 Sharded Cluster（分片集群）共有3个组件： shard（也可以配置成副本集，确保高可用） query router（查询路由器） config sever（配置服务器，一般会配置成副本集，确保高可用；以前的本本才去的方式使用3台独立的mongod实例） config server保存了数据的分布情况，哪些数据在哪一个分片中，而query router提供了用户接口对分片进行操作。 分片实战注意：分片是以集合为目标的 本机部署测试 配置服务器：是一个副本集，由3台mongod实例构成 mongos路由：两台mongos实例 shard节点：两个shard构成，每个shard都是一个副本集（包含了3个mongod实例） 11台机器。 启动配置服务器 不能拥有arbiter 不能拥有delayed member 必须能够build indexes（buildIndexes属性不能为false） 先准备目录： 123456789101112➜ mongodb tree sharding sharding└── config_server ├── config1 │ ├── data │ └── log ├── config2 │ ├── data │ └── log └── config3 ├── data └── log 启动配置服务器1234567mongod --configsvr --replSet mytest --port 28010 --dbpath sharding/config_server/config1/data --logpath sharding/config_server/config1/log/log.log --logappend --forkmongod --configsvr --replSet mytest --port 28011 --dbpath sharding/config_server/config2/data --logpath sharding/config_server/config2/log/log.log --logappend --forkmongod --configsvr --replSet mytest --port 28012 --dbpath sharding/config_server/config3/data --logpath sharding/config_server/config3/log/log.log --logappend --fork➜ mongodb mongo localhost:28010&gt; myconfig = &#123;_id:'mytest', configsvr:true, members:[&#123;_id:0,host:'localhost:28010'&#125;,&#123;_id:1,host:'localhost:28011'&#125;,&#123;_id:2,host:'localhost:28012'&#125;]&#125;;&gt; rs.initiate(myconfig);&gt; rs.status(); 启动查询路由器12mongos --configdb mytest/localhost:28010,localhost:28011,localhost:28012 --port 29010mongos --configdb mytest/localhost:28010,localhost:28011,localhost:28012 --port 29011 分片副本集1234567891011121314── shard1│ ├── mongod1│ │ └── data│ ├── mongod2│ │ └── data│ └── mongod3│ └── data└── shard2 ├── mongod4 │ └── data ├── mongod5 │ └── data └── mongod6 └── data 启动： 12345678910111213141516mongod --replSet shard1 --port 40001 --dbpath sharding/shard1/mongod1/datamongod --replSet shard1 --port 40002 --dbpath sharding/shard1/mongod2/datamongod --replSet shard1 --port 40003 --dbpath sharding/shard1/mongod3/datamongod --replSet shard2 --port 50001 --dbpath sharding/shard2/mongod4/datamongod --replSet shard2 --port 50002 --dbpath sharding/shard2/mongod5/datamongod --replSet shard2 --port 50003 --dbpath sharding/shard2/mongod6/datamongo localhost:40001&gt; myconfig = &#123;_id:'shard1', members:[&#123;_id:0,host:'localhost:40001'&#125;,&#123;_id:1,host:'localhost:40002'&#125;,&#123;_id:2,host:'localhost:40003'&#125;]&#125;;&gt; rs.initiate(myconfig);mongo localhost:50001&gt; myconfig = &#123;_id:'shard2', members:[&#123;_id:0,host:'localhost:50001'&#125;,&#123;_id:1,host:'localhost:50002'&#125;,&#123;_id:2,host:'localhost:50003'&#125;]&#125;;&gt; rs.initiate(myconfig); 配置分片添加分片的时候，只写副本集其中一个mongod实例，它也能自动找到剩余的 123mongo localhost:29010mongos&gt; sh.addShard('shard1/localhost:40001,localhost:40002,localhost:40003');mongos&gt; sh.addShard('shard2/localhost:50001'); 下面开始设置分片，先设置可以分片的数据库，然后设置要分片的集合，需要设置索引字段，索引方式为hashed，注意要先切换到admin数据库。 123mongos&gt; use admin;mongos&gt; sh.enableSharding('mytest');mongos&gt; sh.shardCollection('mytest.student',&#123;_id:'hashed'&#125;); 接下来添加一些数据，然后可以看到分片的数据分片状态，我们也可以单独连接到某一个shard的副本集查看数据分布情况。 1234567891011121314151617181920212223242526272829303132333435363738394041mongos&gt; for(var i = 0;i &lt; 100;++i)&#123;db.student.insert(&#123;username:'hello' + i&#125;)&#125;;mongos&gt; db.printShardingStatus();--- Sharding Status --- sharding version: &#123; "_id" : 1, "minCompatibleVersion" : 5, "currentVersion" : 6, "clusterId" : ObjectId("5a60594bd9fbe1b3ebca714e") &#125; shards: &#123; "_id" : "shard1", "host" : "shard1/localhost:40001,localhost:40002,localhost:40003", "state" : 1 &#125; &#123; "_id" : "shard2", "host" : "shard2/localhost:50001,localhost:50002,localhost:50003", "state" : 1 &#125; active mongoses: "3.6.1" : 2 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: &#123; "_id" : "config", "primary" : "config", "partitioned" : true &#125; config.system.sessions shard key: &#123; "_id" : 1 &#125; unique: false balancing: true chunks: shard1 1 &#123; "_id" : &#123; "$minKey" : 1 &#125; &#125; --&gt;&gt; &#123; "_id" : &#123; "$maxKey" : 1 &#125; &#125; on : shard1 Timestamp(1, 0) &#123; "_id" : "mytest", "primary" : "shard1", "partitioned" : true &#125; mytest.student shard key: &#123; "_id" : "hashed" &#125; unique: false balancing: true chunks: shard1 2 &#123; "_id" : &#123; "$minKey" : 1 &#125; &#125; --&gt;&gt; &#123; "_id" : NumberLong(0) &#125; on : shard1 Timestamp(1, 0) &#123; "_id" : NumberLong(0) &#125; --&gt;&gt; &#123; "_id" : &#123; "$maxKey" : 1 &#125; &#125; on : shard1 Timestamp(1, 1) &#123; "_id" : "test", "primary" : "shard1", "partitioned" : false &#125;]]></content>
      <categories>
        <category>Mongo</category>
      </categories>
      <tags>
        <tag>Mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB5-副本集]]></title>
    <url>%2F2018%2F01%2F07%2Fmongo5%2F</url>
    <content type="text"><![CDATA[副本集MongoDB复制是将数据同步在多个服务器的过程。 复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。 复制还允许您从硬件故障和服务中断中恢复数据。 什么是复制? 保障数据的安全性 数据高可用性 (24*7) 灾难恢复 无需停机维护（如备份，重建索引，压缩） 分布式读取数据 MongoDB复制原理mongodb的复制至少需要两个节点。其中一个是主节点，负责处理客户端请求，其余的都是从节点，负责复制主节点上的数据。 mongodb各个节点常见的搭配方式为：一主一从、一主多从。 主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。 MongoDB复制结构图如下所示： 以上结构图中，客户端从主节点读取数据，在客户端写入数据到主节点时， 主节点与从节点进行数据交互保障数据的一致性。 副本集特征： N个节点的集群 任何节点可作为主节点 所有写入操作都在主节点上 自动故障转移 自动恢复 MongoDB副本集配置 MongoDB数据文件存储路径 MongoDB日志文件存储路径 MongoBD key文件存储路径 MongoDB实例监听端口（28010/28011/28012） 先把要存储的目录和文件先建一下，key0，key1，key2的文件内容是一样的。 12345678910111213141516171819.├── data│ ├── data0│ ├── data1│ └── data2├── key│ ├── key0│ │ └── key0│ ├── key1│ │ └── key1│ └── key2│ └── key2└── log ├── log0 │ └── log0.log ├── log1 │ └── log1.log └── log2 └── log2.log 启动3个mongod实例的副本集 123mongod --replSet myset --keyFile key/key0/key0 --port 28010 --dbpath data/data0 --logpath log/log0/log0.log --logappendmongod --replSet myset --keyFile key/key1/key1 --port 28011 --dbpath data/data1 --logpath log/log1/log1.log --logappendmongod --replSet myset --keyFile key/key2/key2 --port 28012 --dbpath data/data2 --logpath log/log2/log2.log --logappend 启动可能会有一个错误： mongodb/key/key0/key0 are too open 查了一下说是key的文件权限太大了，chmod 700 key/key0/key0改一下权限再启动就好了。 现在这3个mongod的实例还没有通信，还没有在一个副本集中，还需要用客户端连接到其中一个实例做一些配置。 1234567891011121314151617181920212223242526mongo --port 28010&gt; config_myset=&#123;_id:'bafeite', members:[&#123;_id:0, host:'localhost:28010'&#125;,&#123;_id:1, host:'localhost:28011'&#125;,&#123;_id:2, host:'localhost:28012'&#125;]&#125;;&#123; "_id" : "bafeite", "members" : [ &#123; "_id" : 0, "host" : "localhost:28010" &#125;, &#123; "_id" : 1, "host" : "localhost:28011" &#125;, &#123; "_id" : 2, "host" : "localhost:28012" &#125; ]&#125;&gt; rs.initiate(config_myset);&#123; "ok" : 0, "errmsg" : "Attempting to initiate a replica set with name bafeite, but command line reports myset; rejecting", "code" : 93, "codeName" : "InvalidReplicaSetConfig"&#125; 出错了，说准备初始化一个bafeite的副本集，但是命令行启动的时候是myset，所以拒绝。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174&gt; config_myset=&#123;_id:'myset', members:[&#123;_id:0, host:'localhost:28010'&#125;,&#123;_id:1, host:'localhost:28011'&#125;,&#123;_id:2, host:'localhost:28012'&#125;]&#125;;&#123; "_id" : "myset", "members" : [ &#123; "_id" : 0, "host" : "localhost:28010" &#125;, &#123; "_id" : 1, "host" : "localhost:28011" &#125;, &#123; "_id" : 2, "host" : "localhost:28012" &#125; ]&#125;&gt; rs.initiate(config_myset);&#123; "ok" : 1, "operationTime" : Timestamp(1515328983, 1), "$clusterTime" : &#123; "clusterTime" : Timestamp(1515328983, 1), "signature" : &#123; "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="), "keyId" : NumberLong(0) &#125; &#125;&#125;&gt; rs.isMaster();&#123; "hosts" : [ "localhost:28010", "localhost:28011", "localhost:28012" ], "setName" : "myset", "setVersion" : 1, "ismaster" : true, "secondary" : false, "primary" : "localhost:28010", "me" : "localhost:28010", "electionId" : ObjectId("7fffffff0000000000000001"), "lastWrite" : &#123; "opTime" : &#123; "ts" : Timestamp(1515329025, 1), "t" : NumberLong(1) &#125;, "lastWriteDate" : ISODate("2018-01-07T12:43:45Z"), "majorityOpTime" : &#123; "ts" : Timestamp(1515329025, 1), "t" : NumberLong(1) &#125;, "majorityWriteDate" : ISODate("2018-01-07T12:43:45Z") &#125;, "maxBsonObjectSize" : 16777216, "maxMessageSizeBytes" : 48000000, "maxWriteBatchSize" : 100000, "localTime" : ISODate("2018-01-07T12:43:49.070Z"), "logicalSessionTimeoutMinutes" : 30, "minWireVersion" : 0, "maxWireVersion" : 6, "readOnly" : false, "ok" : 1, "operationTime" : Timestamp(1515329025, 1), "$clusterTime" : &#123; "clusterTime" : Timestamp(1515329025, 1), "signature" : &#123; "hash" : BinData(0,"PnIvjYWgF8Z6qEIViYwQHnBs/M8="), "keyId" : NumberLong("6508288476205547521") &#125; &#125;&#125;&gt; rs.status();&#123; "set" : "myset", "date" : ISODate("2018-01-07T12:50:20.742Z"), "myState" : 1, "term" : NumberLong(1), "heartbeatIntervalMillis" : NumberLong(2000), "optimes" : &#123; "lastCommittedOpTime" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125;, "readConcernMajorityOpTime" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125;, "appliedOpTime" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125;, "durableOpTime" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125; &#125;, "members" : [ &#123; "_id" : 0, "name" : "localhost:28010", "health" : 1, "state" : 1, "stateStr" : "PRIMARY", "uptime" : 2022, "optime" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-01-07T12:50:15Z"), "electionTime" : Timestamp(1515328993, 1), "electionDate" : ISODate("2018-01-07T12:43:13Z"), "configVersion" : 1, "self" : true &#125;, &#123; "_id" : 1, "name" : "localhost:28011", "health" : 1, "state" : 2, "stateStr" : "SECONDARY", "uptime" : 437, "optime" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125;, "optimeDurable" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-01-07T12:50:15Z"), "optimeDurableDate" : ISODate("2018-01-07T12:50:15Z"), "lastHeartbeat" : ISODate("2018-01-07T12:50:20.450Z"), "lastHeartbeatRecv" : ISODate("2018-01-07T12:50:20.416Z"), "pingMs" : NumberLong(0), "syncingTo" : "localhost:28010", "configVersion" : 1 &#125;, &#123; "_id" : 2, "name" : "localhost:28012", "health" : 1, "state" : 2, "stateStr" : "SECONDARY", "uptime" : 437, "optime" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125;, "optimeDurable" : &#123; "ts" : Timestamp(1515329415, 1), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-01-07T12:50:15Z"), "optimeDurableDate" : ISODate("2018-01-07T12:50:15Z"), "lastHeartbeat" : ISODate("2018-01-07T12:50:20.450Z"), "lastHeartbeatRecv" : ISODate("2018-01-07T12:50:20.367Z"), "pingMs" : NumberLong(0), "syncingTo" : "localhost:28010", "configVersion" : 1 &#125; ], "ok" : 1, "operationTime" : Timestamp(1515329415, 1), "$clusterTime" : &#123; "clusterTime" : Timestamp(1515329415, 1), "signature" : &#123; "hash" : BinData(0,"ADhHuvIZC6jbKdSh8pH00VK+6ic="), "keyId" : NumberLong("6508288476205547521") &#125; &#125;&#125; members就是副本集里面的mongod实例，stateStr表示了实例是Primary或者Secondary，lastHeartbeat就是上次心跳检测的时间。 插入数据1234567&gt; db.article.save(&#123;title:'MongoDB应用开发实战', author:'zhangsan', creationDate:new Date()&#125;);WriteResult(&#123; "writeError" : &#123; "code" : 13, "errmsg" : "not authorized on mytest to execute command &#123; insert: \"article\", ordered: true, $clusterTime: &#123; clusterTime: Timestamp(1515329415, 1), signature: &#123; hash: BinData(0, 003847BAF2190BA8DB29D4A1F291F4D152BEEA27), keyId: 6508288476205547521 &#125; &#125;, $db: \"mytest\" &#125;" &#125;&#125;) 得到一个错误，查了下是因为指定了keyFile以后，mongodb默认就会加上--auth启动，为了简化操作，先把keyFile参数去掉重启一下吧。 123~ mongo --port 28010use mytest;myset:PRIMARY&gt; db.article.save(&#123;title:'MongoDB应用开发实战', author:'zhangsan', creationDate:new Date()&#125;); 现在去连接另外一个mongod实例，看看这个数据在不在。 123456789101112131415161718mongo --port 28011use mytest;show dbs;myset:SECONDARY&gt; show dbs;2018-01-07T21:41:21.587+0800 E QUERY [thread1] Error: listDatabases failed:&#123; "operationTime" : Timestamp(1515332475, 1), "ok" : 0, "errmsg" : "not master and slaveOk=false", "code" : 13435, "codeName" : "NotMasterNoSlaveOk", "$clusterTime" : &#123; "clusterTime" : Timestamp(1515332475, 1), "signature" : &#123; "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="), "keyId" : NumberLong(0) &#125; &#125;&#125; 执行不了，报错了，是因为从实例需要设置一下才能查询。 123456789myset:SECONDARY&gt; db.getMongo().setSlaveOk();myset:SECONDARY&gt; show dbs;admin 0.000GBconfig 0.000GBlocal 0.000GBmytest 0.000GBtest 0.000GBmyset:SECONDARY&gt; db.article.find();&#123; "_id" : ObjectId("5a52221989c0d47f8598d0ad"), "title" : "MongoDB应用开发实战", "author" : "zhangsan", "creationDate" : ISODate("2018-01-07T13:35:21.111Z") &#125; 添加副本集rs.add(&#39;host:port&#39;) 删除副本集rs.remove(&#39;host:port&#39;) 重新配置12345var config = rs.config();rs....rs.reconfig(config); 监控-mongostatmongodb提供了一个mongostat命令用于监控mongodb服务器。 1234567➜ ~ mongostat --helpUsage: mongostat &lt;options&gt; &lt;polling interval in seconds&gt;Monitor basic MongoDB server statistics.See http://docs.mongodb.org/manual/reference/program/mongostat/ for more information. 监控副本集的命令： 1234➜ ~ mongostat --host=myset/localhost:28010,localhost:28011,localhost:28012 host insert query update delete getmore command dirty used flushes vsize res qrw arw net_in net_out conn set repl timelocalhost:28010 *0 *0 *0 *0 0 3|0 0.0% 0.0% 0 4.94G 39.0M 0|0 1|0 437b 53.4k 3 myset SEC Jan 16 12:18:50.342localhost:28011 *0 *0 *0 *0 0 3|0 0.0% 0.0% 0 4.97G 39.0M 0|0 1|0 432b 52.8k 5 myset PRI Jan 16 12:18:50.342 关于副本集的原理分析： 副本集有且只有一台机器是primary，primary与secondary之间的数据复制是异步进行的，并且通过oplog进行。 副本集中的机器数最好为奇数。 Primary机器接收所有的写操作（无法改变的），我们可以配置read preference，使得读操作可以发生在secondary机器上。如果读操作发生在primary机器上，那么机器就是强一致性的； 副本集中最多有50台机器，之前的版本最多有12台机器。如果超过了50台，那么只能使用Master-Slave方式。不过如果使用Master-Slave方式，那么就失去了自动化的failover机制。 Arbiter机器（仲裁机器），它本身并不存放数据库数据，仅提供选举功能。 不要将arbiter机器放在primary或是secondary机器上。 对于secondary机器，可以进行如下配置： 禁止某台secondary机器成为primary机器，priority为0。 禁止客户端读取某台secondary机器的数据，隐藏成员。 仅记录历史快照的secondary，延迟成员。比如说延迟一小时 最常见的secondary机器依然是进行数据异步复制与保持系统高可用的形式。 副本集中最多有50台机器，其中具有投票功能的机器数量最多是7台。 Priority为0的机器：无法成为primary。不能触发一个选举 关于MongoDB的读写分离： 如果进行读写分离，那么要注意到读会有延迟 secondary机器要通过oplog异步复制primary机器的数据，因此从整体来看，primary与secondary机器的读速度是大体相当的。 默认情况下，MongoDB的读写都是在primary上进行的。 关于Read preference： Primary（默认值） primaryPreferred secondary secondaryPreferred nearest（网络延迟最少的） 关于延迟成员（delayed member） 延迟成员的priority必须为0，表示它无法成为primary。 延迟成员也是隐藏成员，应用是无法通过延迟成员查询数据的。 可以对primary的选举进行投票。 请确保副本集中成员的个数为奇数，如果是偶数的话，请添加一个arbiter成员。 关于MongoDB的投票与故障恢复： MongoDB投票要求投票时系统中可用的机器数量要是全体副本集成员个数的大多数。 如果副本集中有3台机器，那么有几台机器宕掉还可以确保MongoDB副本集可以正常使用？2台。 如果副本集中有4台机器，那么有几台机器宕掉还可以确保MongoDB副本集可以正常使用？3台。 如果副本集中有5台机器，那么有几台机器宕掉还可以确保MongoDB副本集可以正常使用？3台。 如果副本集中有6台机器，那么有几台机器宕掉还可以确保MongoDB副本集可以正常使用？4台。 如果副本集中有7台机器，那么有几台机器宕掉还可以确保MongoDB副本集可以正常使用？4台。 向副本集中添加成员并不总是会确保系统的故障恢复能力，不过这样做可以实现一些额外的功：备份、统计报表等。]]></content>
      <categories>
        <category>Mongo</category>
      </categories>
      <tags>
        <tag>Mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB4-GridFS、Capped Collections、索引与查询计划]]></title>
    <url>%2F2018%2F01%2F07%2Fmongo4%2F</url>
    <content type="text"><![CDATA[GridFShttps://docs.mongodb.com/manual/core/gridfs/index.html#use-gridfs GridFS 用于存储和恢复那些超过16M（BSON文件限制）的文件(如：图片、音频、视频等)。 GridFS 也是文件存储的一种方式，但是它是存储在MonoDB的集合中。 GridFS 可以更好的存储大于16M的文件。 GridFS 会将大文件对象分割成多个小的chunk(文件片段),一般为256k/个,每个chunk将作为MongoDB的一个文档(document)被存储在chunks集合中。 GridFS 用两个集合来存储一个文件：fs.files与fs.chunks。 每个文件的实际内容被存在chunks(二进制数据)中,和文件有关的meta数据(filename,content_type,还有用户自定义的属性)将会被存在files集合中。以下是简单的 fs.files 集合文档： 1234567&#123; "filename": "test.txt", "chunkSize": NumberInt(261120), "uploadDate": ISODate("2014-04-13T11:32:33.557Z"), "md5": "7b762939321e146569b07f72c62cca4f", "length": NumberInt(646)&#125; 以下是简单的 fs.chunks 集合文档： 12345&#123; "files_id": ObjectId("534a75d19f54bfec8a2fe44b"), "n": NumberInt(0), "data": "Mongo Binary Data"&#125; mongofilesmongodb提供了一个mongofiles命令来管理GridFS。 mongofiles &lt;options&gt; &lt;commands&gt; &lt;filename&gt; 它的参数在文档都有详细的解释 https://docs.mongodb.com/manual/reference/program/mongofiles/index.html 上传上传一个东西测试一下，-d指定数据库，-l指定要上传的文件地址 123~ mongofiles -d mytest -l /Users/xiaomai/Desktop/WXWork_2.4.5.213.dmg put wxwork.dmg2018-01-07T11:59:51.831+0800 connected to: localhostadded file: wxwork.dmg 可以查一下fs.files和fs.chunks这两个数据库 12&gt; db.fs.files.find();&#123; "_id" : ObjectId("5a519b37e59cf07feeb8eca8"), "chunkSize" : 261120, "uploadDate" : ISODate("2018-01-07T03:59:52.285Z"), "length" : 22230276, "md5" : "b0c833f7325342227ddff80e03462000", "filename" : "wxwork.dmg" &#125; 由于chunks里保存的二进制数据，特别多，所以查询的时候把data字段给忽略一下 123456789&gt; db.fs.chunks.find(&#123;&#125;,&#123;data:0&#125;);&#123; "_id" : ObjectId("5a519b37e59cf07feeb8eca9"), "files_id" : ObjectId("5a519b37e59cf07feeb8eca8"), "n" : 0 &#125;&#123; "_id" : ObjectId("5a519b37e59cf07feeb8ecaa"), "files_id" : ObjectId("5a519b37e59cf07feeb8eca8"), "n" : 1 &#125;&#123; "_id" : ObjectId("5a519b37e59cf07feeb8ecab"), "files_id" : ObjectId("5a519b37e59cf07feeb8eca8"), "n" : 2 &#125;&#123; "_id" : ObjectId("5a519b37e59cf07feeb8ecac"), "files_id" : ObjectId("5a519b37e59cf07feeb8eca8"), "n" : 3 &#125;&#123; "_id" : ObjectId("5a519b37e59cf07feeb8ecad"), "files_id" : ObjectId("5a519b37e59cf07feeb8eca8"), "n" : 4 &#125;&#123; "_id" : ObjectId("5a519b37e59cf07feeb8ecae"), "files_id" : ObjectId("5a519b37e59cf07feeb8eca8"), "n" : 5 &#125;&#123; "_id" : ObjectId("5a519b37e59cf07feeb8ecaf"), "files_id" : ObjectId("5a519b37e59cf07feeb8eca8"), "n" : 6 &#125;Type "it" for more 当然，这个数据也不少，chunkSize是261120，除以1024，一块是255k。 搜索123➜ ~ mongofiles -d mytest search w2018-01-07T12:44:41.433+0800 connected to: localhostwxwork.dmg 22230276 删除123➜ ~ mongofiles -d mytest delete wxwork.dmg 2018-01-07T12:45:46.190+0800 connected to: localhostsuccessfully deleted all instances of 'wxwork.dmg' from GridFS Capped Collectionshttps://docs.mongodb.com/manual/core/capped-collections/index.html Capped collections are fixed-size collections that support high-throughput operations that insert and retrieve documents based on insertion order. Capped collections work in a way similar to circular buffers: once a collection fills its allocated space, it makes room for new documents by overwriting the oldest documents in the collection. MongoDB 固定集合（Capped Collections）是性能出色且有着固定大小的集合，对于大小固定，我们可以想象其就像一个环形队列，当集合空间用完后，再插入的元素就会覆盖最初始的头部的元素！ 一般只新增不更新，更新要求更新后的集合大小不能超过之前的大小，否则会出错。 2.2版本之后，_id就自带索引了，在2.2之前，需要显式的在_id字段上创建索引。 创建https://docs.mongodb.com/manual/core/capped-collections/index.html#create-a-capped-collection 12&gt; db.createCollection("log",&#123;capped:true,size:1000&#125;)&#123; "ok" : 1 &#125; mongodb会先判断size的大小是否超过限制，然后再判断集合里文档的数量是否超过了max设定的值。 1234567891011121314&gt; db.createCollection("log", &#123; capped : true, size : 100, max : 2 &#125; )&gt; db.log.insert(&#123;count:1&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.log.insert(&#123;count:2&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.log.insert(&#123;count:3&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.log.insert(&#123;count:4&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.log.insert(&#123;count:5&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.log.find(&#123;&#125;);&#123; "_id" : ObjectId("5a51aec1fef2eb6e769d4f36"), "count" : 4 &#125;&#123; "_id" : ObjectId("5a51aec5fef2eb6e769d4f37"), "count" : 5 &#125; 这也印证了之前的说法，在超过2条以后，新插入的会覆盖最早的记录。 查询https://docs.mongodb.com/manual/core/capped-collections/index.html#query-a-capped-collection 如果没有指定排序，mongodb会保证查询的结果就是插入的顺序。 123&gt; db.log.find().sort(&#123;$natural: -1&#125;)&#123; "_id" : ObjectId("5a51aec5fef2eb6e769d4f37"), "count" : 5 &#125;&#123; "_id" : ObjectId("5a51aec1fef2eb6e769d4f36"), "count" : 4 &#125; 判断一个集合是否是capped： 12&gt; db.log.isCapped();true 转换一个普通的集合成固定集合： 1234&gt; db.runCommand(&#123;'convertToCapped': 'hello', size:1000&#125;);&#123; "ok" : 1 &#125;&gt; db.hello.isCapped()true 索引与查询计划索引通常能够极大的提高查询的效率，如果没有索引，MongoDB在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。 这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。 索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构。 mongodb默认会给每一个集合的_id字段上创建一个索引。 创建索引数据准备： 1234567&gt; db.myindex.insert(&#123;name:'zhangsan', age:10&#125;);&gt; db.myindex.insert(&#123;name:'lisi', age:20&#125;);&gt; db.myindex.insert(&#123;name:'wangwu', age:14&#125;);&gt; db.myindex.find();&#123; "_id" : ObjectId("5a51c3d1a893e40faf1670cb"), "name" : "zhangsan", "age" : 10 &#125;&#123; "_id" : ObjectId("5a51c3e5a893e40faf1670cc"), "name" : "lisi", "age" : 20 &#125;&#123; "_id" : ObjectId("5a51c404a893e40faf1670cd"), "name" : "wangwu", "age" : 14 &#125; 查询一下现有索引： 1234567891011&gt; db.myindex.getIndexes();[ &#123; "v" : 2, "key" : &#123; "_id" : 1 &#125;, "name" : "_id_", "ns" : "test.myindex" &#125;] 给age和name创建一个索引，1和-1表示升序还是降序： 1234567891011121314151617181920212223242526272829303132333435363738394041&gt; db.myindex.createIndex(&#123;age:1&#125;);&#123; "createdCollectionAutomatically" : false, "numIndexesBefore" : 1, "numIndexesAfter" : 2, "ok" : 1&#125;&gt; db.myindex.createIndex(&#123;name:1&#125;);&#123; "createdCollectionAutomatically" : false, "numIndexesBefore" : 2, "numIndexesAfter" : 3, "ok" : 1&#125;&gt; db.myindex.getIndexes();[ &#123; "v" : 2, "key" : &#123; "_id" : 1 &#125;, "name" : "_id_", "ns" : "test.myindex" &#125;, &#123; "v" : 2, "key" : &#123; "age" : 1 &#125;, "name" : "age_1", "ns" : "test.myindex" &#125;, &#123; "v" : 2, "key" : &#123; "name" : 1 &#125;, "name" : "name_1", "ns" : "test.myindex" &#125;] 给索引起个别名，并将创建索引的操作后台运行，意思不会阻塞当前的线程。 123456789101112131415161718192021222324252627&gt; db.myindex.createIndex(&#123;name:1&#125;, &#123;name:'helloIndex', background:1&#125;)&#123; "createdCollectionAutomatically" : false, "numIndexesBefore" : 1, "numIndexesAfter" : 2, "ok" : 1&#125;&gt; db.myindex.getIndexes();[ &#123; "v" : 2, "key" : &#123; "_id" : 1 &#125;, "name" : "_id_", "ns" : "test.myindex" &#125;, &#123; "v" : 2, "key" : &#123; "name" : 1 &#125;, "name" : "helloIndex", "ns" : "test.myindex", "background" : 1 &#125;] 删除索引删除单个： 1&gt; db.myindex.dropIndex(&#123;name:1&#125;) 删除所有： 1234567891011121314151617&gt; db.myindex.dropIndexes();&#123; "nIndexesWas" : 2, "msg" : "non-_id indexes dropped for collection", "ok" : 1&#125;&gt; db.myindex.getIndexes();[ &#123; "v" : 2, "key" : &#123; "_id" : 1 &#125;, "name" : "_id_", "ns" : "test.myindex" &#125;] 唯一索引指定unique参数创建一个唯一索引，插入字段有相同的值，会执行失败。 12345678910111213141516171819202122232425262728293031323334353637&gt; db.myindex.dropIndex(&#123;name:1&#125;);&#123; "nIndexesWas" : 2, "ok" : 1 &#125;&gt; db.myindex.createIndex(&#123;name:1&#125;, &#123;unique:1&#125;)&#123; "createdCollectionAutomatically" : false, "numIndexesBefore" : 1, "numIndexesAfter" : 2, "ok" : 1&#125;&gt; db.myindex.getIndexes();[ &#123; "v" : 2, "key" : &#123; "_id" : 1 &#125;, "name" : "_id_", "ns" : "test.myindex" &#125;, &#123; "v" : 2, "unique" : true, "key" : &#123; "name" : 1 &#125;, "name" : "name_1", "ns" : "test.myindex" &#125;]&gt; db.myindex.insert(&#123;name:'zhangsan',age:2&#125;);WriteResult(&#123; "nInserted" : 0, "writeError" : &#123; "code" : 11000, "errmsg" : "E11000 duplicate key error collection: test.myindex index: name_1 dup key: &#123; : \"zhangsan\" &#125;" &#125;&#125;) 查询计划虽然设置了索引，我们在做查询的时候，可能并不是很确定到底用到了索引没有。跟传统sql一样，mongo提供了查询计划来检测。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&gt; db.myindex.find(&#123;name:'zhangsan'&#125;).explain();&#123; "queryPlanner" : &#123; "plannerVersion" : 1, "namespace" : "test.myindex", "indexFilterSet" : false, "parsedQuery" : &#123; "name" : &#123; "$eq" : "zhangsan" &#125; &#125;, "winningPlan" : &#123; "stage" : "FETCH", "inputStage" : &#123; "stage" : "IXSCAN", "keyPattern" : &#123; "name" : 1 &#125;, "indexName" : "name_1", "isMultiKey" : false, "multiKeyPaths" : &#123; "name" : [ ] &#125;, "isUnique" : true, "isSparse" : false, "isPartial" : false, "indexVersion" : 2, "direction" : "forward", "indexBounds" : &#123; "name" : [ "[\"zhangsan\", \"zhangsan\"]" ] &#125; &#125; &#125;, "rejectedPlans" : [ ] &#125;, "serverInfo" : &#123; "host" : "2dd4d04418b1", "port" : 27017, "version" : "3.6.0", "gitVersion" : "a57d8e71e6998a2d0afde7edc11bd23e5661c915" &#125;, "ok" : 1&#125; 主要是看indexBounds，可以确定是用了name的索引。 因为之前把age上的索引删除了，所以查询age&gt;5的数据，则没有用到索引。 123456789101112131415161718192021222324252627282930&gt; db.myindex.find(&#123;age:&#123;$gt:5&#125;&#125;).explain();&#123; "queryPlanner" : &#123; "plannerVersion" : 1, "namespace" : "test.myindex", "indexFilterSet" : false, "parsedQuery" : &#123; "age" : &#123; "$gt" : 5 &#125; &#125;, "winningPlan" : &#123; "stage" : "COLLSCAN", "filter" : &#123; "age" : &#123; "$gt" : 5 &#125; &#125;, "direction" : "forward" &#125;, "rejectedPlans" : [ ] &#125;, "serverInfo" : &#123; "host" : "2dd4d04418b1", "port" : 27017, "version" : "3.6.0", "gitVersion" : "a57d8e71e6998a2d0afde7edc11bd23e5661c915" &#125;, "ok" : 1&#125; 重建索引12345678910111213141516171819202122232425&gt; db.myindex.reIndex();&#123; "nIndexesWas" : 2, "nIndexes" : 2, "indexes" : [ &#123; "v" : 2, "key" : &#123; "_id" : 1 &#125;, "name" : "_id_", "ns" : "test.myindex" &#125;, &#123; "v" : 2, "unique" : true, "key" : &#123; "name" : 1 &#125;, "name" : "name_1", "ns" : "test.myindex" &#125; ], "ok" : 1&#125; 组合索引https://docs.mongodb.com/manual/indexes/index.html#compound-index 123456789101112131415161718192021222324252627282930313233343536&gt; db.myindex.createIndex(&#123;age:1, name:1&#125;, &#123;name:'myNameAndAge'&#125;)&#123; "createdCollectionAutomatically" : false, "numIndexesBefore" : 2, "numIndexesAfter" : 3, "ok" : 1&#125;&gt; db.myindex.getIndexes();[ &#123; "v" : 2, "key" : &#123; "_id" : 1 &#125;, "name" : "_id_", "ns" : "test.myindex" &#125;, &#123; "v" : 2, "unique" : true, "key" : &#123; "name" : 1 &#125;, "name" : "name_1", "ns" : "test.myindex" &#125;, &#123; "v" : 2, "key" : &#123; "age" : 1, "name" : 1 &#125;, "name" : "myNameAndAge", "ns" : "test.myindex" &#125;] 查询优化器mysql里面有一个慢查询日志，记录了查询比较慢的一些查询，可以让开发人员分析。 mongodb也提供了profiling，它默认是没有开启的，需要手动开启，0表示没有开启。 12&gt; db.getProfilingLevel();0 这个值为1的时候，大于100毫秒的的查询会被记录，如果为2，所有的查询记录都会被记录。 数据准备: 123&gt; db.post.save(&#123;name:'zhangsan', age:20&#125;)&gt; db.post.save(&#123;name:'lisi', age:21&#125;)&gt; db.post.save(&#123;name:'wangwu', age:22&#125;) 改变profilingLevel，开启profiling，为了演示，设置成2，记录所有的查询。 123&gt; db.setProfilingLevel(2);&#123; "was" : 0, "slowms" : 100, "sampleRate" : 1, "ok" : 1 &#125;&gt; 可以看到多了一个集合，实际上查询记录就会被记录到system.profile集合当中。 123&gt; show collections;postsystem.profile 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&gt; db.post.find();&#123; "_id" : ObjectId("5a51cef5a893e40faf1670cf"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a51cf00a893e40faf1670d0"), "name" : "lisi", "age" : 21 &#125;&#123; "_id" : ObjectId("5a51cf0aa893e40faf1670d1"), "name" : "wangwu", "age" : 22 &#125;&gt; db.system.profile.find().pretty();&#123; "op" : "query", "ns" : "mytest.post", "command" : &#123; "find" : "post", "filter" : &#123; &#125;, "$db" : "mytest" &#125;, "keysExamined" : 0, "docsExamined" : 3, "cursorExhausted" : true, "numYield" : 0, "locks" : &#123; "Global" : &#123; "acquireCount" : &#123; "r" : NumberLong(2) &#125; &#125;, "Database" : &#123; "acquireCount" : &#123; "r" : NumberLong(1) &#125; &#125;, "Collection" : &#123; "acquireCount" : &#123; "r" : NumberLong(1) &#125; &#125; &#125;, "nreturned" : 3, "responseLength" : 249, "protocol" : "op_msg", "millis" : 0, "planSummary" : "COLLSCAN", "execStats" : &#123; "stage" : "COLLSCAN", "nReturned" : 3, "executionTimeMillisEstimate" : 0, "works" : 5, "advanced" : 3, "needTime" : 1, "needYield" : 0, "saveState" : 0, "restoreState" : 0, "isEOF" : 1, "invalidates" : 0, "direction" : "forward", "docsExamined" : 3 &#125;, "ts" : ISODate("2018-01-07T07:58:31.109Z"), "client" : "172.17.0.1", "appName" : "MongoDB Shell", "allUsers" : [ ], "user" : ""&#125; profile的内容有点多，docsExamined扫描过3个文档，nreturned表示返回了3个文档。接着给name创建一个索引，再查询一次。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596db.post.createIndex(&#123;name:1&#125;)db.post.find(&#123;name:'zhangsan'&#125;);db.system.profile.find().pretty()&#123; "op" : "query", "ns" : "mytest.post", "command" : &#123; "find" : "post", "filter" : &#123; "name" : "zhangsan" &#125;, "$db" : "mytest" &#125;, "keysExamined" : 1, "docsExamined" : 1, "cursorExhausted" : true, "numYield" : 0, "locks" : &#123; "Global" : &#123; "acquireCount" : &#123; "r" : NumberLong(2) &#125; &#125;, "Database" : &#123; "acquireCount" : &#123; "r" : NumberLong(1) &#125; &#125;, "Collection" : &#123; "acquireCount" : &#123; "r" : NumberLong(1) &#125; &#125; &#125;, "nreturned" : 1, "responseLength" : 141, "protocol" : "op_msg", "millis" : 4, "planSummary" : "IXSCAN &#123; name: 1 &#125;", "execStats" : &#123; "stage" : "FETCH", "nReturned" : 1, "executionTimeMillisEstimate" : 0, "works" : 2, "advanced" : 1, "needTime" : 0, "needYield" : 0, "saveState" : 0, "restoreState" : 0, "isEOF" : 1, "invalidates" : 0, "docsExamined" : 1, "alreadyHasObj" : 0, "inputStage" : &#123; "stage" : "IXSCAN", "nReturned" : 1, "executionTimeMillisEstimate" : 0, "works" : 2, "advanced" : 1, "needTime" : 0, "needYield" : 0, "saveState" : 0, "restoreState" : 0, "isEOF" : 1, "invalidates" : 0, "keyPattern" : &#123; "name" : 1 &#125;, "indexName" : "name_1", "isMultiKey" : false, "multiKeyPaths" : &#123; "name" : [ ] &#125;, "isUnique" : false, "isSparse" : false, "isPartial" : false, "indexVersion" : 2, "direction" : "forward", "indexBounds" : &#123; "name" : [ "[\"zhangsan\", \"zhangsan\"]" ] &#125;, "keysExamined" : 1, "seeks" : 1, "dupsTested" : 0, "dupsDropped" : 0, "seenInvalidated" : 0 &#125; &#125;, "ts" : ISODate("2018-01-07T08:04:05.695Z"), "client" : "172.17.0.1", "appName" : "MongoDB Shell", "allUsers" : [ ], "user" : ""&#125; 因为post的name字段是加了索引的，所以这次的查询记录，indexName显示用到的索引名称，keysExamined为1，表示扫描了1条索引，这次docsExamined为1，只扫描了一条文档。 索引和查询计划就说到这里，下面的文章，讲解最后一部分，也就是副本集。]]></content>
      <categories>
        <category>Mongo</category>
      </categories>
      <tags>
        <tag>Mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB3-三种聚合操作详解]]></title>
    <url>%2F2018%2F01%2F01%2Fmongo3%2F</url>
    <content type="text"><![CDATA[聚合聚合通常在mysql中是group by，例如统计sum等操作，MongoDB也为我们提供了聚合操作，但是实现却不一样。 group()这次需要的数据会比较多，这次直接用js来准数据。 1234for(var i = 1; i &lt; 30; ++i ) &#123; var count = i % 5; db.mygroup.insert(&#123;name: 'name' + i, count : count&#125;);&#125; mongo3.4开始，这个方法已经被官方弃用了，用db.collection.aggregate()替代了，不过这里的例子仍然以db.collection.group()进行讲解。 实例-计数我们需要按照count字段进行分组然后统计每个分组的数量 12345678910111213141516171819202122232425&gt; db.mygroup.group(&#123;key: &#123;count:true&#125;, initial: &#123;totalCount:0&#125;, reduce: function(current, aggregator)&#123;... aggregator.totalCount++;... &#125;&#125;);[ &#123; "count" : 1, "totalCount" : 6 &#125;, &#123; "count" : 2, "totalCount" : 6 &#125;, &#123; "count" : 3, "totalCount" : 6 &#125;, &#123; "count" : 4, "totalCount" : 6 &#125;, &#123; "count" : 0, "totalCount" : 5 &#125;] 参数解释： key： 需要分组的字段 initial：每一组初始化的值 reduce：计算 keyf：二次计算然后产生的key值 cond：查询条件 finalize：完成器，在返回之前对结果进行计算 reduce的两个参数，就是分组后对数据的处理，current表示每次循环的当前对象，aggregator则每一组拥有一个共享对象，所以在这个例子里aggregator的totalCount每一组开始循环的时候都是0，最终统计出来每一组的总数。 关于这些的详细介绍，官网文档也给了非常好的介绍：https://docs.mongodb.com/manual/reference/method/db.collection.group/index.html 实例-最大值重新准备数据： 12345db.mygroup.drop()for(var i = 1; i &lt; 30; ++i ) &#123; var count = i % 5; db.mygroup.insert(&#123;name: 'name' + i, age:i, count : count&#125;);&#125; 以count作为分组，取出每一组里age最大值。 123456789101112131415161718192021222324252627&gt; db.mygroup.group(&#123;key: &#123;count:true&#125;, initial: &#123;maxAge:-1&#125;, reduce: function(current, aggregator)&#123;... if(current.age &gt; aggregator.maxAge) &#123;... aggregator.maxAge = current.age... &#125;... &#125;&#125;);[ &#123; "count" : 1, "maxAge" : 26 &#125;, &#123; "count" : 2, "maxAge" : 27 &#125;, &#123; "count" : 3, "maxAge" : 28 &#125;, &#123; "count" : 4, "maxAge" : 29 &#125;, &#123; "count" : 0, "maxAge" : 25 &#125;] 实例-找最小值找出年龄最小值，这里优化一下效率 12345678910111213141516171819202122232425262728293031&gt; db.mygroup.group(&#123;key: &#123;count:true&#125;, initial: &#123;minAge:0, count:0&#125;, reduce: function(current, aggregator)&#123;... if(aggregator.count == 0) &#123;... aggregator.minAge = current.age;... aggregator.count++;... &#125;else if(current.age &lt; aggregator.minAge) &#123;... aggregator.minAge = current.age... aggregator.count++;... &#125;... &#125;&#125;);[ &#123; "count" : 1, "minAge" : 1 &#125;, &#123; "count" : 1, "minAge" : 2 &#125;, &#123; "count" : 1, "minAge" : 3 &#125;, &#123; "count" : 1, "minAge" : 4 &#125;, &#123; "count" : 1, "minAge" : 5 &#125;] 实例4-平均值这个例子需要使用finalize完成器来做，先用reduce来完成计数和求和的操作，然后用finalize完成器来求平均值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&gt; db.mygroup.group(&#123;key: &#123;count:true&#125;, initial: &#123;minAge:0, count:0, totalAge:0,totalCount:0&#125;, reduce: function(current, aggregator)&#123;... if(aggregator.count == 0) &#123;... aggregator.minAge = current.age;... aggregator.count++... &#125;else if(current.age &lt; aggregator.minAge) &#123;... aggregator.minAge = current.age... aggregator.count++... &#125;... aggregator.totalCount++;... aggregator.totalAge += current.age;... &#125;, finalize: function(aggregator) &#123;... aggregator.avgAge = aggregator.totalAge / aggregator.totalCount;... &#125;&#125;);[ &#123; "count" : 1, "minAge" : 1, "totalAge" : 81, "totalCount" : 6, "avgAge" : 13.5 &#125;, &#123; "count" : 1, "minAge" : 2, "totalAge" : 87, "totalCount" : 6, "avgAge" : 14.5 &#125;, &#123; "count" : 1, "minAge" : 3, "totalAge" : 93, "totalCount" : 6, "avgAge" : 15.5 &#125;, &#123; "count" : 1, "minAge" : 4, "totalAge" : 99, "totalCount" : 6, "avgAge" : 16.5 &#125;, &#123; "count" : 1, "minAge" : 5, "totalAge" : 75, "totalCount" : 5, "avgAge" : 15 &#125;] mapReducemapReduce主要分为两个阶段，mapReduce在mongodb中可以在分片的环境中运行，而group则不行。 map阶段 处理数据，例如分组，转换等 reduce阶段 根据map的输出计算数据，得到想要的结果 https://docs.mongodb.com/manual/reference/command/mapReduce/index.html 1234567891011121314151617db.runCommand( &#123; mapReduce: &lt;collection&gt;, map: &lt;function&gt;, reduce: &lt;function&gt;, finalize: &lt;function&gt;, out: &lt;output&gt;, query: &lt;document&gt;, sort: &lt;document&gt;, limit: &lt;number&gt;, scope: &lt;document&gt;, jsMode: &lt;boolean&gt;, verbose: &lt;boolean&gt;, bypassDocumentValidation: &lt;boolean&gt;, collation: &lt;document&gt; &#125; ) For those keys that have multiple values, MongoDB applies the reduce phase, which collects and condenses the aggregated data. map的结果只有一个值的话，是不会运行reduce的。 实例1-求长度数据准备： 1234for(var i = 1; i &lt; 10; ++i ) &#123; var count = i % 3; db.student.insert(&#123;name: 'name' + i, age:i, count : count&#125;);&#125; 现在用MapReduce来统计每一组数据长度。 map必须调用emit函数。 123456789101112&gt; db.getCollection('student').mapReduce(function()&#123;emit(this.count, 1)&#125;, function(key, values)&#123;return values.length;&#125;, &#123;out:"length"&#125;)&#123; "result" : "length", "timeMillis" : 58, "counts" : &#123; "input" : 9, "emit" : 9, "reduce" : 3, "output" : 3 &#125;, "ok" : 1&#125; 注意out指定的是一个collection，MapReduce会把结果生产到当前db的out属性指定的集合里。 1234&gt; db.length.find();&#123; "_id" : 0, "value" : 3 &#125;&#123; "_id" : 1, "value" : 3 &#125;&#123; "_id" : 2, "value" : 3 &#125; 实例2-求和12345678910111213141516&gt; db.getCollection('student').mapReduce(function()&#123;emit(this.count, this.age)&#125;, function(key, values)&#123;var totalAge = 0; for (i = 0; i &lt; values.length; i++)&#123;totalAge += values[i]&#125; return totalAge;&#125;, &#123;out:"totalAge"&#125;);&#123; "result" : "totalAge", "timeMillis" : 53, "counts" : &#123; "input" : 9, "emit" : 9, "reduce" : 3, "output" : 3 &#125;, "ok" : 1&#125;&gt; db.totalAge.find();&#123; "_id" : 0, "value" : 18 &#125;&#123; "_id" : 1, "value" : 12 &#125;&#123; "_id" : 2, "value" : 15 &#125; 实例3-求最大值1234567891011121314151617var maxFun = function(key, values)&#123; var maxAge = 0; values.forEach(function(current) &#123; if(current&gt;maxAge)&#123; maxAge = current; &#125; &#125;); return maxAge; &#125;;db.getCollection('student').mapReduce(function()&#123;emit(this.count, this.age)&#125;, maxFun, &#123;out:"maxAge"&#125;);&gt; db.getCollection('maxAge').find(&#123;&#125;)&#123; "_id" : 0, "value" : 9 &#125;&#123; "_id" : 1, "value" : 7 &#125;&#123; "_id" : 2, "value" : 8 &#125;&gt; 实例4-求平均数求平均数也很简单，就是把和除以长度。 123456&gt; var avgFun = function(key, values)&#123;var totalAge = 0; for (i = 0; i &lt; values.length; i++)&#123;totalAge += values[i]&#125; return totalAge / values.length;&#125;;&gt; db.getCollection('student').mapReduce(function()&#123;emit(this.count, this.age)&#125;, avgFun, &#123;out:"avgAge"&#125;);&gt; db.avgAge.find();&#123; "_id" : 0, "value" : 6 &#125;&#123; "_id" : 1, "value" : 4 &#125;&#123; "_id" : 2, "value" : 5 &#125; 实例5-标签统计数据准备： 12345db.getCollection('article').insert(&#123;name:"article1", tags:['java','python','mongodb','ruby']&#125;);db.getCollection('article').insert(&#123;name:"article2", tags:['perl','scala','mongodb','ruby']&#125;);db.getCollection('article').insert(&#123;name:"article3", tags:['perl','kotlin','mongodb','ruby']&#125;);db.getCollection('article').insert(&#123;name:"article4", tags:['perl','kotlin','mongodb','groovy']&#125;);db.getCollection('article').insert(&#123;name:"article5", tags:['perl','kotlin','redis','groovy']&#125;); 每个文章都有标签，现在要统计每一个标签出现的次数。 有思路吗？这个用mongodb来做实在是太方便了，如果是传统的sql，可能还要好好想想，我们在map的时候，就用每一个tag分组，得到我们要的数据。 1234567891011121314151617db.article.mapReduce(function()&#123; this.tags.forEach(function(currrnt)&#123; emit(currrnt, 1) &#125;);&#125;, function(key, values)&#123; return values.length; &#125;, &#123;out:"aa"&#125;);&gt; db.aa.find();&#123; "_id" : "groovy", "value" : 2 &#125;&#123; "_id" : "java", "value" : 1 &#125;&#123; "_id" : "kotlin", "value" : 3 &#125;&#123; "_id" : "mongodb", "value" : 4 &#125;&#123; "_id" : "perl", "value" : 4 &#125;&#123; "_id" : "python", "value" : 1 &#125;&#123; "_id" : "redis", "value" : 1 &#125;&#123; "_id" : "ruby", "value" : 3 &#125;&#123; "_id" : "scala", "value" : 1 &#125; finalize使用123456789101112131415161718192021222324252627282930313233343536373839404142434445db.article.mapReduce(function()&#123; this.tags.forEach(function(currrnt)&#123; emit(currrnt, 1) &#125;);&#125;, function(key, values)&#123; return values.length; &#125;, &#123;out:"aa"&#125;);&gt; db.getCollection('aa').find(&#123;&#125;)&#123; "_id" : "groovy", "value" : 2 &#125;&#123; "_id" : "java", "value" : &#123; "count" : 1 &#125; &#125;&#123; "_id" : "kotlin", "value" : 3 &#125;&#123; "_id" : "mongodb", "value" : 4 &#125;&#123; "_id" : "perl", "value" : 4 &#125;&#123; "_id" : "python", "value" : &#123; "count" : 1 &#125; &#125;&#123; "_id" : "redis", "value" : &#123; "count" : 1 &#125; &#125;&#123; "_id" : "ruby", "value" : 3 &#125;&#123; "_id" : "scala", "value" : &#123; "count" : 1 &#125; &#125;``` 为什么会出现这样的结果？有的是对象，有的是值，之前说过，如果map产出的values的结果只有一个，是不会执行reduce阶段的，这里可以用finalize来保证结果的统一。```jsdb.article.mapReduce(function()&#123; this.tags.forEach(function(currrnt)&#123; emit(currrnt, &#123;count:1&#125;) &#125;);&#125;, function(key, values)&#123; return values.length; &#125;, &#123;out:"aa",finalize: function(key,reduced)&#123; if(reduced.count) &#123; return reduced.count; &#125; return reduced; &#125;&#125;);&gt; db.getCollection('aa').find(&#123;&#125;)&#123; "_id" : "groovy", "value" : 2 &#125;&#123; "_id" : "java", "value" : 1 &#125;&#123; "_id" : "kotlin", "value" : 3 &#125;&#123; "_id" : "mongodb", "value" : 4 &#125;&#123; "_id" : "perl", "value" : 4 &#125;&#123; "_id" : "python", "value" : 1 &#125;&#123; "_id" : "redis", "value" : 1 &#125;&#123; "_id" : "ruby", "value" : 3 &#125;&#123; "_id" : "scala", "value" : 1 &#125; 进阶，mapReduce过滤现在我们只统计发布了java标签的数据，可以用query参数来过滤，也可以用map阶段来用代码过滤。 12345678910111213141516171819202122232425db.article.mapReduce(function()&#123; var flag = false; for(i in this.tags)&#123; if(this.tags[i] == 'java') &#123; flag = true; break; &#125; &#125; if(flag) &#123; this.tags.forEach(function(tag)&#123; emit(tag, &#123;count:1&#125;); &#125;); &#125;&#125;, function(key, values)&#123; return values.length; &#125;, &#123;out:"aa",finalize: function(key,reduced)&#123; if(reduced.count) &#123; return reduced.count; &#125; return reduced; &#125;&#125;);&gt; db.getCollection('aa').find(&#123;&#125;)&#123; "_id" : "java", "value" : 1 &#125;&#123; "_id" : "mongodb", "value" : 1 &#125;&#123; "_id" : "python", "value" : 1 &#125; 聚合框架（aggregation framework）聚合框架是mongo聚合操作的第三种方式，它与group()解决的问题是的一样的，group()和MapReduce都是需要写函数的，所以mongodb提供了聚合框架，简化聚合操作。 注意：只有MapReduce和聚合框架能分片的环境下使用。 聚合框架是不能自定义函数的，它帮我们实现了一些函数。 https://docs.mongodb.com/manual/core/aggregation-pipeline/index.html 聚合框架为我们提供了很多操作，aggregate参数是一个pipeline，每一个按操作按顺序执行。 https://docs.mongodb.com/manual/reference/operator/aggregation/ 常用的几个操作： $project 过滤需要用到的字段 $match 查询，过滤需要用到的数据 $unwind 用于将一个文档拆成多个文档 {id:tags[‘java’,’python’,’ruby’]} $unwind {id:1,tags:’java’},{id:1,tags:’python’},{id:1,tags:’ruby’} $group 分组，需要指定_id字段为分组的key $group本身又提供一些操作：https://docs.mongodb.com/manual/reference/operator/aggregation/group/#accumulator-operator 实例1-长度数据准备： 12345db.commodity.insert(&#123;category:1, price:200, name: 'name1'&#125;);db.commodity.insert(&#123;category:1, price:300, name: 'name2'&#125;);db.commodity.insert(&#123;category:2, price:100, name: 'name3'&#125;);db.commodity.insert(&#123;category:2, price:500, name: 'name4'&#125;);db.commodity.insert(&#123;category:3, price:200, name: 'name5'&#125;); 按照category分组并统计每一组的长度 1234&gt; db.commodity.aggregate(&#123;$group: &#123;_id:'$category', totalCount: &#123;$sum:1&#125;&#125;&#125;);&#123; "_id" : 3, "totalCount" : 1 &#125;&#123; "_id" : 2, "totalCount" : 2 &#125;&#123; "_id" : 1, "totalCount" : 2 &#125; 实例2-求和1234&gt; db.commodity.aggregate(&#123;$group: &#123;_id:'$category', totalCount: &#123;$sum:1&#125;, totalPrice: &#123;$sum: '$price'&#125;&#125;&#125;);&#123; "_id" : 3, "totalCount" : 1, "totalPrice" : 200 &#125;&#123; "_id" : 2, "totalCount" : 2, "totalPrice" : 600 &#125;&#123; "_id" : 1, "totalCount" : 2, "totalPrice" : 500 &#125; 在没有指定排序规则的时候，aggregate是不保证排序的，需要用$sort指定排序。 1234&gt; db.commodity.aggregate(&#123;$group: &#123;_id:'$category', totalCount: &#123;$sum:1&#125;, totalPrice: &#123;$sum: '$price'&#125;&#125;&#125;, &#123;$sort: &#123;'totalPrice':-1&#125;&#125;);&#123; "_id" : 2, "totalCount" : 2, "totalPrice" : 600 &#125;&#123; "_id" : 1, "totalCount" : 2, "totalPrice" : 500 &#125;&#123; "_id" : 3, "totalCount" : 1, "totalPrice" : 200 &#125; 实例3-求平均值1234&gt; db.commodity.aggregate(&#123;$group: &#123;_id:'$category', totalCount: &#123;$sum:1&#125;, totalPrice: &#123;$sum: '$price'&#125;, avgPrice: &#123;$avg:'$price'&#125;&#125;&#125;, &#123;$sort: &#123;'totalPrice':-1&#125;&#125;);&#123; "_id" : 2, "totalCount" : 2, "totalPrice" : 600, "avgPrice" : 300 &#125;&#123; "_id" : 1, "totalCount" : 2, "totalPrice" : 500, "avgPrice" : 250 &#125;&#123; "_id" : 3, "totalCount" : 1, "totalPrice" : 200, "avgPrice" : 200 &#125; 实例4-标签统计之前在演示MapReduce的时候，用MapReduce对article的tags统计tag出现的次数，这里用聚合框架来达到同样的效果，当然想到的就是之前介绍到的$unwind操作。 12345678910&gt; db.article.aggregate(&#123;$unwind: '$tags'&#125;, &#123;$group: &#123;_id:'$tags', total:&#123;$sum: 1&#125;&#125;&#125;);&#123; "_id" : "redis", "total" : 1 &#125;&#123; "_id" : "groovy", "total" : 2 &#125;&#123; "_id" : "kotlin", "total" : 3 &#125;&#123; "_id" : "scala", "total" : 1 &#125;&#123; "_id" : "perl", "total" : 4 &#125;&#123; "_id" : "python", "total" : 1 &#125;&#123; "_id" : "mongodb", "total" : 4 &#125;&#123; "_id" : "java", "total" : 1 &#125;&#123; "_id" : "ruby", "total" : 3 &#125; 实例5-标签过滤找出打了java标签的文章并进行统计 12345&gt; db.article.aggregate(&#123;$match:&#123;'tags' : &#123;$in:['java']&#125;&#125;&#125;, &#123;$unwind: '$tags'&#125;, &#123;$group: &#123;_id:'$tags', total:&#123;$sum: 1&#125;&#125;&#125;);&#123; "_id" : "ruby", "total" : 1 &#125;&#123; "_id" : "mongodb", "total" : 1 &#125;&#123; "_id" : "python", "total" : 1 &#125;&#123; "_id" : "java", "total" : 1 &#125; 实例6-各种操作12345678&gt; db.article.aggregate(&#123;$match:&#123;'tags' : &#123;$nin:['java']&#125;&#125;&#125;, &#123;$unwind: '$tags'&#125;, &#123;$group: &#123;_id:'$tags', total:&#123;$sum: 1&#125;&#125;&#125;, &#123;$sort:&#123;'total':-1&#125;&#125;);&#123; "_id" : "perl", "total" : 4 &#125;&#123; "_id" : "kotlin", "total" : 3 &#125;&#123; "_id" : "mongodb", "total" : 3 &#125;&#123; "_id" : "groovy", "total" : 2 &#125;&#123; "_id" : "ruby", "total" : 2 &#125;&#123; "_id" : "redis", "total" : 1 &#125;&#123; "_id" : "scala", "total" : 1 &#125; 取前三： 1234&gt; db.article.aggregate(&#123;$match:&#123;'tags' : &#123;$nin:['java']&#125;&#125;&#125;, &#123;$unwind: '$tags'&#125;, &#123;$group: &#123;_id:'$tags', total:&#123;$sum: 1&#125;&#125;&#125;, &#123;$sort:&#123;'total':-1&#125;&#125;, &#123;$limit:3&#125;);&#123; "_id" : "perl", "total" : 4 &#125;&#123; "_id" : "mongodb", "total" : 3 &#125;&#123; "_id" : "kotlin", "total" : 3 &#125; 取第二到到第四： 1234&gt; db.article.aggregate(&#123;$match:&#123;'tags' : &#123;$nin:['java']&#125;&#125;&#125;, &#123;$unwind: '$tags'&#125;, &#123;$group: &#123;_id:'$tags', total:&#123;$sum: 1&#125;&#125;&#125;, &#123;$sort:&#123;'total':-1&#125;&#125;,&#123;$skip:1&#125; ,&#123;$limit:3&#125;);&#123; "_id" : "mongodb", "total" : 3 &#125;&#123; "_id" : "kotlin", "total" : 3 &#125;&#123; "_id" : "groovy", "total" : 2 &#125; 只显示total： 1234&gt; db.article.aggregate(&#123;$match:&#123;'tags' : &#123;$nin:['java']&#125;&#125;&#125;, &#123;$unwind: '$tags'&#125;, &#123;$group: &#123;_id:'$tags', total:&#123;$sum: 1&#125;&#125;&#125;, &#123;$sort:&#123;'total':-1&#125;&#125;,&#123;$skip:1&#125; ,&#123;$limit:3&#125;, &#123;$project:&#123;_id:0&#125;&#125;);&#123; "total" : 3 &#125;&#123; "total" : 3 &#125;&#123; "total" : 2 &#125; 把total加100 1234&gt; db.article.aggregate(&#123;$match:&#123;'tags' : &#123;$nin:['java']&#125;&#125;&#125;, &#123;$unwind: '$tags'&#125;, &#123;$group: &#123;_id:'$tags', total:&#123;$sum: 1&#125;&#125;&#125;, &#123;$sort:&#123;'total':-1&#125;&#125;,&#123;$skip:1&#125; ,&#123;$limit:3&#125;, &#123;$project:&#123;_id:0,hello:&#123;$add:['$total',100]&#125;&#125;&#125;);&#123; "hello" : 103 &#125;&#123; "hello" : 103 &#125;&#123; "hello" : 102 &#125;]]></content>
      <categories>
        <category>Mongo</category>
      </categories>
      <tags>
        <tag>Mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB2-查询详解]]></title>
    <url>%2F2018%2F01%2F01%2Fmongo2%2F</url>
    <content type="text"><![CDATA[查询MongoDB为我们提供了很强大的查询功能，之前演示的都比较简单，接下来将展示一些进阶用法。 数据准备1234db.personalinfo.remove(&#123;&#125;);db.personalinfo.save(&#123;name:'zhangsan',age:10&#125;);db.personalinfo.save(&#123;name:'lisi',age:11&#125;);db.personalinfo.save(&#123;name:'wangsu',age:12&#125;); 根据某个字段查询12&gt; db.personalinfo.find(&#123;age:11&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125; 过滤返回的字段find的第二个参数可以设置需要返回的字段，以节省网络传输 1234&gt; db.personalinfo.find(&#123;&#125;, &#123;name:1&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db4"), "name" : "zhangsan" &#125;&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi" &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu" &#125; 不能同时使用包含或者排除，如果需要排除，直接不写age的字段即可。 1234567&gt; db.personalinfo.find(&#123;&#125;, &#123;name:1, age:0&#125;);Error: error: &#123; "ok" : 0, "errmsg" : "Projection cannot have a mix of inclusion and exclusion.", "code" : 2, "codeName" : "BadValue"&#125; findOne()findOne()跟find()的参数是一样的，只是findOne只返回查询到的第一条数据。 条件运算符大于&amp;小于123&gt; db.personalinfo.find(&#123;age:&#123;$gt:10&#125;&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu", "age" : 12 &#125; 大于：$gt 大于等于：$gte 小于：$lt 小于等于：$lte 多个条件： 12&gt; db.personalinfo.find(&#123;age:&#123;$gt:10,$lt:12&#125;&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125; $all$all运算符表示从数组中过滤包含的字段 123456789db.address.insert(&#123;name:['beijing','tianjin']&#125;);db.address.insert(&#123;name:['beijing','shanghai']&#125;);db.address.insert(&#123;name:['dalian','shanghai']&#125;);&gt; db.address.find(&#123;name: &#123;$all: ['beijing','tianjin']&#125;&#125;);&#123; "_id" : ObjectId("5a4994c2f0ec47e9f5ce2db7"), "name" : [ "beijing", "tianjin" ] &#125;&gt; db.address.find(&#123;name: &#123;$all: ['shanghai']&#125;&#125;);&#123; "_id" : ObjectId("5a4994c2f0ec47e9f5ce2db8"), "name" : [ "beijing", "shanghai" ] &#125;&#123; "_id" : ObjectId("5a4994c2f0ec47e9f5ce2db9"), "name" : [ "dalian", "shanghai" ] &#125; $exists查询包含某个字段的文档。 1234&gt; db.personalinfo.find(&#123;age:&#123;$exists:true&#125;&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db4"), "name" : "zhangsan", "age" : 10 &#125;&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu", "age" : 12 &#125; $exists要和$in结合使用来判断某个字段的值为null，并且字段真的存在的情况下，默认age:null的查询方式，会把没有这个字段的数据也查出来。 12345678910&gt; db.personalinfo.insert(&#123;name:'zhaoliu', age:null&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.insert(&#123;name:'zhaoliu', myage:14&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.find(&#123;age:null&#125;);&#123; "_id" : ObjectId("5a499774f0ec47e9f5ce2dba"), "name" : "zhaoliu", "age" : null &#125;&#123; "_id" : ObjectId("5a49977ef0ec47e9f5ce2dbb"), "name" : "zhaoliu", "myage" : 14 &#125;&gt; db.personalinfo.find(&#123;age:&#123;$in:[null],$exists: true&#125;&#125;);&#123; "_id" : ObjectId("5a499774f0ec47e9f5ce2dba"), "name" : "zhaoliu", "age" : null &#125;&gt; $mod$mod是取模 12&gt; db.personalinfo.find(&#123;age:&#123;$mod:[5,2]&#125;&#125;);&#123; &quot;_id&quot; : ObjectId(&quot;5a498d87f0ec47e9f5ce2db6&quot;), &quot;name&quot; : &quot;wangsu&quot;, &quot;age&quot; : 12 &#125; $ne$ne表示不等于 1234567891011&gt; db.personalinfo.find(&#123;age:&#123;$ne:5&#125;&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db4"), "name" : "zhangsan", "age" : 10 &#125;&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu", "age" : 12 &#125;&#123; "_id" : ObjectId("5a499774f0ec47e9f5ce2dba"), "name" : "zhaoliu", "age" : null &#125;&#123; "_id" : ObjectId("5a49977ef0ec47e9f5ce2dbb"), "name" : "zhaoliu", "myage" : 14 &#125;&gt; db.personalinfo.find(&#123;age:&#123;$ne:5, $exists:1&#125;&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db4"), "name" : "zhangsan", "age" : 10 &#125;&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu", "age" : 12 &#125;&#123; "_id" : ObjectId("5a499774f0ec47e9f5ce2dba"), "name" : "zhaoliu", "age" : null &#125; $in123&gt; db.personalinfo.find(&#123;age:&#123;$in:[11,12]&#125;&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu", "age" : 12 &#125; not in 12&gt; db.personalinfo.find(&#123;age:&#123;$nin:[11,12]&#125;&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db4"), "name" : "zhangsan", "age" : 10 &#125; $size根据数组长度筛选 123456db.mydemo.insert(&#123;myarray:[1,2,3,4]&#125;);db.mydemo.insert(&#123;myarray:[1,2,3]&#125;);db.mydemo.insert(&#123;myarray:[1,2,3,5]&#125;);&gt; db.mydemo.find(&#123;myarray: &#123;$size: 3&#125;&#125;)&#123; "_id" : ObjectId("5a4999d8f26cb8a9ec47407e"), "myarray" : [ 1, 2, 3 ] &#125; /a/查询某个字段包含字符 123&gt; db.personalinfo.find(&#123;name:/a/&#125;)&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db4"), "name" : "zhangsan", "age" : 10 &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu", "age" : 12 &#125; $where$where比较常用在一些复杂的查询条件，它的内容是一个JavaScript的代码表达式。 123&gt; db.personalinfo.find(&#123;$where: 'this.age &gt; 10'&#125;);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu", "age" : 12 &#125; 分页12345678&gt; db.personalinfo.find().count();3&gt; db.personalinfo.find().skip(1);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125;&#123; "_id" : ObjectId("5a498d87f0ec47e9f5ce2db6"), "name" : "wangsu", "age" : 12 &#125;&gt; db.personalinfo.find().limit(2);&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db4"), "name" : "zhangsan", "age" : 10 &#125;&#123; "_id" : ObjectId("5a498d86f0ec47e9f5ce2db5"), "name" : "lisi", "age" : 11 &#125; count()默认是统计整个文档的数量，要根据之前的条件统计，需要加上参数true。 1234&gt; db.personalinfo.find().limit(2).count();3&gt; db.personalinfo.find().limit(2).count(true);2 查询就介绍到这里，后面的文章将继续介绍MongoDB的聚合操作。]]></content>
      <categories>
        <category>Mongo</category>
      </categories>
      <tags>
        <tag>Mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何理财？基金定投！]]></title>
    <url>%2F2017%2F12%2F13%2F%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%2F</url>
    <content type="text"><![CDATA[理财在工作之余，理财也是非常重要的一件事情，现在很多人的投资渠道无非是买一些P2P，要不就买房。现在买房不太划算了，因为不能贷款，没有杠杆，如果是投资的话，性价比就不是很好了，最重要的是流动性太差，在你需要现金的时候，你拿不出来。 最近接触了一个公众号，里面讲了很多基金和投资的知识，可以说是受益匪浅，给我普及了一个价值投资的观念。 基金公众号发了这么几篇文章，说是凝集了几十本书的知识和多年投资经验的总结。 基金定投三部曲 除了在公众号获取知识之余，还在东方财富网的股吧里面发现了一位大神，每个月会发几篇长篇大论，也是满满的干货，对我等小白指明了投资方向。 非常适合基金小白用户阅读，这里也推荐几篇帖子： 新手要警剔短期基金排名“陷阱”！ 新手如何制定自己的基金投资计划？ 遭人嫌弃的中证500还值得投资吗？附2018年策略！ 我什么也不懂，只能照葫芦画瓢，结合两位大师的观点对自己的投资进行了计划。 投资计划大师们投资都讲究资产配置，规避风险。 我的方案就是：资产配置=固定收益+浮动收益 比如资金5万，4成投资基金，6成投资余额宝或者货币基金。 品种 金额 基金 20000 固收 30000 我们采取定投的方式来投资基金，因为是定投，可以将未来的工资收入新增的投资金额考虑其中，比如发工资后每月还定投1000（400基金，600余额宝）。 根据估值定投我们在看这些帖子和文章的时候经常会看到一个叫做市盈率的名字，知乎上有一个回答特别通俗易懂： PE （市盈率）是什么意思？ 策略：估值在分为标准差50%以下的时候开始定投，越跌越买，当估值上涨到70%以上的时候，开始分批止盈。资金按仓位分批定投，当下跌过大的时候加一笔。 例如目前中证500的指数是6300，PE是28.56。 准备4成仓位，其中定投2成，低位加码2成，另预备1成仓应对极端行情。 设置周投(双周)定投，期限12-18个月，资金平均分配，按18个月算就是每月投资也就是每月550+400。 以目前位置中证500指数每跌5%，9%，13%，15%分别加码0.5成仓。 止盈：当投资收益&gt;30%且PE&gt;35倍以上考虑分批止盈且暂停定投；当收益&gt;50%且PE&gt;40倍以上，赎回投资本金，其它待定，也可以等牛市止盈；（或者估值的分位标准差在70%以上的时候分批止盈） 极端行情：当中证500下跌超20%或者估值处于20倍以下时，启动预备仓位至少1-2成，到时建议把固收类转移过来。 止损计划：无，止盈不止损。 指数估值的市盈率查看地址：https://www.jisilu.cn/data/idx/ 注意图里面的PE就是市盈率，PE百分位，就是分为标准差，表示在这个指数的历史中，他大白了多少比例的自己。例如中证500的17%，意思是现在的估值已经高于了历史上17%的时间。 最后根据目前的状况，中证500是一个比较好播种的指数，我自己的投的163110，它跟踪中证500的指数90%，但是他波动更大，意思就是涨得更多，跌的也更多。 上面写了一堆多少多少又加仓，直观一点计算出来就是下面的表：]]></content>
      <tags>
        <tag>基金</tag>
        <tag>理财</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-编解码器&处理器]]></title>
    <url>%2F2017%2F12%2F12%2Fnetty9%2F</url>
    <content type="text"><![CDATA[Netty处理器编解码器本质上也是ChannelHandler的特殊实现，Netty本身为我们提供了很多处理器。 Netty处理器重要概念： Netty的处理器可以分为两类：入站处理器与出战处理器。 入站处理器的顶层是ChannelInboundHandler，出站处理器的顶层是ChannelOutboundHandler。 数据处理时常用的各种编解码器本质上都是处理器。 编解码器：无论我们向网络中写入的数据是什么类型（int、char、String、二进制等），数据在网络中传递时，其都是以字节流的形式呈现的；将数据由原本的形式转换为字节流的操作称为编码（encode），将数据由字节转换为它原本的格式或是其他格式的操作称为解码（decode），编解码统一称为codec。 编码：本质上是一种出站处理器，因此，编码是一种ChannelOutboundHandler。 解码：本质上是一种入站处理器，因此，解码是一种ChannelInboundHandler。 在Netty中，编码器通常以XXXEncoder命名；解码器通常以XXXDecoder命名。 回顾回顾一个之前写的例子： 123456789protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0,4,0,4)); pipeline.addLast(new LengthFieldPrepender(4)); pipeline.addLast(new StringDecoder(CharsetUtil.UTF_8)); pipeline.addLast(new StringEncoder(CharsetUtil.UTF_8)); // 最后添加我们自己的处理器 pipeline.addLast(new MyServerHandler());&#125; 学习到现在，我们已经很清楚的知道ChannelInitializer的职责，他本身是一个特殊的ChannelHandler，是用来初始化添加处理器的，在添加完成后，它自己会被销毁。 在这个例子中，根据命名或者他的继承类可以看出来，这里一共有4个入站处理器，1个出站处理器，虽然添加的时候代码都写在一起，实际上数据的流向却是两条线，从上往下进行解码，最后我们自定义的处理器拿到的时候就已经是字符串了，写出数据的时候也一样，写出的是String，但是通过StringEncoder转换成了字节。 自定义实现在io.netty.handler.codec包中，包含了Netty为我们提供的很多编解码器。 下面我们自己实现一个。 要实现的效果：当客户端的channelActive事件触发的时候，客户端向服务端发送一个Long类型的数据，服务端也返回一个Long类型的数据。 MyServerInitializer.java 123456789public class MyServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); // 需要将字节转换为Long pipeline.addLast(new MyServerHandler()); &#125;&#125; 我们需要一个解码器，来将字节转换Long类型的数据，然后MyServerHandler才能处理。 Netty为我们提供了一个抽象类ByteToMessageDecoder，它是一个ChannelInboundHandlerAdapter，它的作用是将ByteBuf转换成另外一种消息类型，这个消息类型是我们自己来定的。 MessageToByteEncoder&amp;ByteToMessageDecoderNetty为我们提供了一个MessageToByteEncoder，基本大多数的解码器都直接或间接的实现了这个抽象类，我们也实现这个类，是需要直线它的encode方法。相对应的，编码器是ByteToMessageDecoder，需要实现它的decode方法。 自定义解码器将字节转换成一个Long类型的数据。 12345678910111213public class MyByteToLongDecoder extends ByteToMessageDecoder &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; System.out.println("decode invoked"); System.out.println(in.readableBytes()); // Long是8个字节 if (in.readableBytes() &gt;=8 ) &#123; out.add(in.readLong()); &#125; &#125;&#125; 自定义编码器将Long转换为字节写入，编码器是有泛型的。 123456public class MyLongToByteEncoder extends MessageToByteEncoder&lt;Long&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, Long msg, ByteBuf out) throws Exception &#123; out.writeLong(msg); &#125;&#125; 以上完整实例代码在：https://github.com/sail-y/netty/tree/master/src/main/java/com/sail/netty/handler 数据执行流程： 客户端先编码发送数据，然后服务端解码，收到数据后再编码写出一个数据，客户端最后再解码。 客户端MyLongToByteEncoder -&gt; 服务端MyByteToLongDecoder -&gt; 服务端MyServerHandler -&gt; 服务端MyLongToByteEncoder -&gt; 客户端MyByteToLongDecoder -&gt; 客户端MyClientHandler 如果客户端再返回一个字符串，那么客户端的MyLongToByteEncoder就已经执行失败了，所以数据不会发送给服务端。 ReplayingDecoderReplayingDecoder 是 byte-to-message 解码的一种特殊的抽象基类，读取缓冲区的数据之前需要检查缓冲区是否有足够的字节，使用ReplayingDecoder就无需自己检查；若ByteBuf中有足够的字节，则会正常读取；若没有足够的字节则会停止解码。 The biggest difference between ReplayingDecoder and ByteToMessageDecoder is that ReplayingDecoder allows you to implement the decode() and decodeLast() methods just like all required bytes were received already, rather than checking the availability of the required bytes. 意思是我们在使用ReplayingDecoder的时候，就像数据已经全部接受到了一样，不用再去检测数据是否已经接受足够可以解码了。如果数据够了，它就直接读取，如果数据不够，它就抛出一个Error，ReplayingDecoder会捕获这个错误，然后ReplayingDecoder继续处理，并重置buffer的readerIndex，直到处理成功为止。 ReplayingDecoder的限制： 某些buffer操作是被禁止的 如果网络很慢，消息也很复杂，可能性能比较差 一个消息的decode方法可能会被调用很多次 自定义实现继承ReplayingDecoder实现一个解码器，替换之前的实现。 1234567public class MyByteToLongDecoder2 extends ReplayingDecoder&lt;Void&gt; &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) &#123; System.out.println("decode invoked"); out.add(in.readLong()); &#125;&#125; MessageToMessageDecoder用于从一种消息解码为另外一种消息（例如，POJO 到 POJO）,将 Integer 转为 String，我们提供了 IntegerToStringDecoder，继承自 MessageToMessageDecoder。 1234567public class MyLongToStringDecoder extends MessageToMessageDecoder&lt;Long&gt; &#123; @Override protected void decode(ChannelHandlerContext ctx, Long msg, List&lt;Object&gt; out) &#123; out.add(msg.toString()); &#125;&#125; LengthFieldBasedFrameDecoderLengthFieldBasedFrameDecoder是一个非常常用的解码器，它会将ByteBuf根据消息里长度的值进行分割，这对有消息头里有长度的二进制消息特别有用。 关于LengthFieldBasedFrameDecoder的具体使用，和它的应用场景在文章后面的TCP粘包和拆包有演示。 关于Netty编解码器的重要结论： 无论是编码器还是解码器，其所接收的消息类型必须要与待处理的参数类型一致，否则该编码器或解码器并不会被执行。 在解码器进行数据解码时，一定要记得判断缓冲（ByteBuf）中的数据是否足够，否则将会产生一些问题。 TCP粘包和拆包TCP是个“流”协议，所谓流，就是没有界限的一串数据。大家可以想想河里的流水，是连成一片的，其间并没有分界线。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分，所以在业务上认为，一个完整的包可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。 粘包演示1234567891011121314151617181920212223242526272829303132public class MyClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; private int count; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; byte[] buffer = new byte[msg.readableBytes()]; msg.readBytes(buffer); String message = new String(buffer, CharsetUtil.UTF_8); System.out.println("客户端接收到的消息内容：" + message); System.out.println("客户端接收到的消息数量：" + ++count); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; /** * 如果不重写这个方法，运行程序后并不会触发数据的传输，因为双方都在等待read，所以要先发送一次消息。 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; for (int i = 0; i &lt; 10; i++) &#123; ByteBuf buffer = Unpooled.copiedBuffer("send from client", CharsetUtil.UTF_8); ctx.writeAndFlush(buffer); &#125; &#125;&#125; 控制台输出结果： 12客户端接收到的消息内容：496faaef-6ed7-4802-bdd7-d4e9客户端接收到的消息数量：1 1234567891011121314151617181920212223242526272829public class MyServerHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; private int count; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; byte[] buffer = new byte[msg.readableBytes()]; msg.readBytes(buffer); String message = new String(buffer, CharsetUtil.UTF_8); System.out.println("服务端接收到的消息内容：" + message); System.out.println("服务器接收到的消息数量：" + (++count)); ByteBuf responseByteBuf = Unpooled.copiedBuffer(UUID.randomUUID().toString(), CharsetUtil.UTF_8); ctx.writeAndFlush(responseByteBuf); &#125; /** * 如果出现异常，关闭连接 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 控制条输出的内容： 12服务端接收到的消息内容：send from clientsend from clientsend from clientsend from clientsend from clientsend from clientsend from clientsend from clientsend from clientsend from client服务器接收到的消息数量：1 如果再重启几次客户端，服务端的结果还会发生如下变化，这是没有什么规律的。 123456789101112131415161718192021222324服务端接收到的消息内容：send from client服务器接收到的消息数量：1服务端接收到的消息内容：send from client服务器接收到的消息数量：2服务端接收到的消息内容：send from clientsend from clientsend from clientsend from clientsend from clientsend from clientsend from clientsend from client服务器接收到的消息数量：3服务端接收到的消息内容：send from client服务器接收到的消息数量：1服务端接收到的消息内容：send from client服务器接收到的消息数量：2服务端接收到的消息内容：send from clientsend from clientsend from clientsend from client服务器接收到的消息数量：3服务端接收到的消息内容：send from clientsend from clientsend from clientsend from client服务器接收到的消息数量：4服务端接收到的消息内容：send from client服务器接收到的消息数量：1服务端接收到的消息内容：send from client服务器接收到的消息数量：2服务端接收到的消息内容：send from clientsend from clientsend from client服务器接收到的消息数量：3服务端接收到的消息内容：send from clientsend from client服务器接收到的消息数量：4服务端接收到的消息内容：send from clientsend from clientsend from client服务器接收到的消息数量：5 完整可运行的代码在这里：https://github.com/sail-y/netty/tree/master/src/main/java/com/sail/netty/handler2 解决粘包-&gt;拆包演示自定义一个协议，它包含了长度和内容两个字段 12345678910111213141516171819202122public class PersonProtocol &#123; private int length; private byte[] content; public int getLength() &#123; return length; &#125; public void setLength(int length) &#123; this.length = length; &#125; public byte[] getContent() &#123; return content; &#125; public void setContent(byte[] content) &#123; this.content = content; &#125;&#125; 解码器继承自ReplayingDecoder，好处是不需要去判断消息长度是否已经足够。 1234567891011121314151617public class MyPersonDecoder extends ReplayingDecoder&lt;Void&gt; &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) &#123; System.out.println("MyPersonDecoder decode invoked!"); int length = in.readInt(); byte[] content = new byte[length]; in.readBytes(content); PersonProtocol personProtocol = new PersonProtocol(); personProtocol.setLength(length); personProtocol.setContent(content); out.add(personProtocol); &#125;&#125; 编码器就要简单很多了，需要将PersonProtocol输出为bytes。 12345678910public class MyPersonEncoder extends MessageToByteEncoder&lt;PersonProtocol&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, PersonProtocol msg, ByteBuf out) &#123; System.out.println("MyPersonEncoder encode invoked!"); out.writeInt(msg.getLength()); out.writeBytes(msg.getContent()); &#125;&#125; 以及ServerHandler： 123456789101112131415161718192021222324public class MyServerHandler extends SimpleChannelInboundHandler&lt;PersonProtocol&gt; &#123; private int count; @Override protected void channelRead0(ChannelHandlerContext ctx, PersonProtocol msg) throws UnsupportedEncodingException &#123; int length = msg.getLength(); byte[] content = msg.getContent(); System.out.println("服务端接收到的数据："); System.out.println("长度；" + length); System.out.println("内容：" + new String(content, CharsetUtil.UTF_8)); System.out.println("服务端接受到的消息数量：" + ++count); String responseMessage = UUID.randomUUID().toString(); int responseLength = responseMessage.getBytes("utf-8").length; byte[] responseContent = responseMessage.getBytes("utf-8"); PersonProtocol personProtocol = new PersonProtocol(); personProtocol.setLength(responseLength); personProtocol.setContent(responseContent); ctx.writeAndFlush(personProtocol); &#125;&#125; 篇幅有限，剩余的代码文章就不贴完了，代码都在：https://github.com/sail-y/netty/tree/master/src/main/java/com/sail/netty/handler3 客户端输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：3726cb1a-163f-498c-82b2-9731aeff94e0客户端接受到的消息数量：1MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：7cbb68f4-bdfd-4cf8-8de6-e18f78d58770客户端接受到的消息数量：2MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：463d503d-873f-4a4e-8a62-f3c1ccedd6ce客户端接受到的消息数量：3MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：6d8b8361-18e5-402c-8977-3ba4316633f4客户端接受到的消息数量：4MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：eccdc204-d589-4e24-8585-36d4c07f8ce8客户端接受到的消息数量：5MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：3dd02117-3a8b-4d2e-8c9c-e0a356cb54e1客户端接受到的消息数量：6MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：5db00fcc-1e07-4ac1-bb61-f514049d1643客户端接受到的消息数量：7MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：b306c7f5-790e-45f8-8031-f7fd74a54b07客户端接受到的消息数量：8MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：386acc20-986a-441a-a265-849a45c28119客户端接受到的消息数量：9MyPersonDecoder decode invoked!客户端接收到的数据：长度；36内容：24c13a50-60c4-4d34-9d8e-fde19beda657客户端接受到的消息数量：10 先是发送了10条消息，然后再接收到了10条消息。 服务端也是收到了10条消息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：1MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：2MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：3MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：4MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：5MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：6MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：7MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：8MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：9MyPersonEncoder encode invoked!MyPersonDecoder decode invoked!服务端接收到的数据：长度；17内容：sent from client 服务端接受到的消息数量：10MyPersonEncoder encode invoked!]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB1-安装和增删改查]]></title>
    <url>%2F2017%2F12%2F08%2Fmongo1%2F</url>
    <content type="text"><![CDATA[安装https://www.mongodb.com/download-center 按照传统的方式，可以在这里下载mongo的安装包进行安装。 OSX如果是mac系统，并且按照了brew，可以直接执行brew install mongodb进行安装。 docker现在用docker快速的搭建软件环境已经非常方便了，所以我在这里将使用docker下载mongo的镜像来部署mongo，方便随后的测试。 https://hub.docker.com/_/mongo/ dockerhub提供了官方镜像，我们选择好想要下载的版本，然后映射/data/db到我们本机的目录，--auth表示开启授权验证，mongo默认是不需要密码的，本机测试我们可以先不加这个参数。 1docker run --name my-mongo -v /Users/xiaomai/software/mongo:/data/db -d -p 27017:27017 -p 28017:28017 mongo:3.6.0 --auth 因为是docker安装的，所以我们不能直接在本机上执行mongo命令来连接。 可以使用下面的命令来连接mongo： 1docker exec -it my-mongo mongo test 查看mongo的日志： 1docker logs -f my-mongo 安装完毕。 客户端连接工具免费版： https://robomongo.org/download 收费版： https://studio3t.com/download/ 这是同一个公司开发的，目前是比较推荐这一款工具，非常好用。 简单使用123456789101112&gt; show dbs;admin 0.000GBconfig 0.000GBlocal 0.000GB&gt; use testswitched to db test&gt; dbtest&gt; show dbs;admin 0.000GBconfig 0.000GBlocal 0.000GB 查看数据库列表，切换了数据库，再次查看数据库列表，却发现没有新增的test数据库，这是mongo的特殊所在，现在的test只是一个临时数据库，再没有产生数据的时候，他不会被真正的创建只有当真正的插入数据的时候，数据库随之才会被创建。 插入1234567&gt; db.hello.insert(&#123;name:'zhangsan', age:20&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; show dbs;admin 0.000GBconfig 0.000GBlocal 0.000GBtest 0.000GB 查询查询一下刚才插入的数据： 123&gt; db.hello.find(&#123;&#125;)&#123; "_id" : ObjectId("5a2a3af1e2beff69d97c54c3"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2a3b3ae2beff69d97c54c4"), "name" : "lisi", "age" : 30 &#125; 刚才插入的数据被查询出来了，mongodb用的就是JavaScript的语法，他们的数据也是用一种叫做bson的数据结构存储的，类似于json。 _id这两条数据都有一个_id的字段，是mongo自动生成的主键/索引。 删除删除数据的演示，{}里面可以加删除的过滤条件。 123&gt; db.hello.remove(&#123;&#125;);WriteResult(&#123; "nRemoved" : 2 &#125;)&gt; db.hello.find(&#123;&#125;).limit(1); 修改数据准备可以直接使用JS语法： 1234567&gt; use mytest;switched to db mytest&gt; var info = &#123;name: 'zhangsan', age:10, city: 'chengdu', street: 'gaoxin'&#125;;&gt; db.personalinfo.insert(info);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f847dcdf5d844e48cddb7"), "name" : "zhangsan", "age" : 10, "city" : "chengdu", "street" : "gaoxin" &#125; 修改先把数据查出来： 123456789&gt; var info = db.personalinfo.findOne(&#123;name:'zhangsan'&#125;);&gt; info&#123; "_id" : ObjectId("5a2f847dcdf5d844e48cddb7"), "name" : "zhangsan", "age" : 10, "city" : "chengdu", "street" : "gaoxin"&#125; 一样的用js语法对数据进行修改： 1234567891011121314&gt; info.address = &#123;'city':info.city, 'street':info.street&#125;;&#123; "city" : "chengdu", "street" : "gaoxin" &#125;&gt; info.username = info.name;zhangsan&gt; info.userage = info.age;10&gt; delete info.name;true&gt; delete info.age;true&gt; delete info.city;true&gt; delete info.street;true 完成修改，update有4个参数，第一个是查询条件，第二个是替换的对象，还有两个参数是可选的： 1234&gt; db.personalinfo.update(&#123;'name':'zhangsan'&#125;, info);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f847dcdf5d844e48cddb7"), "address" : &#123; "city" : "chengdu", "street" : "gaoxin" &#125;, "username" : "zhangsan", "userage" : 10 &#125; 修改器演示1234567891011121314151617181920212223242526&gt; db.personalinfo.remove(&#123;&#125;);&gt; db.personalinfo.insert(&#123;name:'zhangsan', age:10&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.insert(&#123;name:'zhangsan', age:20&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.insert(&#123;name:'zhangsan', age:30&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.insert(&#123;name:'zhangsan', age:40&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f8a2afd5fb3999c47142b"), "name" : "zhangsan", "age" : 10 &#125;&#123; "_id" : ObjectId("5a2f8a2dfd5fb3999c47142c"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2f8a30fd5fb3999c47142d"), "name" : "zhangsan", "age" : 30 &#125;&#123; "_id" : ObjectId("5a2f8a32fd5fb3999c47142e"), "name" : "zhangsan", "age" : 40 &#125;&gt; var zhangsan = db.personalinfo.findOne(&#123;name:'zhangsan',age:10&#125;);&gt; zhangsan.age++;10&gt; zhangsan.age11&gt; db.personalinfo.update(&#123;name:'zhangsan'&#125;, zhangsan);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f8a2afd5fb3999c47142b"), "name" : "zhangsan", "age" : 11 &#125;&#123; "_id" : ObjectId("5a2f8a2dfd5fb3999c47142c"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2f8a30fd5fb3999c47142d"), "name" : "zhangsan", "age" : 30 &#125;&#123; "_id" : ObjectId("5a2f8a32fd5fb3999c47142e"), "name" : "zhangsan", "age" : 40 &#125; 这里看到第一条记录被修改成了11岁。 12345678910111213&gt; var zhangsan = db.personalinfo.findOne(&#123;name:'zhangsan',age:40&#125;);&gt; zhangsan.age++;40&gt; db.personalinfo.update(&#123;name:'zhangsan'&#125;, zhangsan);WriteResult(&#123; "nMatched" : 0, "nUpserted" : 0, "nModified" : 0, "writeError" : &#123; "code" : 66, "errmsg" : "After applying the update, the (immutable) field '_id' was found to have been altered to _id: ObjectId('5a2f8a32fd5fb3999c47142e')" &#125;&#125;) 这次我们查询出40岁的张三，然后再次进行修改，这里得到一个错误，说_id不能被修改，说明update默认是条件过滤到的第一条数据来进行修改。 $inc修改器我们可以用_id字段来进行简便的修改，这里用了$inc修改器，来对数值进行修改： 12345678&gt; db.personalinfo.update(&#123;_id:ObjectId('5a2f8a2afd5fb3999c47142b')&#125;, &#123;$inc: &#123;age:1&#125;&#125;);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f8a2afd5fb3999c47142b"), "name" : "zhangsan", "age" : 12 &#125;&#123; "_id" : ObjectId("5a2f8a2dfd5fb3999c47142c"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2f8a30fd5fb3999c47142d"), "name" : "zhangsan", "age" : 30 &#125;&#123; "_id" : ObjectId("5a2f8a32fd5fb3999c47142e"), "name" : "zhangsan", "age" : 40 &#125;&gt; $set修改器$set修改可以完成对属性的设定，如果属性不存在则添加，如果存在则修改。 1234567&gt; db.personalinfo.update(&#123;_id:ObjectId('5a2f8a2afd5fb3999c47142b')&#125;, &#123;$set: &#123;address:'chengdu'&#125;&#125;);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f8a2afd5fb3999c47142b"), "name" : "zhangsan", "age" : 12, "address" : "chengdu" &#125;&#123; "_id" : ObjectId("5a2f8a2dfd5fb3999c47142c"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2f8a30fd5fb3999c47142d"), "name" : "zhangsan", "age" : 30 &#125;&#123; "_id" : ObjectId("5a2f8a32fd5fb3999c47142e"), "name" : "zhangsan", "age" : 40 &#125; 同样的，$unset修改器可以删除一个属性。 123456&gt; db.personalinfo.update(&#123;_id:ObjectId('5a2f8a2afd5fb3999c47142b')&#125;, &#123;$unset: &#123;address:'chengdu'&#125;&#125;);&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f8a2afd5fb3999c47142b"), "name" : "zhangsan", "age" : 12 &#125;&#123; "_id" : ObjectId("5a2f8a2dfd5fb3999c47142c"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2f8a30fd5fb3999c47142d"), "name" : "zhangsan", "age" : 30 &#125;&#123; "_id" : ObjectId("5a2f8a32fd5fb3999c47142e"), "name" : "zhangsan", "age" : 40 &#125; $push修改器$push修改器可以对数组进行操作，增加一个books的属性，是一个数组。 1234567&gt; db.personalinfo.update(&#123;_id:ObjectId('5a2f8a2afd5fb3999c47142b')&#125;, &#123;$push:&#123;books:'MongoDB'&#125;&#125;);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f8a2afd5fb3999c47142b"), "name" : "zhangsan", "age" : 12, "books" : [ "MongoDB" ] &#125;&#123; "_id" : ObjectId("5a2f8a2dfd5fb3999c47142c"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2f8a30fd5fb3999c47142d"), "name" : "zhangsan", "age" : 30 &#125;&#123; "_id" : ObjectId("5a2f8a32fd5fb3999c47142e"), "name" : "zhangsan", "age" : 40 &#125; $addToSet修改器跟Java里的Set概念，如果是重复的数据，则不会被添加。 12345678&gt; db.personalinfo.update(&#123;_id:ObjectId(&apos;5a2f8a2afd5fb3999c47142b&apos;)&#125;, &#123;$addToSet:&#123;books:&apos;MongoDB&apos;&#125;&#125;);WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 0 &#125;)&gt; db.personalinfo.find();&#123; &quot;_id&quot; : ObjectId(&quot;5a2f8a2afd5fb3999c47142b&quot;), &quot;name&quot; : &quot;zhangsan&quot;, &quot;age&quot; : 12, &quot;books&quot; : [ &quot;MongoDB&quot; ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5a2f8a2dfd5fb3999c47142c&quot;), &quot;name&quot; : &quot;zhangsan&quot;, &quot;age&quot; : 20 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5a2f8a30fd5fb3999c47142d&quot;), &quot;name&quot; : &quot;zhangsan&quot;, &quot;age&quot; : 30 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5a2f8a32fd5fb3999c47142e&quot;), &quot;name&quot; : &quot;zhangsan&quot;, &quot;age&quot; : 40 &#125;&gt; 我们再次为books属性添加了一个MongoDB的数据，跟我们期望的一样，重复的数据没有被添加。 修改数组某一个值 （索引）在需要修改数组中的某一个值的时候，还是可以用$set修改器来进行修改，只需要在key的后面加上一个索引。 12345678&gt; db.personalinfo.update(&#123;_id:ObjectId('5a2f8a2afd5fb3999c47142b')&#125;, &#123;$set:&#123;'books.0':'Mongo'&#125;&#125;);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f8a2afd5fb3999c47142b"), "name" : "zhangsan", "age" : 12, "books" : [ "Mongo", "JVM" ] &#125;&#123; "_id" : ObjectId("5a2f8a2dfd5fb3999c47142c"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2f8a30fd5fb3999c47142d"), "name" : "zhangsan", "age" : 30 &#125;&#123; "_id" : ObjectId("5a2f8a32fd5fb3999c47142e"), "name" : "zhangsan", "age" : 40 &#125;&gt; 修改数组某一个值（根据查询条件）先把books改成一个对象的数组 1234567&gt; db.personalinfo.update(&#123;_id:ObjectId('5a2f8a2afd5fb3999c47142b')&#125;, &#123;$push:&#123;'books':&#123;name:'Mongo', price:20&#125;&#125;&#125;);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a2f8a2afd5fb3999c47142b"), "name" : "zhangsan", "age" : 12, "books" : &#123; "name" : "Mongo", "price" : 20 &#125; &#125;&#123; "_id" : ObjectId("5a2f8a2dfd5fb3999c47142c"), "name" : "zhangsan", "age" : 20 &#125;&#123; "_id" : ObjectId("5a2f8a30fd5fb3999c47142d"), "name" : "zhangsan", "age" : 30 &#125;&#123; "_id" : ObjectId("5a2f8a32fd5fb3999c47142e"), "name" : "zhangsan", "age" : 40 &#125; 现在需要将books里mongo的价格改成30，但是不用索引。mongo为我们提供了一个$表示当前查询到的数据。update的前两个参数&lt;查询条件&gt;和&lt;更新操作&gt;中，如果你在&lt;查询条件&gt;中查询的内容是array里的内容，&lt;更新操作&gt;中就可以使用”$”来引用前查询中匹配到的元素。 1db.personalinfo.update(&#123;'books.name':'Mongo'&#125;, &#123;$set:&#123;'books.$.price':30&#125;&#125;); update参数详解1234&gt; db.personalinfo.updatefunction (query, obj, upsert, multi) &#123;....&#125; query表示查询条件 obj表示修改的对象 upsert表示更新或者插入 multi表示批量更新 upsert演示12345678910&gt; db.personalinfo.remove(&#123;&#125;);WriteResult(&#123; "nRemoved" : 4 &#125;)&gt; db.personalinfo.insert(&#123;name:'zhangsan'&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a3130c1fd5fb3999c47142f"), "name" : "zhangsan" &#125;&gt; db.personalinfo.update(&#123;name:'zhangsan'&#125;, &#123;$inc: &#123;count:3&#125;&#125;);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a3130c1fd5fb3999c47142f"), "name" : "zhangsan", "count" : 3 &#125; 直接修改不存在的属性，会新增一个属性。 这里查询了一个lisi的数据，默认情况下是不存在的，但是如果把upsert设置为true之后，就成了有数据就更新，没有就insert一条。 12345678910111213&gt; db.personalinfo.update(&#123;name:'lisi'&#125;, &#123;$inc: &#123;count:3&#125;&#125;);WriteResult(&#123; "nMatched" : 0, "nUpserted" : 0, "nModified" : 0 &#125;)&gt; db.personalinfo.update(&#123;name:'lisi'&#125;, &#123;$inc: &#123;count:3&#125;&#125;, true);WriteResult(&#123; "nMatched" : 0, "nUpserted" : 1, "nModified" : 0, "_id" : ObjectId("5a31313f7cb00d3f6c6c4591")&#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a3130c1fd5fb3999c47142f"), "name" : "zhangsan", "count" : 3 &#125;&#123; "_id" : ObjectId("5a31313f7cb00d3f6c6c4591"), "name" : "lisi", "count" : 3 &#125;&gt; 批量更新先准备几条数据： 12345678&gt; db.personalinfo.save(&#123;'name':'zhangsan','age':10,'address':'beijing'&#125;)WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.save(&#123;'name':'lisi','age':10,'address':'guangzhou'&#125;)WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.save(&#123;'name':'wangwu','age':20,'address':'shanghai'&#125;)WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.personalinfo.save(&#123;'name':'zhaoliu','age':10,'address':'shenzhen'&#125;)WriteResult(&#123; "nInserted" : 1 &#125;) 把第4个参数设置成了true，表示执行批量更新 1234567&gt; db.personalinfo.update(&#123;age:10&#125;, &#123;$set:&#123;company:'sap'&#125;&#125;, false, true);WriteResult(&#123; "nMatched" : 3, "nUpserted" : 0, "nModified" : 3 &#125;)&gt; db.personalinfo.find();&#123; "_id" : ObjectId("5a48c6c9f0ec47e9f5ce2dac"), "name" : "zhangsan", "age" : 10, "address" : "beijing", "company" : "sap" &#125;&#123; "_id" : ObjectId("5a48c6c9f0ec47e9f5ce2dad"), "name" : "lisi", "age" : 10, "address" : "guangzhou", "company" : "sap" &#125;&#123; "_id" : ObjectId("5a48c6c9f0ec47e9f5ce2dae"), "name" : "wangwu", "age" : 20, "address" : "shanghai" &#125;&#123; "_id" : ObjectId("5a48c6caf0ec47e9f5ce2daf"), "name" : "zhaoliu", "age" : 10, "address" : "shenzhen", "company" : "sap" &#125; savemongo提供了一个save操作，它的行为取决于你是否传入了_id参数，如果你没有传，会洗澡一个，如果你传了_id，则会进行修改。 1234567891011121314&gt; db.personalinfo.save(&#123;'_id':5,'username':'zhangsan'&#125;)WriteResult(&#123; "nMatched" : 0, "nUpserted" : 1, "nModified" : 0, "_id" : 5 &#125;)&gt; db.personalinfo.save(&#123;'_id':5,'username':'lisi'&#125;)WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.personalinfo.find();&#123; "_id" : 5, "username" : "lisi" &#125;&gt; db.personalinfo.insert(&#123;'_id':5,'username':'lisi'&#125;)WriteResult(&#123; "nInserted" : 0, "writeError" : &#123; "code" : 11000, "errmsg" : "E11000 duplicate key error collection: mytest.personalinfo index: _id_ dup key: &#123; : 5.0 &#125;" &#125;&#125;) findAndModify它是一种原子操作，正常情况下，查询并修改是两步操作，但是mongo为我们提供的这个函数，是原子性的，不会在多线程情况下出现并发问题。 Modifies and returns a single document. By default, the returned document does not include the modifications made on the update. To return the document with the modifications made on the update, use the new option. 修改并返回一个文档，默认是返回修改之前的文档，如果要返回修改之后的，设置new这个参数。 12345678910111213db.collection.findAndModify(&#123; query: &lt;document&gt;, sort: &lt;document&gt;, remove: &lt;boolean&gt;, update: &lt;document&gt;, new: &lt;boolean&gt;, fields: &lt;document&gt;, upsert: &lt;boolean&gt;, bypassDocumentValidation: &lt;boolean&gt;, writeConcern: &lt;document&gt;, collation: &lt;document&gt;, arrayFilters: [ &lt;filterdocument1&gt;, ... ]&#125;); 重要参数解释： query：查询参数 sort：排序的方式 remove：remove和update只能设置一个，删除查询条件筛选的内容 update：remove和update只能设置一个，修改查询条件筛选的内容 new：返回修改之后的文档 fields：表示返回的数据需要的字段 演示数据准备： 12345db.personalinfo.remove(&#123;&#125;);db.personalinfo.save(&#123;'name':'zhangsan','age':10,'address':'beijing'&#125;)db.personalinfo.save(&#123;'name':'lisi','age':10,'address':'guangzhou'&#125;)db.personalinfo.save(&#123;'name':'wangwu','age':20,'address':'shanghai'&#125;)db.personalinfo.save(&#123;'name':'zhaoliu','age':10,'address':'shenzhen'&#125;) 传入必要的参数，对数据进行修改，返回了修改成功的那一条数据。 12345678910111213&gt; db.personalinfo.findAndModify(&#123;... query: &#123;name:'zhangsan'&#125;,... sort: &#123;age:-1&#125;,... remove: false,... update:&#123;$set:&#123;address:'nanjing'&#125;&#125;,... new:true... &#125;);&#123; "_id" : ObjectId("5a48d102f0ec47e9f5ce2db0"), "name" : "zhangsan", "age" : 10, "address" : "nanjing"&#125;]]></content>
      <categories>
        <category>Mongo</category>
      </categories>
      <tags>
        <tag>Mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-cherry-pick、rebase]]></title>
    <url>%2F2017%2F11%2F30%2Fgit7%2F</url>
    <content type="text"><![CDATA[cherry-pickcherry-pick可以将你对某个分支的修改信息应用到另外一个分支上。 应用场景在现有一个文件在master分支上，但是却不小心在develop分支上做了修改并新增了两次提交，也就是提交错了分支。 传统方式是我们备份好已经修改的文件然后切换分支再复制过去，这样做太低效了，而且容易出错。 我们可以用cherry-pick来解决这个问题。 12345678910111213141516171819➜ git_cherry_pick git:(develop) git logcommit 9b7651337815b3697283c38a2cde43a17edf622b (HEAD -&gt; develop)Author: yangfan &lt;hyyangfan@gmail.com&gt;Date: Thu Nov 30 09:19:01 2017 +0800 hello2commit 0947abad01c529e1b14d0a364df55c295ae21896Author: yangfan &lt;hyyangfan@gmail.com&gt;Date: Thu Nov 30 09:18:50 2017 +0800 hello1commit aa53a19826d28c9f795efd901c386e4c6267594dAuthor: yangfan &lt;hyyangfan@gmail.com&gt;Date: Thu Nov 30 09:18:23 2017 +0800 initial commit(END) 12345➜ git_cherry_pick git:(master) git cherry-pick 0947ab[master 264078d] hello1 Date: Thu Nov 30 09:18:50 2017 +0800 1 file changed, 1 insertion(+)➜ git_cherry_pick git:(master) git自动的将commit应用到了另外一个分支上。现在在develop上再新增一次提交，如果我们直接在master上应用跨越2次的提交会怎么样呢？ 1234567891011121314151617commit 0318ff6fd8b6f7696701227ba6f260cbf4633524 (HEAD -&gt; develop)Author: yangfan &lt;hyyangfan@gmail.com&gt;Date: Thu Nov 30 09:24:12 2017 +0800 hello4commit d071b32aaabcd1ea0f5551b66605c5ac5ee6d464Author: yangfan &lt;hyyangfan@gmail.com&gt;Date: Thu Nov 30 09:22:32 2017 +0800 hello3commit 9b7651337815b3697283c38a2cde43a17edf622bAuthor: yangfan &lt;hyyangfan@gmail.com&gt;Date: Thu Nov 30 09:19:01 2017 +0800 hello2 执行命令后，出现了冲突 1234567891011121314➜ git_cherry_pick git:(master) git cherry-pick 0318ff6fd8b6f76error: could not apply 0318ff6... hello4hint: after resolving the conflicts, mark the corrected pathshint: with 'git add &lt;paths&gt;' or 'git rm &lt;paths&gt;'hint: and commit the result with 'git commit'➜ git_cherry_pick git:(master) ✗ cat test.txt hellohello1&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD=======hello2hello3hello4&gt;&gt;&gt;&gt;&gt;&gt;&gt; 0318ff6... hello4 我们把冲突处理掉就可以了。 rebaserebase我们可以翻译成变基或者是衍合，它的功能是类似于merge的。不同的是merge在合并后是两条线合并，rebase是把另外一个分支的新的提交直接嫁接过来，我觉得有点像cherry-pick。 rebase过程中也会出现冲突，解决冲突后，使用git add添加，然后执行git rebase --continue 接下来Git会继续应用余下的补丁 任何时候都可以通过如下命令终止rebase，分支会恢复到rebase开始前的状态。git rebase --abort 不要对master分支执行reabase，否则会引起很多问题 一般来说，执行reabse的分支都是自己的本地分支，没有推送到远程版本库。 因为rebase会修改分支的提交历史，所以不要在与其他人共享的分支上执行这个命令，否则会造成非常麻烦的结果。 实战要演示rebase的命令，首先遵从最佳实践，不要在master上做操作，所以创建两个分支，分别进行两次提交。 develop分支： 123456789101112131415161718commit f1e1d6ab0397578a06100772e6e89fffa6f274dc (HEAD -&gt; develop)Author: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:21:57 2017 +0800 add develop1commit 88e11d005f5fd66157d165e99be8aca2d6078fffAuthor: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:21:39 2017 +0800 add worldcommit 31dec318f7cf42e0289228535a89efe0b6c5bb8e (test, master)Author: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:20:26 2017 +0800 initial commit(END) test分支： 1234567891011121314151617commit ca7495957b750285fb6ad32bf0aaff2d3d89b44f (HEAD -&gt; test)Author: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:23:43 2017 +0800 add test2commit 6ca36bae9ff45bfd0df6a5f05b26c08d4ac94a52Author: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:23:32 2017 +0800 add test1commit 31dec318f7cf42e0289228535a89efe0b6c5bb8e (master)Author: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:20:26 2017 +0800 initial commit 这两个分支都有共同的祖先，也就是31dec318f7cf42e028922这次提交。 下图显示了merge和rebase操作对分支历史的不同区别。 因为我们对文件修改的不一样，那么一定会产生冲突。 123456789101112131415➜ git_rebase git:(test) git rebase develop First, rewinding head to replay your work on top of it...Applying: add test1Using index info to reconstruct a base tree...M test.txtFalling back to patching base and 3-way merge...Auto-merging test.txtCONFLICT (content): Merge conflict in test.txterror: Failed to merge in the changes.Patch failed at 0001 add test1The copy of the patch that failed is found in: .git/rebase-apply/patchWhen you have resolved this problem, run "git rebase --continue".If you prefer to skip this patch, run "git rebase --skip" instead.To check out the original branch and stop rebasing, run "git rebase --abort". 如果执行git rebase --skip，就会丢弃掉test分支上提交的修改，直接使用develop上的内容。 1234567891011121314➜ git_rebase git:(f1e1d6a) ✗ git rebase --skipApplying: add test2Using index info to reconstruct a base tree...M test.txtFalling back to patching base and 3-way merge...Auto-merging test.txtCONFLICT (content): Merge conflict in test.txterror: Failed to merge in the changes.Patch failed at 0002 add test2The copy of the patch that failed is found in: .git/rebase-apply/patchWhen you have resolved this problem, run "git rebase --continue".If you prefer to skip this patch, run "git rebase --skip" instead.To check out the original branch and stop rebasing, run "git rebase --abort". 因为有2次提交，所以忽略第一次后，还有第二次的内容 12345678910➜ git_rebase git:(f1e1d6a) ✗ cat test.txt hello&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADworlddevelop1=======test1test2&gt;&gt;&gt;&gt;&gt;&gt;&gt; add test2 现在把冲突解决一下，手工编辑一下文件，保留下自己想保留的内容。 1234➜ git_rebase git:(f1e1d6a) ✗ git add .➜ git_rebase git:(f1e1d6a) ✗ git rebase --continueApplying: add test2➜ git_rebase git:(test) 12345678910111213141516171819202122232425➜ git_rebase git:(test) git logcommit 1b18ea5189f30c5a725b586297a747e46572962a (HEAD -&gt; test)Author: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:23:43 2017 +0800 add test2commit f1e1d6ab0397578a06100772e6e89fffa6f274dc (develop)Author: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:21:57 2017 +0800 add develop1commit 88e11d005f5fd66157d165e99be8aca2d6078fffAuthor: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:21:39 2017 +0800 add worldcommit 31dec318f7cf42e0289228535a89efe0b6c5bb8e (master)Author: 张三 &lt;zhangsan@git.com&gt;Date: Tue Dec 5 09:20:26 2017 +0800 initial commit 因为丢弃了一次test的提交，所以很明显的看到，master第一次提交后，是develop的两次提交，然后是add test2的一次提交，test分支变基操作执行完毕。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-裸库和submodule、substree]]></title>
    <url>%2F2017%2F11%2F21%2Fgit6%2F</url>
    <content type="text"><![CDATA[裸库通常我们在本地初始化仓库的时候，都是用git init。如果我们不借助github，如何自己建立一个中心仓库呢，这个就是裸库的概念。 git init --bare &lt;repo&gt; 这个命令执行后，将在本地创建一个名为 repo 的文件夹， 里面包含着 Git 的基本目录， 我们一般会将这个文件夹命名为后面加 .git 的形式，如 repo.git （这也是为什么我们从 GitHub clone 仓库的时候，地址都是 xxx.git 这样的形式的原因）.使用 –bare 参数初始化的仓库，我们一般称之为裸仓库， 因为这样创建的仓库并不包含 工作区 ， 也就是说，我们并不能在这个目录下执行我们一般使用的 Git 命令。 submodule有种情况我们经常会遇到：某个工作中的项目需要包含并使用另一个项目。 也许是第三方库，或者你独立开发的，用于多个父项目的库。 现在问题来了：你想要把它们当做两个独立的项目，同时又想在一个项目中使用另一个。 一般在Java中，我们会打包成jar包放入nexus中，可是如果是频繁更新的库，这样做会比较麻烦。那么如果我们将代码直接包含在项目中呢，这里就可以使用submodule。Git 通过子模块来解决这个问题。 子模块允许你将一个 Git 仓库作为另一个 Git 仓库的子目录。 它能让你将另一个仓库克隆到自己的项目中，同时还保持提交的独立。 先在github上创建2个仓库： https://github.com/sail-y/git_parent.githttps://github.com/sail-y/git\_child.git 在本地也分别创建2个仓库并与之关联。 那么我们如何在一个仓库中包含另外一个仓库呢？ 命令如下： 123456➜ git_parent git:(master) git submodule add git@github.com:sail-y/git_child.git mymoduleCloning into '/Users/xiaomai/code/zhanglong/git/git_parent/mymodule'...remote: Counting objects: 6, done.remote: Compressing objects: 100% (3/3), done.remote: Total 6 (delta 0), reused 6 (delta 0), pack-reused 0Receiving objects: 100% (6/6), done. 我们在git_parent仓库中执行这个命令，表示将git_child作为submodule拉取到mymodule目录中，注意这里mymodule是不能事先存在的，否则会报错。 12345678➜ git_parent git:(master) ✗ git stOn branch masterYour branch is up-to-date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: .gitmodules new file: mymodule 多出来一个文件和一个目录，目录里面就包含了另一个仓库里的文件 123➜ git_parent git:(master) ✗ cd mymodule ➜ mymodule git:(master) lshello.txt submodule.txt 如果子仓库更新了，只需要在mymodule中执行git pull就可以拉取变更。 123456789101112➜ git_parent git:(master) cd mymodule ➜ mymodule git:(master) git pullremote: Counting objects: 3, done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:sail-y/git_child 5fc54e6..5adf3e0 master -&gt; origin/masterUpdating 5fc54e6..5adf3e0Fast-forward hello.txt | 1 + 1 file changed, 1 insertion(+) 如果包含的submodule比较多的话，也可以使用命令将仓库包含的所有submodule都进行更新。 12345678910111213➜ git_parent git:(master) ✗ git submodule foreach git pullEntering 'mymodule'remote: Counting objects: 3, done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:sail-y/git_child 5adf3e0..19a2793 master -&gt; origin/masterUpdating 5adf3e0..19a2793Fast-forward welcome.txt | 1 + 1 file changed, 1 insertion(+) create mode 100644 welcome.txt 当子模块发生变更的时候，父模块也会检测到，需要重新提交。 12345678910111213141516171819202122➜ git_parent git:(master) ✗ git stOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: mymodule (new commits)no changes added to commit (use "git add" and/or "git commit -a")➜ git_parent git:(master) ✗ git add .➜ git_parent git:(master) ✗ git cm 'update submodule' [master a5cec07] update submodule 1 file changed, 1 insertion(+), 1 deletion(-)➜ git_parent git:(master) git pushCounting objects: 2, done.Delta compression using up to 4 threads.Compressing objects: 100% (2/2), done.Writing objects: 100% (2/2), 311 bytes | 0 bytes/s, done.Total 2 (delta 0), reused 0 (delta 0)To github.com:sail-y/git_parent.git d0bc606..a5cec07 master -&gt; master 拉取submodule如果我们拉取了一个新的项目，它包含了submodule，直接clone仓库的时候，submodule里面是没有内容的。 123456789➜ git git clone git@github.com:sail-y/git_parent.git git_parent2Cloning into 'git_parent2'...remote: Counting objects: 8, done.remote: Compressing objects: 100% (6/6), done.remote: Total 8 (delta 0), reused 8 (delta 0), pack-reused 0Receiving objects: 100% (8/8), done.➜ git cd git_parent2/mymodule ➜ mymodule git:(master) ls➜ mymodule git:(master) 我们需要执行2个命令来更新submodule的内容。 12345➜ mymodule git:(master) git submodule initSubmodule 'mymodule' (git@github.com:sail-y/git_child.git) registered for path './'➜ mymodule git:(master) git submodule update --recursive Cloning into '/Users/xiaomai/code/zhanglong/git/git_parent2/mymodule'...Submodule path './': checked out '19a2793acc723c501f7422a9fdd810e46a528381' 这样做有3步操作，稍微麻烦了一点，实际上clone有一个参数可以一键完成。 1234567891011121314➜ git git clone git@github.com:sail-y/git_parent.git git_parent3 --recursive Cloning into 'git_parent3'...remote: Counting objects: 8, done.remote: Compressing objects: 100% (6/6), done.Receiving objects: 100% (8/8), done.remote: Total 8 (delta 0), reused 8 (delta 0), pack-reused 0Submodule 'mymodule' (git@github.com:sail-y/git_child.git) registered for path 'mymodule'Cloning into '/Users/xiaomai/code/zhanglong/git/git_parent3/mymodule'...remote: Counting objects: 12, done. remote: Compressing objects: 100% (7/7), done. remote: Total 12 (delta 1), reused 11 (delta 0), pack-reused 0 Receiving objects: 100% (12/12), done.Resolving deltas: 100% (1/1), done.Submodule path 'mymodule': checked out '19a2793acc723c501f7422a9fdd810e46a528381' subtree因为submodule在使用过程存在一些弊端，比如在父工程中修改了被依赖的项目的代码（因为代码就在项目中），然后推送后，在子模块本身的项目再拉取，实际上这样是有很多问题的。所以就出现了subtree这么一个开源功能，被git团队纳入了git中，它更加优秀，使用起来也很简单。 官方也是建议用substree来替代submodule。 下面开始演示，在github新建两个仓库。 https://github.com/sail-y/git\_substree\_parent.git https://github.com/sail-y/git\_substree\_child.git 准备工作需要在父项目里先添加一个远程 1234➜ git_substree_parent git:(master) git remote add subtree-origin git@github.com:sail-y/git_substree_child.git➜ git_substree_parent git:(master) git remote show originsubtree-origin 然后再添加subtree 1234567891011➜ git_substree_parent git:(master) git subtree add --prefix=substree subtree-origin master --squashgit fetch subtree-origin masterwarning: no common commitsremote: Counting objects: 6, done.remote: Compressing objects: 100% (3/3), done.remote: Total 6 (delta 0), reused 6 (delta 0), pack-reused 0Unpacking objects: 100% (6/6), done.From github.com:sail-y/git_substree_child * branch master -&gt; FETCH_HEAD * [new branch] master -&gt; subtree-origin/masterAdded dir 'substree' git subtree add这个命令将subtree-origin的master分支拉取到了substree这个目录下。 12➜ git_substree_parent git:(master) lsparent.txt substree 实际上这个命令会将child的commit合并过来，并将作者改了。 12345678910111213141516171819202122commit 8f08b4edbb537806f4a34feb8f773b97aa898d10 (HEAD -&gt; master, origin/master)Merge: 3c75631 5090ac2Author: 张三 &lt;zhangsan@git.com&gt;Date: Thu Nov 23 09:16:59 2017 +0800 Merge commit '5090ac20e7c71524697e762276f924343c0bed54' as 'substree'commit 5090ac20e7c71524697e762276f924343c0bed54Author: 张三 &lt;zhangsan@git.com&gt;Date: Thu Nov 23 09:16:59 2017 +0800 Squashed 'substree/' content from commit 1112351 git-subtree-dir: substree git-subtree-split: 1112351e504917e6fed1c6decee6bb81e931d647commit 3c75631cda4535d854e9ff7629aab2c9afcdebe6Author: 张三 &lt;zhangsan@git.com&gt;Date: Thu Nov 23 09:01:28 2017 +0800 initial commit(END) 它与submodule最显著的区别就是，submodule是保存的对子模块的一个引用，一个指针，而subtree是直接保存的文件。 在子模块新增一次提交，添加一个world.txt文件。 在父模块更新的命令是：git subtree pull --prefix=substree subtree-origin master --squash 123456789101112➜ git_substree_parent git:(master) git subtree pull --prefix=substree subtree-origin master --squashremote: Counting objects: 3, done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:sail-y/git_substree_child * branch master -&gt; FETCH_HEAD 1112351..844d9fd master -&gt; subtree-origin/masterMerge made by the 'recursive' strategy. substree/world.txt | 1 + 1 file changed, 1 insertion(+) create mode 100644 substree/world.txt 看一下git log 12345678910111213141516171819➜ git_substree_parent git:(master) git logcommit d7fbb7665c6d1af3cbdd4f5cf1977d682d6aa054 (HEAD -&gt; master)Merge: 8f08b4e 64dcef5Author: 张三 &lt;zhangsan@git.com&gt;Date: Thu Nov 23 09:29:03 2017 +0800 Merge commit '64dcef57bc7eea0fda072d53628b9464040a6072'commit 64dcef57bc7eea0fda072d53628b9464040a6072Author: 张三 &lt;zhangsan@git.com&gt;Date: Thu Nov 23 09:29:03 2017 +0800 Squashed 'substree/' changes from 1112351..844d9fd 844d9fd add world git-subtree-dir: substree git-subtree-split: 844d9fdb772f8e2c57218176b40b064f618523a4 并没有出现李四的提交，844d9fd add world，被修改成了张三。 再child里再提交一个hellworld.txt文件，然后更新。 123456789➜ git_substree_parent git:(master) git subtree pull --prefix=substree subtree-origin master remote: Counting objects: 3, done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:sail-y/git_substree_child * branch master -&gt; FETCH_HEAD 844d9fd..a940bda master -&gt; subtree-origin/masterfatal: refusing to merge unrelated histories 注意这次没有–squash参数 1234567➜ git_substree_parent git:(master) git logcommit a940bdaf1c8dd5048782c7179c62d93b1bc03d80 (subtree-origin/master)Author: 李四 &lt;lisi@git.com&gt;Date: Thu Nov 23 09:31:47 2017 +0800 add helloworld 这次就出现了李四的提交信息，–squash会将要合并的分支所有的提交全部合并成一个提交信息。 接下来我们在parent的项目里修改child.txt，并push。 12➜ git_substree_parent git:(master) cd subtree ➜ subtree git:(master) vi child.txt 可以看到github上parent项目中的child.txt是有新增的内容的，但是child项目里面却没有。 实际上在父项目中，还需要单独执行一次push。 123456789➜ git_substree_parent git:(master) git subtree push --prefix=subtree subtree-origin mastergit push using: subtree-origin masterCounting objects: 3, done.Delta compression using up to 4 threads.Compressing objects: 100% (2/2), done.Writing objects: 100% (3/3), 347 bytes | 0 bytes/s, done.Total 3 (delta 0), reused 0 (delta 0)To github.com:sail-y/git_substree_child.git a940bda..0c887fa 0c887fa4eb23fe282551d1368e72d31a177182e6 -&gt; master –squash--squash会将subtree的所有提交信息合并成parent的一个提交，然后会将这一次合并的提交和parent再次合并，那么就产生了2次合并。--squash是为了防止主仓库的提交历史被污染，但是它的使用也有一些问题。 所以在使用--squash的时候，我们需要保证要么一直使用，要么一直不使用。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-源码分析(三）]]></title>
    <url>%2F2017%2F11%2F19%2Fnetty8%2F</url>
    <content type="text"><![CDATA[源码分析(三）前面我们已经了解了Netty的启动流程，以及各个组件之间的关系，接下来要看看Netty的数据容器（ByteBuf)。 回顾Java NIO先回顾一下Java NIO的Buffer使用方式。 使用NIO进行文件读取所涉及的步骤： 从FileInputStream对象获取到Channel对象。 创建Buffer. 将数据从Channel中读取到Buffer对象中。 0 &lt;= mark &lt;= postion &lt;= limit &lt;= capacity flip()方法切换读和写的状态： 将limit值设为当前的position。 将position设为0. clear()方法，改变属性值，并没有删除数组里面的数据 ： 将limit值设置成capacity。 将position值设为0。 compact()方法： 将所有未读的数据复制到buffer起始位置处。 将position设为最后一个未读元素的后面。 将limit设为capacity。 现在buffer准备好了，但是不会覆盖未读的数据。 ByteBufByteBuf的使用很简单，一般不建议使用构造方法创建，用非池化的Buffer即可。 1234567891011public static void main(String[] args) &#123; ByteBuf buffer = Unpooled.buffer(10); for (int i = 0; i &lt; 10; i++) &#123; buffer.writeByte(i); &#125; for (int i = 0; i &lt; buffer.capacity(); i++) &#123; System.out.println(buffer.getByte(i)); &#125;&#125; 上面的代码就包含了创建ByteBuf，写入数据和读取数据。 在ByteBuf中，Netty提供了一个readerIndex和writerIndex，这样避免了调用flip()方法，只有read开头的方法或和write方法开头的方法才会改变这2个变量的值。get和set方法是不会改变索引的，我们可以通过readerIndex()和writerIndex()来修改。 下面图中前面的部分表示已被读取过的数据，是可以丢弃的，中间是可读的数据，最后是可写的数据。 123456+-------------------+------------------+------------------+| discardable bytes | readable bytes | writable bytes || | (CONTENT) | |+-------------------+------------------+------------------+| | | |0 &lt;= readerIndex &lt;= writerIndex &lt;= capacity 3种缓冲区类型Netty ByteBuf所提供的3种缓冲区类型： heap buffer (堆缓冲区) direct buffer （直接缓冲区） composite buffer（复合缓冲区） 堆上的ByteBuf，这是最常用的类型，ByteBuf将数据存储到JVM的堆空间中， 并且将实际的数据存放到byte array中来实现。 优点：由于数据是存储在JVM的堆红，因此可以快速的创建与快速的释放，并且它提供了直接访问内部字节数组的方法。 缺点：每次读写数据时，都需要先将数据复制到直接缓冲区中再进行网络传输。 123456789101112131415public class ByteBufTest1 &#123; public static void main(String[] args) &#123; ByteBuf byteBuf = Unpooled.copiedBuffer("hello world", Charset.forName("utf-8")); // 表示是堆上的缓冲，是用字节数组存放的 if (byteBuf.hasArray()) &#123; byte[] content = byteBuf.array(); System.out.println(new String(content, Charset.forName("utf-8"))); System.out.println(byteBuf); &#125; &#125;&#125; DirectBuffer（直接缓冲区） 在堆之外直接分配内存空间，直接缓冲区并不会占用堆的容量空间，因为它是由操作系统在本地内存进行的数据分配。 优点：在使用Socket进行数据传递时，性能非常好，因为数据直接位于操作系统的本地内存中，所以不需要从JVM将数据复制到直接缓冲区中，性能很好。 缺点：因为Direct Buffer是直接在操作系统内存中的，所以内存空间的分配与释放要比堆空间更加复杂，而且速度要慢一些。Netty通过提供内存池来解决这个问题。直接缓冲区并不支持通过字节数组方式来访问数据。 重点：对于后端的业务消息的编解码来说，推荐使用HeapByteBuf；对于I/O通信线程在读写缓冲区时，推荐使用DirectByteBuf。 Composite Buffer（复合缓冲区） 复合缓冲区为多个ByteBuf提供一个聚合视图。在这里你可以根据需要添加或者删除ByteBuf实例， 是一个JDK的ByteBuffer实全缺失的特性。 123456789101112131415161718192021public class ByteBufTest2 &#123; public static void main(String[] args) &#123; CompositeByteBuf compositeByteBuf = Unpooled.compositeBuffer(); ByteBuf heapBuf = Unpooled.buffer(10); ByteBuf directBuf = Unpooled.directBuffer(8); compositeByteBuf.addComponents(heapBuf, directBuf);// compositeByteBuf.removeComponent(0); Iterator&lt;ByteBuf&gt; iterator = compositeByteBuf.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; compositeByteBuf.forEach(System.out::println); &#125;&#125; JDK的ByteBuffer与Netty的ByteBuf之间的差异比对 Netty的ByteBuf采用了读写索引分离的策略（readerIndex与writerIndex），一个初始化（里面尚未有任何数据）的ByteBuf的readerIndex和writerIndex值都为0。 当读索引与写索引处于同一个位置时，如果我们继续读取，那么就会抛出常见的IndexOutOfBoundsException。 对于ByteBuf的任何读写操作都会分别单独维护读索引和写索引。maxCapacity最大容量默认的限制就是Integer.MAX_VALUE。 JDK的ByteBuffer的缺点： final byte[] hb;这是JDK的ByteBuffer对中用于存储数据的对象声明；可以看到其数据是被声明为final的，也就是长度是固定不变的。一旦分配好就不能动态扩容与收缩；而且当待存储的数据字节很大时就很有可能出现IndexOutOfBoundsException。如果要预防这个异常，那就需要在存储之前完全确定好待存储的字节大小。如果ByteBuffer的空间不足，我们只有一种解决方案：创建一个全新的ByteBuffer对象，然后再将之前的ByteBuffer的数据复制过去，这一切的操作都需要由开发者自己来手动完成。 ByteBuffer只使用一个position指针来标识位置信息，在进行读写切换时就需要调用flip方法或是rewind方法，使用起来很不方便。 Netty的ByteBuf的优点： 存储字节是动态的，其最大值默认是Integer.MAX_VALUE。这里的动态性是体现在write方法中的，write方法在执行时会判断buffer容量，如果不足则自动扩容。 ByteBuf的读写索引是完全分开的，使用起来就很方便。 在写入的时候，ensureWritable0()方法会检查容量大小来决定是否需要扩容。 12345public ByteBuf writeByte(int value) &#123; ensureWritable0(1); _setByte(writerIndex++, value); return this;&#125; clear()clear()方法只会重置readerIndex和writerIndex()，不会产生数据的移动。 discardReadBytes()通过discardReadBytes()方法，可以丢弃已经读取过的字节，并回收它们的空间。 引用计数引用计数是一种通过在某个对象所持有的资源不再被其他对象引用时释放该对象所持有的资源来优化内存使用和性能的技术。Netty在第4版为ByteBuf和ByteBufHolder引入了引用技术技术，它们都实现了interface ReferenceCounted。 ReferenceCounted包含了获取引用数量（refCnt），增加计数（retain），减少计数（realse)等API，用过OC的都应该比较熟悉。 AbstractReferenceCountedByteBuf的retain0()方法是一个死循环，这段代码确保了ByteBuf的引用计数变为0的时候不会再次被使用。里面用了CAS来确保refCnt是原子的修改，没有并发问题。 AtomicIntegerFieldUpdater VS AtomicIntegerAbstractReferenceCountedByteBuf维护了一个AtomicIntegerFieldUpdater来修改volatile修饰的refCnt，那么我们都知道JDK5提供了一个AtomicInteger来对int进行原子修改，为什么Netty不用我们更熟悉的AtomicInteger呢？ AtomicIntegerFieldUpdater要点总结： 更新器更新的必须是int类型变量，不能是其包装类型。 更新器更新的必须是volatile类型变量，确保线程之间共享变量值的立即可见性。 变量不能是static的，必须是实例变量。因为Unsafe.objectFieldOffset()方法不支持静态变量（CAS操作本质上是通过对象实例的偏移量来直接进行赋值）。 更新器只能修改它课件范围内的变量，因为更新器是通过反射来得到这个变量，如果变量不可见就会报错。 如果要更新的变量是包装类型，我们可以使用AtomicReferenceFieldUpdater来进行更新。 12345if (fieldt != int.class) throw new IllegalArgumentException("Must be integer type");if (!Modifier.isVolatile(modifiers)) throw new IllegalArgumentException("Must be volatile type"); 为什么不用AtomicInteger这是Netty为了性能上的考虑，因为使用AtomicInteger，创建多个ByteBuf也会随之创建多个AtomicInteger对象，但是AtomicIntegerFieldUpdater是static修饰的，只有一个对象，由此可见Netty对性能的优化也是考虑到了极致。 数据 -&gt; ByteBufNetty是在AbstractNioChannel的NioByteUnsafe内部类的read()方法将入站数据转换成ByteBuf对象的。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-远程仓库2]]></title>
    <url>%2F2017%2F11%2F12%2Fgit5%2F</url>
    <content type="text"><![CDATA[refspecReference Specification简称refspec。在执行push或fetch操作时，refspec用以给出本地Ref和远程Ref之间的映射关系，通常是本地分支或本地tag与远程库中的分支或tag之间的映射关系。 refspec格式：+&lt;src_ref&gt;:&lt;dst_refs&gt;其中的+是可选的，表示强制更新典型的push refspec为HEAD:refs/heads/master典型的fetch refspec为refs/heads/:refs/remotes/origin/ 在缺省情况下，refspec会被git remote add命令所自动生成，Git会获取远端上refs/heads下的所有引用，并将它们写到本地的refs/remotes/origin目录下，这些信息我们都可以在.git/config文件里面可以看到。所以，如果远端上有一个master分支，你在本地就可以通过下面几种方式来访问它们的历史记录： 123➜ mygit2 git:(develop) git log origin/master➜ mygit2 git:(develop) git log remotes/origin/master➜ mygit2 git:(develop) git log refs/remotes/origin/master 推送新的分支表示本地分支和远程分支的关联，我们在创建好分支以后直接执行push是无法推送的，因为git并不知道你要推送到远程的哪一个分支上。 12345678910111213➜ mygit git:(master) git co -b developSwitched to a new branch 'develop'➜ mygit git:(develop) git br test➜ mygit git:(develop) git br -av* develop 93cd003 git gui test master 93cd003 git gui test test 93cd003 git gui test remotes/origin/master 93cd003 git gui test➜ mygit git:(develop) git pushfatal: The current branch develop has no upstream branch.To push the current branch and set the remote as upstream, use git push --set-upstream origin develop git push --set-upstream origin develop 就表示推送到远程，并在远程创建一个叫develop的分支，和本地develop分支关联。 12345➜ mygit git:(develop) git push --set-upstream origin developTotal 0 (delta 0), reused 0 (delta 0)To github.com:sail-y/git_demo.git * [new branch] develop -&gt; developBranch develop set up to track remote branch develop from origin. 这个时候如果我们在另外一台机器想拉取这个远程分支怎么办呢？ 123456789➜ mygit2 git:(master) git checkout -b develop origin/develop Branch develop set up to track remote branch develop from origin.Switched to a new branch 'develop'➜ mygit2 git:(develop) git br -av * develop 93cd003 git gui test master f7b8c54 [ahead 2] Merge branch 'master' of github.com:sail-y/git_demo remotes/origin/HEAD -&gt; origin/master remotes/origin/develop 93cd003 git gui test remotes/origin/master 93cd003 git gui test 只需在checkout的时候最后面跟上远程分支的名称origin/develop。 另一种推送分支的方式1234567➜ mygit2 git:(develop) git co testSwitched to branch 'test'➜ mygit2 git:(test) git push -u origin testTotal 0 (delta 0), reused 0 (delta 0)To github.com:sail-y/git_demo.git * [new branch] test -&gt; testBranch test set up to track remote branch test from origin. 注意git push -u和git push --set-upstream的作用，是一样的。 12345678910111213141516171819➜ mygit git:(develop) git fetchFrom github.com:sail-y/git_demo * [new branch] test -&gt; origin/test➜ mygit git:(develop) git br -av * develop 93cd003 git gui test master 93cd003 git gui test remotes/origin/develop 93cd003 git gui test remotes/origin/master 93cd003 git gui test remotes/origin/test 93cd003 git gui test➜ mygit git:(develop) git checkout --track origin/testBranch test set up to track remote branch test from origin.Switched to a new branch 'test'➜ mygit git:(test) git br -av develop 93cd003 git gui test master 93cd003 git gui test* test 93cd003 git gui test remotes/origin/develop 93cd003 git gui test remotes/origin/master 93cd003 git gui test remotes/origin/test 93cd003 git gui test 这里用另外一种方式拉取远程分支，git checkout --track origin/test实际上就是git checkout -b test origin/test的一种特殊简写，为什么说特殊呢，因为它并没有指定本地分支的名称，而是直接使用了远程分支的名称。 如何删除远程分支第一种方式git push的完整写法：git push origin src:dest 将本地的分支推送到远程分支，名称可以不一样，那么删除远程分支，就是传递一个空分支到远程。 123➜ mygit git:(develop) git push origin :developTo github.com:sail-y/git_demo.git - [deleted] develop 第二种方式123➜ mygit git:(develop) git push --delete origin developTo github.com:sail-y/git_demo.git - [deleted] develop 推送的时候可以设置远程分支不同的名字： 12345➜ mygit git:(develop) git push --set-upstream origin develop:develop2Total 0 (delta 0), reused 0 (delta 0)To github.com:sail-y/git_demo.git * [new branch] develop -&gt; develop2Branch develop set up to track remote branch develop2 from origin. 详解 push操作的完整命令是：git push origin srcBrnach:destBranch pull操作的完整命令是：git poll origin srcBranch:destBranch HEAD标记：HEAD文件是一个指向你当前所在分支的引用标识符，该文件内部并不包含SHA-1值，而是一个指向另外一个引用的指针。(可以在.git/HEAD中查看) 12345➜ mygit git:(develop) git co testSwitched to branch 'test'Your branch is up-to-date with 'origin/test'.➜ mygit git:(test) cat .git/HEADref: refs/heads/test 当执行git commit命令时，git会创建一个commit对象，并且将这个commit对象的parent指针设置为HEAD所指向的引用的SHA-1值。 我们对于HEAD修改的任何操作，都会被git reflog完整记录下来。 实际上我们可以通过git底层命令symbolic-ref来实现对HEAD文件内容的修改。 1234➜ mygit2 git:(develop) git symbolic-ref HEAD refs/heads/develop➜ mygit2 git:(develop) git symbolic-ref HEAD refs/heads/test ➜ mygit2 git:(test) 推送标签先回顾一下创建标签的知识。 git tag v1.0创建了一个轻量级标签，只包含了对commit的引用。 git tag -a v2.0 -m &#39;v2.0 released&#39;创建了一个重量级标签，他不光包含了引用，还包含了自身的注释信息。可以用git show v2.0查看。 12345tag v2.0Tagger: 七七 &lt;qiqi@gmail.com&gt;Date: Sun Nov 12 20:47:55 2017 +0800v2.0 released 推送标签也是用push命令： 123456➜ mygit git:(test) git push origin v2.0Counting objects: 1, done.Writing objects: 100% (1/1), 160 bytes | 0 bytes/s, done.Total 1 (delta 0), reused 0 (delta 0)To github.com:sail-y/git_demo.git * [new tag] v2.0 -&gt; v2.0 如果标签比较多的话，可以一次性推送所有标签。 1234567➜ mygit git:(test) git push origin --tagsCounting objects: 1, done.Writing objects: 100% (1/1), 159 bytes | 0 bytes/s, done.Total 1 (delta 0), reused 0 (delta 0)To github.com:sail-y/git_demo.git * [new tag] v1.0 -&gt; v1.0 * [new tag] v3.0 -&gt; v3.0 完整命令git push origin refs/tags/v4.0:refs/tags/v4.0 拉取标签git fetch origin tag v4.0 删除远程标签跟删除分支是类似的，是将一个空的标签推送到远程。 123➜ mygit git:(develop) git push origin :refs/tags/v3.0To github.com:sail-y/git_demo.git - [deleted] v3.0 另一种方式 123➜ mygit git:(test) git push origin --delete tag v2.0To github.com:sail-y/git_demo.git - [deleted] v2.0 git prune当我们有一边删除了远程分支后： 123➜ mygit git:(test) git push --delete origin developTo github.com:sail-y/git_demo.git - [deleted] develop 如何在另外一个仓库同步这个信息呢，先查看远程分支的状态。 12345678910111213141516➜ mygit2 git:(develop) git remote show origin* remote origin Fetch URL: git@github.com:sail-y/git_demo.git Push URL: git@github.com:sail-y/git_demo.git HEAD branch: master Remote branches: master tracked refs/remotes/origin/develop stale (use 'git remote prune' to remove) test tracked Local branches configured for 'git pull': develop merges with remote develop master merges with remote master test merges with remote test Local refs configured for 'git push': master pushes to master (fast-forwardable) test pushes to test (up to date) 注意这里是另外一个用户，这里git给了提示：refs/remotes/origin/develop stale (use ‘git remote prune’ to remove) 如果我们pull，会得到一个错误，因为远程的develop已经被删除了。 12➜ mygit2 git:(develop) git pull origin develop:developfatal: Couldn't find remote ref develop 我们试试git remote prune的命令，本地和远程分支develop之间的关联，就已经没有了。 1234➜ mygit2 git:(develop) git remote prune originPruning originURL: git@github.com:sail-y/git_demo.git * [pruned] origin/develop]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-远程仓库]]></title>
    <url>%2F2017%2F09%2F29%2Fgit4%2F</url>
    <content type="text"><![CDATA[Git远程仓库Git本身是一个分布式的版本控制系统，每个用户的机器上都会有一个完整的版本库。之前我们的操作都是在本地机器上操作的，最终别人需要获取我们的修改，需要我们将本地的修改推送(push)到远程机器上，然后别人再拉取(pull)合并我们的代码。 说到远程仓库呢，就不得不提一下Github，全球最大的同性交友社区（😄开个玩笑）。 https://github.com/ Github是免费使用的条件就是你在上面创建的代码都是开源的，开源被查看的，如果要想创建私有的仓库，就是需要收费的。那么应运而生的产生了一个叫Gitlab的开源产品可以让我们在公司的服务器搭建私有的仓库，让我们可以享受到免费的，私有的，内网的服务。 先注册一个Github的帐号吧，欢迎大家关注我https://github.com/sail-y。 接下来就是要把本地的仓库要推送到远程，Github有一个叫README.md的文件用来描述项目，它是采用MarkDown语法编写的，跟我这篇文章的编写方式是一样的，可以去了解一下，很方便。 现在新建一个仓库，创建几个文件多进行几次提交 1234567891011121314151617181920212223242526272829303132333435➜ mygit git:(master) lsREADME.md test.txt➜ mygit git:(master) cat test.txt hello worldadd second lineadd a thrid line➜ mygit git:(master) cat README.md ## Git示例项目### 开发者* 七七* 灰灰➜ mygit git:(master) git logcommit ec41fdfefd806965a088cd160d1fe755c263c5a9 (HEAD -&gt; master)Author: 七七 &lt;qiqi@gmail.com&gt;Date: Fri Sep 29 16:07:15 2017 +0800 add readmecommit ef71fd488241a5276e89dce67681ae923072202eAuthor: 七七 &lt;qiqi@gmail.com&gt;Date: Fri Sep 29 16:04:36 2017 +0800 third commitcommit fee1876da354c9bb45d317ae3a9d00d8ffb50fa0Author: 七七 &lt;qiqi@gmail.com&gt;Date: Fri Sep 29 16:04:08 2017 +0800 second commitcommit e54c3751489948d4c0e3d81d71c9e5c2f02cf11aAuthor: 七七 &lt;qiqi@gmail.com&gt;Date: Fri Sep 29 16:02:04 2017 +0800 接着我们在Github上创建一个远程仓库 创建完成后Github就提示我们可以推送一个现有的仓库 12git remote add origin git@github.com:sail-y/git_demo.gitgit push -u origin master 执行完命令后，发现Github上已经出现了我们推送的仓库内容：https://github.com/sail-y/git_demo 第一次关联远程仓库并推送后，以后只需要执行git push就可以将当前分支推送到远程仓库了。 123456789101112➜ mygit git:(master) vi test.txt ➜ mygit git:(master) ✗ git commit -am 'forth commit' [master c3b0d8b] forth commit 1 file changed, 1 insertion(+)➜ mygit git:(master) git pushCounting objects: 3, done.Delta compression using up to 4 threads.Compressing objects: 100% (3/3), done.Writing objects: 100% (3/3), 316 bytes | 0 bytes/s, done.Total 3 (delta 0), reused 0 (delta 0)To github.com:sail-y/git_demo.git ec41fdf..c3b0d8b master -&gt; master 查看远程仓库前面我们在github上创建了一个远程仓库，并且跟本地仓库进行了关联，实际上git本地仓库是可以关联很多个远程仓库的。 可以用git remote show查看所有的远程仓库。 12➜ mygit git:(master) git remote showorigin 还可以查看详细信息 1234567891011➜ mygit git:(master) git remote show origin* remote origin Fetch URL: git@github.com:sail-y/git_demo.git Push URL: git@github.com:sail-y/git_demo.git HEAD branch: master Remote branch: master tracked Local branch configured for 'git pull': master merges with remote master Local ref configured for 'git push': master pushes to master (up to date) 上面命令输出表示当前fetch（拉取）的地址是**git@github.com:sail-y/git_demo.git，推送的地址是git@github.com:sail-y/git_demo.git**，当前分支是master，远程关联的分支是master。git pull默认拉取的是master分支，git push默认推送的是master分支，并且已经是最新的了。 123➜ mygit git:(master) ✗ git branch -av* master 460b082 add fifth line remotes/origin/master 460b082 add fifth line 加上-a参数可以看到远程分支。 1234567891011➜ mygit git:(master) ✗ git commit -am 'update test.txt'[master d04325a] update test.txt 1 file changed, 1 insertion(+)➜ mygit git:(master) git statusOn branch masterYour branch is ahead of 'origin/master' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree clean➜ mygit git:(master) git branch -av* master d04325a [ahead 1] update test.txt remotes/origin/master 460b082 add fifth line 接着进行一次提交，git status提示本地的分支是超前了远程master分支的1个提交。git branch -av显示的结果也是2个分支最后一次提交的id，现在也不一样了。 123456789101112➜ mygit git:(master) git pushCounting objects: 3, done.Delta compression using up to 4 threads.Compressing objects: 100% (3/3), done.Writing objects: 100% (3/3), 323 bytes | 0 bytes/s, done.Total 3 (delta 1), reused 0 (delta 0)remote: Resolving deltas: 100% (1/1), completed with 1 local object.To github.com:sail-y/git_demo.git 460b082..d04325a master -&gt; master➜ mygit git:(master) git branch -av* master d04325a update test.txt remotes/origin/master d04325a update test.txt 执行git push，将本地的推送到远程，再执行git branch -av,origin/master显示的最后一次提交发生了一次改变，我们可以理解为origin/master就是保存了一份远程分支的提交信息，是只读的。 处理冲突当另外两个用户同时修改了一个文件，一个人先push，另一个也再push的时候，会被远程拒绝，此时需要处理冲突。 12345678910111213141516171819➜ mygit2 git:(master) git pushTo github.com:sail-y/git_demo.git ! [rejected] master -&gt; master (fetch first)error: failed to push some refs to 'git@github.com:sail-y/git_demo.git'hint: Updates were rejected because the remote contains work that you dohint: not have locally. This is usually caused by another repository pushinghint: to the same ref. You may want to first integrate the remote changeshint: (e.g., 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details.➜ mygit2 git:(master) git pullremote: Counting objects: 3, done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:sail-y/git_demo bb880c1..f162a21 master -&gt; origin/masterAuto-merging hello.txtCONFLICT (content): Merge conflict in hello.txtAutomatic merge failed; fix conflicts and then commit the result. bb880c1..f162a21 master -&gt; origin/master 这个输出的意思是远程的master的commit同步到本地的origin/master。然后发现有冲突，需要自己编辑文件来解决冲突，调用git add来标记冲突已解决，最后提交并push。 123456789101112131415161718192021222324252627➜ mygit2 git:(master) ✗ vi hello.txt➜ mygit2 git:(master) ✗ git statusOn branch masterYour branch and 'origin/master' have diverged,and have 1 and 1 different commits each, respectively. (use "git pull" to merge the remote branch into yours)You have unmerged paths. (fix conflicts and run "git commit") (use "git merge --abort" to abort the merge)Unmerged paths: (use "git add &lt;file&gt;..." to mark resolution) both modified: hello.txtno changes added to commit (use "git add" and/or "git commit -a")➜ mygit2 git:(master) ✗ git add hello.txt ➜ mygit2 git:(master) git statusOn branch masterYour branch and 'origin/master' have diverged,and have 1 and 1 different commits each, respectively. (use "git pull" to merge the remote branch into yours)All conflicts fixed but you are still merging. (use "git commit" to conclude merge)➜ mygit2 git:(master) git commit[master 02b63a9] Merge branch 'master' of github.com:sail-y/git_demo git pull == git fetch + git merge git fetch表示把远程最新的修改拉回到本地，git merge表示将远程的修改合并到代码库里。 我们可以分别测试一下这2个命令，也就是刚才说的先同步远程master信息到本地的origin/master。 123456789101112131415➜ mygit2 git:(master) git branch -av* master 7660696 [ahead 1] hello23 remotes/origin/HEAD -&gt; origin/master remotes/origin/master 02b63a9 Merge branch 'master' of github.com:sail-y/git_demo➜ mygit2 git:(master) git fetchremote: Counting objects: 3, done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:sail-y/git_demo 02b63a9..4ba6b0d master -&gt; origin/master➜ mygit2 git:(master) git branch -av* master 7660696 [ahead 1, behind 1] hello23 remotes/origin/HEAD -&gt; origin/master remotes/origin/master 4ba6b0d change hello.txt 可以看到在执行git fetch之前，remotes/origin/master 的最新commit id是02b63a9，执行以后就变成了4ba6b0d。 接着我们来合并 1234➜ mygit2 git:(master) git merge origin/masterAuto-merging hello.txtCONFLICT (content): Merge conflict in hello.txtAutomatic merge failed; fix conflicts and then commit the result. 就跟之前一样，解决冲突并提交。 分支设计技巧 Gitflow 基于Git分支的开发模型： develop分支（频繁变化的一个分支） test分支（供测试与产品等人员使用的一个分支，变化不是特别频繁） master分支（生产发布分支，变化非常不频繁的一个分支） bugfix（hotfix）分支（生产系统中出现了紧急Bug，用于紧急修复的分支） Git命令别名git可以配置简短的字符串来替代长一些的命令，来方便用户。 这里我们配置几个别名。 1234➜ mygit git:(master) git config --global alias.br branch➜ mygit git:(master) git config --global alias.st status➜ mygit git:(master) git config --global alias.co checkout➜ mygit git:(master) git config --global alias.unstage 'reset HEAD' 这个命令的结果是保存在了~/.gitconfig文件中，可以查看到。 执行外部命令需要，也就是非git命令，需要在前面加感叹号。 ➜ mygit git:(master) git config --global alias.ui &#39;!gitk&#39;]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-checkout、stash等命令]]></title>
    <url>%2F2017%2F09%2F28%2Fgit3%2F</url>
    <content type="text"><![CDATA[前面的文章讲解了分支的操作和原理，接下来接着看一下git的checkout、stash、blame、diff等命令的一些使用方法。 git checkout修改一下文件，看看git status会提示什么？ 12345678910➜ mygit git:(master) vim test.txt ➜ mygit git:(master) ✗ git statusOn branch masterChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: test.txtno changes added to commit (use "git add" and/or "git commit -a") Git提示到：修改没有暂存，请用git add或者git checkout --，我们试一下 1➜ mygit git:(master) ✗ git checkout -- test.txt 文件就已经恢复了，之前我们一直用checkout来切换分支。那么这个命令到底是什么意思呢？ git checkout -- test.txt：丢弃掉相对于暂存区中最后一次添加的文件内容所做的变更。 意思是第一次修改，然后用git add test.txt，将文件添加到暂存区，然后再修改，这个时候执行git checkout -- test.txt，文件就会恢复到暂存区的状态。 123456789101112➜ mygit git:(master) vim test.txt ➜ mygit git:(master) ✗ git add test.txt ➜ mygit git:(master) ✗ git statusOn branch masterChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) modified: test.txt➜ mygit git:(master) ✗ git reset HEAD test.txt Unstaged changes after reset:M test.txt 这次编辑完后将文件添加到暂存区，git status其实我们可以执行git reset HEAD &lt;file&gt;将文件从暂存区移除。 那么git reset HEAD test.txt的作用是： 将之前添加到暂存区（stage,index)的内容从暂存区移除到工作区。 我们要注意这2个命令的区别，一个是从工作区恢复到暂存区，一个是从暂存区移除到工作区。 切换到某一个提交我们都知道分支其实就是指向了某一个提交，那么实际上切换分支，也就是切换到某一个提交上，所以我们也可以直接git checkout commit id来切换提交。 12345678910111213➜ mygit git:(master) git checkout ca07f7bNote: checking out 'ca07f7b'.You are in 'detached HEAD' state. You can look around, make experimentalchanges and commit them, and you can discard any commits you make in thisstate without impacting any branches by performing another checkout.If you want to create a new branch to retain commits you create, you maydo so (now or later) by using -b with the checkout command again. Example: git checkout -b &lt;new-branch-name&gt;HEAD is now at ca07f7b... add another line 切换成功了，但是给了一串提示：你当前在一个游离的状态，你可以随便看看，改一些东西并提交，也切换到其他分支丢弃刚才的修改。如果想保留刚才的提交，就必须基于当前提交新创建一个分支。 123456789101112131415➜ mygit git:(ca07f7b) vi test.txt ➜ mygit git:(ca07f7b) ✗ git statusHEAD detached at ca07f7bChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: test.txtno changes added to commit (use "git add" and/or "git commit -a")➜ mygit git:(ca07f7b) ✗ git checkout mastererror: Your local changes to the following files would be overwritten by checkout: test.txtPlease commit your changes or stash them before you switch branches.Aborting 切换到ca07f7b提交后，对文件进行修改，然后在切换到master分支，被拒绝了，说我们的修改没有提交或者暂存，如果这样做，那么修改就丢失了。 12345678910111213141516➜ mygit git:(ca07f7b) ✗ git add .➜ mygit git:(ca07f7b) ✗ git commit -m 'my commit'[detached HEAD 7d01a11] my commit 1 file changed, 1 insertion(+)➜ mygit git:(7d01a11) git checkout masterWarning: you are leaving 1 commit behind, not connected toany of your branches: 7d01a11 my commitIf you want to keep it by creating a new branch, this may be a good timeto do so with: git branch &lt;new-branch-name&gt; 7d01a11Switched to branch 'master' 我们提交以后再切换到master，成功了，但是又给了提示，说我们有一个提交被落下了，这个提交没有在任何的分支上面。如果你想要创建一个分支来保存提交，那么现在就是一个好的时机 123456➜ mygit git:(master) git branch mycommit 7d01a11➜ mygit git:(master) git branch* master mycommit➜ mygit git:(master) git checkout mycommit Switched to branch 'mycommit' 用git log也查看到了7d01a11是最新一次的提交。 顺道提一下分支改名的命令：git branch -m master master2，把master改名为master2。 stash现在模拟一种情况，新建了一个分支test，master分支上的提交比test要领先一次。 123456789101112131415➜ mygit git:(test) vi test.txt ➜ mygit git:(test) ✗ git statusOn branch testChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: test.txtno changes added to commit (use "git add" and/or "git commit -a")➜ mygit git:(test) ✗ git checkout mastererror: Your local changes to the following files would be overwritten by checkout: test.txtPlease commit your changes or stash them before you switch branches.Aborting 现在在两个分支上的文件不一样，然后在test分支上修改test.txt的内容，随后执行切换分支的操作，git报错了不允许提交：当前的修改会被丢弃，请提交或者暂存。 那这个时候我们工作只是进行了一点，又不想提交怎么办呢，git提供了一个stash命令。 12➜ mygit git:(test) ✗ git stashSaved working directory and index state WIP on test: c96bc8f commit 将当前的工作区保存，现在再切换分支，就没有问题了。 123456789➜ mygit git:(test) ✗ git stashSaved working directory and index state WIP on test: c96bc8f commit➜ mygit git:(test) git statusOn branch testnothing to commit, working tree clean➜ mygit git:(test) git checkout -Switched to branch 'master'➜ mygit git:(master) git checkout -Switched to branch 'test' 现在我们要恢复之前的修改，可以用git stash list查看已经保存的列表。 123➜ mygit git:(test) git stash liststash@&#123;0&#125;: WIP on test: c96bc8f commit(END) 多保存一次： 1234567➜ mygit git:(test) vi test.txt ➜ mygit git:(test) ✗ git stash save 'hello' Saved working directory and index state On test: hello➜ mygit git:(test) git stash liststash@&#123;0&#125;: On test: hellostash@&#123;1&#125;: WIP on test: c96bc8f commit(END) 恢复： 123456789➜ mygit git:(test) git stash pop➜ mygit git:(test) git stash applyAuto-merging test.txtCONFLICT (content): Merge conflict in test.txt➜ mygit git:(test) ✗ vi test.txt ➜ mygit git:(test) ✗ git stash drop stash@&#123;1&#125;stash@&#123;1&#125; is not a valid reference➜ mygit git:(test) ✗ git stash drop stash@&#123;0&#125;Dropped stash@&#123;0&#125; (62dec20aca499f2e2660eb106fae481607d3662f) 保存现场 git stash git stash list 恢复现场 git statsh apply (stash内容并不删除，需要通过git statsh drop stash@{0}手动删除） git stash pop（恢复的同时也将stashn内容删除） git stash apply stash@{0} 标签标签是版本库的一个快照，一般我们在发版的时候会用到。 新建标签标签有两种： 轻量级标签（lightweight）git tag v1.0.1 带有附注标签（annotated）git tag -a v1.0.2 0m &#39;release version&#39; 12345➜ mygit git:(test) git tag v1.0➜ mygit git:(test) git tag -a v2.0 -m '2.0 released'➜ mygit git:(test) git tagv1.0v2.0 查找git tag -l &#39;pattern&#39; 12345➜ mygit git:(test) git tag -l 'v1.0'v1.0➜ mygit git:(test) git tag -l 'v*' v1.0v2.0 删除标签git tag -d v1.0 git blame这个命令可以查看某一个文件上一次是被谁以及什么时间修改的，非常有用。 12345678910➜ mygit git:(master) git blame test.txt ^0d2246f (yangfan 2017-09-27 17:19:28 +0800 1) helloca07f7b3 (sail 2017-09-27 17:21:57 +0800 2) worldd8f57a90 (sail 2017-09-27 17:20:27 +0800 3) hello world3240097a (sail 2017-09-27 17:20:59 +0800 4) hello javac96bc8fc (sail 2017-09-28 17:31:51 +0800 5) hello perle7fb783f (sail 2017-09-28 17:02:47 +0800 6) hello pythona5a26e15 (sail 2017-09-28 17:34:41 +0800 7) hello c#a5a26e15 (sail 2017-09-28 17:34:41 +0800 8) hello scala(END) git diff系统自带diffdiff是Git提供比较文件不同的一个命令，实际上linux或者osx系统本身也提供了一个这样的命令。 先测试一下系统自带的命令 12345678910111213➜ mygit git:(master) ✗ vi test1.txt ➜ mygit git:(master) ✗ diff -u test.txt test1.txt --- test.txt 2017-09-29 10:57:13.000000000 +0800+++ test1.txt 2017-09-29 11:02:54.000000000 +0800@@ -1,8 +1,3 @@ hello world hello world-hello java+hello perl+hello python-hello c#-hello scala ---表示源文件，+++表示要比较的目标文件。 -1,3源文件1到3行，+1,8目标文件1到8行。 再下面的表示源文件-去这一行，就等于目标文件。目标文件+上这一行，就等于源文件。 git diff命令git diff比较的是暂存区与工作区文件的差别。 123➜ mygit git:(master) echo 'hello world' &gt;&gt; aa➜ mygit git:(master) ✗ git add aa➜ mygit git:(master) ✗ git diff aa 因为git add aa添加到暂存区后，工作区和暂存区的文件是一样的，所以这个命令输出空白，表示没有区别。 接着修改一下工作区的aa文件。 12345678910➜ mygit git:(master) ✗ vi aa➜ mygit git:(master) ✗ git diff aadiff --git a/aa b/aaindex 3b18e51..585558d 100644--- a/aa+++ b/aa@@ -1 +1,2 @@ hello world+hello java(END) 这个输出跟系统自带的命令比较像，文件名都一样，表示原始文件是暂存区文件，目标文件是工作区文件，暂存区文件+hello java就等于工作区的aa文件。 git diff commit id这个命令可以比较工作区和某一次提交的差别。 1234567891011121314➜ mygit git:(master) ✗ git commit -am 'aa'[master f6ae6e2] aa 1 file changed, 2 insertions(+) create mode 100644 aa➜ mygit git:(master) vi aa ➜ mygit git:(master) ✗ git diff HEAD diff --git a/aa b/aaindex 585558d..1274e4d 100644--- a/aa+++ b/aa@@ -1,2 +1,3 @@ hello world hello java+hello spring git diff HEAD比较的是最新的提交与工作区之间的差别。 git diff --cached比较的是最新的提交与暂存区之间的差别。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-Fork/Join和CompletableFuture的使用]]></title>
    <url>%2F2017%2F09%2F28%2Fjdk8-11%2F</url>
    <content type="text"><![CDATA[随着多核处理器的出现，提升应用程序处理速度最有效的方式是编写能充分发挥多核能力的软件。可以通过切分大型的任务，让每个子任务并行运行，在Java中目前有直接使用线程的方式、使用Fork/Join框架和JDK8中的并行流来达到这一目的。 这段代码演示了分段求和用线程方式的实现。 分别开启2个线程，给予不同范围的数值进行求和，最后调用join()方法等到线程执行完毕，将2个线程的结果相加得到结果。 Thread方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.Random;public class ThreadJoinTest &#123;static class Computer extends Thread &#123; private int start; private int end; private int result; private int []array; public Computer(int []array , int start , int end) &#123; this.array = array; this.start = start; this.end = end; &#125; public void run() &#123; for(int i = start; i &lt; end ; i++) &#123; result += array[i]; if(result &lt; 0) result &amp;= Integer.MAX_VALUE; &#125; &#125; public int getResult() &#123; return result; &#125;&#125; private final static int COUNTER = 100000001; public static void main(String []args) throws InterruptedException &#123; int []array = new int[COUNTER]; Random random = new Random(); for(int i = 0 ; i &lt; COUNTER ; i++) &#123; array[i] = Math.abs(random.nextInt()); &#125; long start = System.currentTimeMillis(); Computer c1 = new Computer(array , 0 , COUNTER / 2); Computer c2 = new Computer(array , (COUNTER / 2) + 1 , COUNTER); c1.start(); c2.start(); c1.join(); c2.join(); System.out.println(System.currentTimeMillis() - start); //System.out.println(c1.getResult()); System.out.println((c1.getResult() + c2.getResult()) &amp; Integer.MAX_VALUE);&#125; Fork/Join关于Fork/Join框架的介绍的详细介绍，还可以查看方腾飞老师的文章http://ifeve.com/talk-concurrency-forkjoin/，介绍得非常详细。 Fork/Join的实现是要继承RecursiveTask类，实现compute()方法，如例子所示，同样也是求和的计算，用继承的类，将要求和的范围传入，在compute()方法内部进行任务拆分，最后将每个任务的结果进行相加。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 继承RecursiveTask来创建可以用于fork/join框架的任务public class ForkJoinSumCalculator extends RecursiveTask&lt;Long&gt; &#123; // 要求和的数组 private final long[] numbers; // 子任务处理的数组的起始位置和终止位置 private final int start; private final int end; public static final long THRESHHOLD = 10_100; public ForkJoinSumCalculator(long[] numbers) &#123; this(numbers, 0, numbers.length); &#125; private ForkJoinSumCalculator(long[] numbers, int start, int end) &#123; this.numbers = numbers; this.start = start; this.end = end; &#125; @Override protected Long compute() &#123; // 该任务负责求和的部分的大小 int length = end - start; // 如果大小小于或等于阈值，顺序计算结果 if (length &lt; THRESHHOLD) &#123; return computeSequentially(); &#125; // 创建一个子任务来为数组的前一半求和 ForkJoinSumCalculator leftTask = new ForkJoinSumCalculator(numbers, start, start + length / 2); // 利用另一个ForkJoinPool线程异步执行新创建的子任务 leftTask.fork(); // 创建一个任务为数组的后一半求和 ForkJoinSumCalculator rightTask = new ForkJoinSumCalculator(numbers, start + length / 2, end); // 同步执行第二个子任务，有可能允许进一步递归划分 Long rightResult = rightTask.compute(); // 读取第一个子任务的结果，如果尚未完成就等待 Long leftResult = leftTask.join(); // 该任务的结果是两个子任务结果的组合 return leftResult + rightResult; &#125; private Long computeSequentially() &#123; long sum = 0; for (int i = start; i &lt; end; i++) &#123; sum += numbers[i]; &#125; return sum; &#125; public static void main(String[] args) &#123; long[] numbers = LongStream.rangeClosed(1, 1000000).toArray(); ForkJoinTask&lt;Long&gt; task = new ForkJoinSumCalculator(numbers); Long sum = new ForkJoinPool().invoke(task); System.out.println(sum); &#125;&#125; Java8-parallelStreamJava8做这种求和的操作很简洁，一行代码就可以解决。 1IntStream.rangeClosed(0, 100000000).parallel().sum() 之前在网上了解到说Java 8 并行流采用共享线程池，对性能造成了严重影响。可以包装流来调用自己的线程池解决性能问题。 如果要使用线程池，可采用以下方式： 123456789101112131415161718ForkJoinPool forkJoinPool = new ForkJoinPool(3); forkJoinPool.submit(() -&gt; &#123; firstRange.parallelStream().forEach((number) -&gt; &#123; try &#123; Thread.sleep(5); &#125; catch (InterruptedException e) &#123; &#125; &#125;);&#125;); ForkJoinPool forkJoinPool2 = new ForkJoinPool(3); forkJoinPool2.submit(() -&gt; &#123; secondRange.parallelStream().forEach((number) -&gt; &#123; try &#123; Thread.sleep(5); &#125; catch (InterruptedException e) &#123; &#125; &#125;);&#125;); CompletableFutureFuture接口在介绍CompletableFuture之前，必须要说一下Future接口，Future接口在Java5中被引入，它建模了一种异步计算，返回一个执行运算结果的引用，当运算结束后，这个引用被返回给调用方。这样就不需要等到方法结果，调用线程可以接着去做其他事情，这样就完成了异步调用，方法结果只能通过get()方法获取，get()方法是阻塞的，会阻塞直到异步方法运行完成返回结果。打个比方，把衣服放进洗衣机清洗，这个时候我们可以去做点别的事情，等到洗衣机清洗完成后，我们才会去取干净的衣服。下面展示一个简单的示例，只需要将操作封装到Callable接口中： 12345678910111213141516171819202122232425262728293031323334353637383940public class FutureTaskTest &#123; static class CallImpl implements Callable&lt;String&gt; &#123; private String input; public CallImpl(String input) &#123; this.input = input; &#125; public String call() &#123; try &#123;//延迟一点点 Random random = new Random(); Thread.sleep((random.nextInt() + this.input.hashCode()) &amp; 10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return "输入参数:" + input; &#125; &#125; public static void main(String []args) throws InterruptedException, ExecutionException &#123; ExecutorService executorService = Executors.newFixedThreadPool(2); @SuppressWarnings("unchecked") List&lt;Future&lt;String&gt;&gt; list = Arrays.asList( executorService.submit(new CallImpl("t1")) , executorService.submit(new CallImpl("t2")) , executorService.submit(new CallImpl("t3")) ); /*List&lt;Future&lt;String&gt;&gt; list = executorService.invokeAll(Arrays.asList( new CallImpl("t1"), new CallImpl("t2"), new CallImpl("t3") ));*/ for(Future&lt;String&gt; future : list) &#123; String result = future.get();//如果没返回，会阻塞 System.out.println(result + "\t" + System.currentTimeMillis()); &#125; executorService.shutdown(); &#125;&#125; Future接口的局限性Future提供了isDone()方法检测异步计算是否已经结束，get()方法等待异步操作结束，以及获取计算的结果。但是这些操作依然很难实现：等到所有Future任务完成，通知线程获取结果并合并。下面就一起来看看JDK8中新引入的CompletableFuture。 使用CompletableFuture构建异步应用CompletableFuture实现了Future接口，它的complete()方法就相当于结束CompletableFuture对象的执行，并设置变量的值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100/** * @author yangfan * @date 2017/04/11 */public class Shop &#123; private static final Random random = new Random(); private String name; public Shop(String name) &#123; this.name = name; &#125; public String getPrice(String product) &#123; double price = calculatePrice(product); Discount.Code code = Discount.Code.values()[random.nextInt(Discount.Code.values().length)]; return String.format("%s:%.2f:%s", name, price, code); &#125; public Future&lt;Double&gt; getPriceAsync1(String product) &#123; // 创建CompletableFuture对象，它会包含计算的结果 CompletableFuture&lt;Double&gt; futurePrice = new CompletableFuture&lt;&gt;(); // 在另一个线程中以异步方式执行计算 new Thread(() -&gt; &#123; try &#123; double price = calculatePrice(product); // 需长时间计算的任务结束并得出结果时，设置Future的返回值 futurePrice.complete(price); &#125; catch (Exception e) &#123; //异常处理 futurePrice.completeExceptionally(e); &#125; &#125;).start(); // 无需等待还没结束的计算，直接返回Future对象 return futurePrice; &#125; public Future&lt;Double&gt; getPriceAsync(String product) &#123; return CompletableFuture.supplyAsync(() -&gt; calculatePrice(product)); &#125; private double calculatePrice(String product) &#123; randomDelay(); return random.nextDouble() * product.charAt(0) + product.charAt(1); &#125; public static void delay() &#123; try &#123; Thread.sleep(1000L); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; public static void randomDelay() &#123; int delay = 500 + random.nextInt(2000); try &#123; Thread.sleep(delay); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; public static void main(String[] args) &#123; Shop shop = new Shop("BestShop"); long start = System.nanoTime(); // 查询商店，试图去的商品的价格 Future&lt;Double&gt; futurePrice = shop.getPriceAsync("my favorte product"); long invocationTime = (System.nanoTime() - start) / 1_000_000; System.out.println("Invocation returned after " + invocationTime + " msecs"); // 执行更多任务，比如查询其他商店 //doSomethingElse(); // 在计算商品价格的同时 try &#123; // 从Future对象中读取价格，如果价格未知，会发生阻塞 double price = futurePrice.get(); System.out.printf("Price is %.2f%n", price); &#125; catch (InterruptedException | ExecutionException e) &#123; throw new RuntimeException(e); &#125; long retrievalTime = ((System.nanoTime() - start) / 1_000_000); System.out.println("Price returned after " + retrievalTime + " msecs"); &#125; public String getName() &#123; return name; &#125;&#125; 上面的例子来自于《Java8实战》，模拟了一个耗时的操作，然后通过CompletableFuture包装成异步方法进行调用。注意代码里演示了两种方式，一种自己new一个线程再调用complete方法，还有就是用CompletableFuture自身提供的工厂方法，CompletableFuture.supplyAsync，它能更容易地完成整个流程，还不用担心实现的细节。 现在看来好像和Future方式也没有什么区别，都是包装一下最后通过get()方法获取结果，但是CompletableFuture配合Java8用起来就非常厉害了，它提供了很多方便的方法，下面进行一个演示。 同样是价格查询，我们现在接到一个需求，就是获取一个商品在不同商店的报价，一般来说用传统的方式就是写一个for循环，遍历商店然后获取价格，要想效率快一点我们也可以用并行流的方式来查询，但是并行流返回的结果是无序的。下面再将异步也引入，我们可以实现有序的并行操作： 12345private static List&lt;String&gt; findPrices_1(String product) &#123; List&lt;CompletableFuture&lt;String&gt;&gt; priceFutures = shops.stream() .map(shop -&gt; CompletableFuture.supplyAsync(() -&gt; shop.getName() + " price is " + shop.getPrice(product), executor)).collect(Collectors.toList()); return priceFutures.stream().map(CompletableFuture::join).collect(Collectors.toList());&#125; 这里创建CompletableFuture和调用join()方法是两个不同的流是有原因的，如果只在一个流里，那么就没有异步的效果了，下一个Future必须等到上一个完成后才会被创建和执行。上面的代码执行效率并不会比并行流的效率差。 默认情况下，并行流和CompletableFuture默认都是用固定数目的线程，都是取决于Runtime. getRuntime().availableProcessors()的返回值。并行流的线程池并不好控制，其本质是内部隐含使用了ForkJoinPool线程池，最大并发数可以通过系统变量设置。所以CompletableFuture也就具有了优势，它允许配置自定义的线程池，这也可以为实际应用程序带来性能上的提升（并行流无法提供的API），CompletableFuture.supplyAsync(Supplier supplier,Executor executor)提供了重载的方法来指定执行器使用自定义的线程池。 12345678910// 创建一个线程池，线程池中线程的数目为100和商店数目二者中较小的一个值private static final Executor executor = Executors.newFixedThreadPool(Math.min(shops.size(), 100), new ThreadFactory() &#123; @Override public Thread newThread(Runnable r) &#123; Thread t = new Thread(r); // 使用守护线程---这种方式不会阻止程序关掉 t.setDaemon(true); return t; &#125;&#125;); 并行–使用流还是CompletableFutures？现在我们知道对集合进行并行计算有两种方式： 转化为并行流，利用map开展工作。 取出每一个元素，创建线程，在CompletableFuture内对其进行操作 后者提供了更多的灵活性，你可以调整线程池的大小，而这能帮助我们确保整体的计算不会因为线程都在等待I/O而发生阻塞。 那么如何选择呢，建议如下： 进行计算密集型的操作，并且没有I/O，那么推荐使用Stream接口，因为实现简单，同时效率也可能是最高的（如果所有的线程都是计算密集型的，那就没有必要创建比处理器核数更多的线程）。 如果并行操作设计等到I/O的操作（网络连接，请求等），那么使用CompletableFuture灵活性更好，通过控制线程数量来优化程序的运行。 CompletableFuture还提供了了一些非常有用的操作例如，thenApply(),thenCompose(),thenCombine()等 thenApply()是操作完成后将结果传入进行转换 thenCompose()是对两个异步操作进行串联，第一个操作完成时，对第一个CompletableFuture对象调用thenCompose，并向其传递一个函数。当第一个CompletableFuture执行完毕后，它的结果将作为该函数的参数，这个函数的返回值是以第一个CompletableFuture的返回做输入计算出第二个CompletableFuture对象。 thenCombine()会异步执行两个CompletableFuture任务，然后等待它们计算出结果后再进行计算。 123456789101112private static List&lt;String&gt; findPrices(String product) &#123; List&lt;CompletableFuture&lt;String&gt;&gt; priceFutures = shops.stream() // 以异步方式取得每个shop中指定产品的原始价格 .map(shop -&gt; CompletableFuture.supplyAsync(() -&gt; shop.getPrice(product), executor)) // Quote对象存在时，对其返回值进行转换 .map(future -&gt; future.thenApply(Quote::parse)) // 使用另一个异步任务构造期望的Future，申请折扣 .map(future -&gt; future.thenCompose(quote -&gt; CompletableFuture.supplyAsync(() -&gt; Discount.applyDiscount(quote), executor))) .collect(Collectors.toList()); return priceFutures.stream().map(CompletableFuture::join).collect(Collectors.toList());&#125; 通常而言，名称中不带Async的方法和它的前一个任务一样，在同一个线程中运行；而名称以Async结尾的方法会将后续的任务提交到一个线程池，所以每个任务都是由不同线程处理的，例如thenApplyAsync()，thenComposeAsync()等。 最后看一段利用thenAccept()来使用异步计算结果的代码： 12345678910111213141516171819202122// 这里演示获取最先返回的数据public static Stream&lt;CompletableFuture&lt;String&gt;&gt; findPricesStream(String product) &#123; return shops.stream().map(shop -&gt; CompletableFuture.supplyAsync(() -&gt; shop.getPrice(product), executor)) .map(future -&gt; future.thenApply(Quote::parse)) .map(future -&gt; future.thenCompose(quote -&gt; CompletableFuture.supplyAsync( () -&gt; Discount.applyDiscount(quote), executor)));&#125;public static void main(String[] args) &#123; long start = System.nanoTime();// System.out.println(findPrices("myPhone27S")); CompletableFuture[] futures = findPricesStream("myPhone27S") .map(f -&gt; f.thenAccept(s -&gt; System.out.println(s + " (done in " + ((System.nanoTime() - start) / 1_000_000) + " msecs)"))) .toArray(CompletableFuture[]::new);// CompletableFuture.allOf(futures).join(); CompletableFuture.anyOf(futures).join(); long duration = (System.nanoTime() - start) / 1_000_000; System.out.println("Done in " + duration + " msecs");&#125; 这样就几乎无需等待findPricesStream的调用，实现了一个真正的异步方法。]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-源码分析(二)]]></title>
    <url>%2F2017%2F09%2F22%2Fnetty7%2F</url>
    <content type="text"><![CDATA[源码分析(二)在理解了Reactor模式后，因为Netty框架本身就是基于Reactor模式的一种实现，所以回过头来再看源码，才能更好的理解代码中一些类的角色和意义。 ServerBootStrap在调用bind()方法后，通过channelFactory反射的方式创建了我们指定的Channel（NioServerSocketChannel.class）对象，然后调用了init(channel)方法对channel进行了初始化设置。 看一下ServerBootStrap.init()，初始化Channel的代码，ChannelPipeline p = channel.pipeline();这里面就出现了一个Netty中一个又一个非常核心的类ChannelPipeline，它是在Channel的父类AbstractChannel初始化的时候创建的。 ChannelPipelineChannelPipeline里面是一个一个的ChannelHandler，当客户端的请求到来或者出去的的时候，会一个一个的通过这些处理器，就像一个过滤器一样。每一个Channel都拥有自己的ChannelPipeline，当一个Channel被创建的时候，ChannelPipeline也跟着被创建了。I/O事件只能被ChannelInboundHandler和ChannelOutboundHandler其中之一所处理，处理完成后再传递到别的处理器中（ChannelHandlerContext#fireChannelRead(Object)或者ChannelHandlerContext#write(Object)）。 1234567891011121314151617181920212223242526272829303132333435363738* I/O Request* via &#123;@link Channel&#125; or* &#123;@link ChannelHandlerContext&#125;* |* +---------------------------------------------------+---------------+* | ChannelPipeline | |* | \|/ |* | +---------------------+ +-----------+----------+ |* | | Inbound Handler N | | Outbound Handler 1 | |* | +----------+----------+ +-----------+----------+ |* | /|\ | |* | | \|/ |* | +----------+----------+ +-----------+----------+ |* | | Inbound Handler N-1 | | Outbound Handler 2 | |* | +----------+----------+ +-----------+----------+ |* | /|\ . |* | . . |* | ChannelHandlerContext.fireIN_EVT() ChannelHandlerContext.OUT_EVT()|* | [ method call] [method call] |* | . . |* | . \|/ |* | +----------+----------+ +-----------+----------+ |* | | Inbound Handler 2 | | Outbound Handler M-1 | |* | +----------+----------+ +-----------+----------+ |* | /|\ | |* | | \|/ |* | +----------+----------+ +-----------+----------+ |* | | Inbound Handler 1 | | Outbound Handler M | |* | +----------+----------+ +-----------+----------+ |* | /|\ | |* +---------------+-----------------------------------+---------------+* | \|/* +---------------+-----------------------------------+---------------+* | | | |* | [ Socket.read() ] [ Socket.write() ] |* | |* | Netty Internal I/O Threads (Transport Implementation) |* +-------------------------------------------------------------------+ 上面的流程图很清楚的描述了一个请求进和出两个方向是如何被Netty处理的。 ChannelOption提供协议相关的配置，比如TCP的一些配置，它是类型安全的，实现了Constant接口，通过ConstantPool维护。ChannelOption本身就是Key的信息，真正的值是在ConstantPool中。详情可以查看ChannelConfig，ChannelOption只是Key。 AttributeMap AttributeMap接口只有一个attr()方法，接收一个AttributeKey类型的key，返回一个Attribute类型的value。AttributeMap这是是绑定在Channel或者ChannelHandlerContext上的一个附件，相当于依附在这两个对象上的寄生虫一样，相当于附件一样。 AttributeKey相当于Map中的key，AttributeMap相当于Map, Attribute相当于Map中的value，它的实现方式和ChannelOption是类似的，都是继承了AbstractConstant，包含了一个ConstantPool的属性。 在Netty4.0中，每一个ChannelHandlerContext都是ChannelHandler和ChannelPipeline之间连接的桥梁，每一个ChannelHandlerContext都有属于自己的上下文，也就说每一个ChannelHandlerContext上如果有AttributeMap都是绑定上下文的，也就说如果A的ChannelHandlerContext中的AttributeMap，B的ChannelHandlerContext是无法读取到的 但是Channel上的AttributeMap就是大家共享的，每一个ChannelHandler都能获取到。 不过在Netty4.1中，这个情况发生了改变，只在Channel中维护了一个Map，ChannelHanlderContext也是用的Channel中的Map。他们的attr()方法是等价的。 ServerBootstrapAcceptor123456789101112131415161718p.addLast(new ChannelInitializer&lt;Channel&gt;() &#123; @Override public void initChannel(final Channel ch) throws Exception &#123; final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) &#123; pipeline.addLast(handler); &#125; ch.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); &#125; &#125;); &#125;&#125;); 继续看init()代码，上一篇文章也提到了Netty在pipeline里添加了一个处理器，将ocnfig.handler也就是代码中的LoggingHandler添加到了channelPipeline中(注意这个是通过handler()方法注册的，也就是提供给BossGroup使用的)。然后紧接着添加了一个ServerBootstrapAcceptor，在看完Reactor模式后，这个类是不是觉得很眼熟，它的角色也就是Reactor模式当中的Acceptor。 ChannelHandler 接受连接或创建他们只是你的应用程序的一部分，虽然这些任务很重要，但是一个网络应用程序往往是更复杂的，需要更多的代码编写，如处理传入和传出的数据。Netty提供了一个强大的处理这些事情的功能，允许用户自定义ChannelHandler的实现来处理数据。使得ChannelHandler更强大的是可以连接每个ChannelHandler来实现任务，这有助于代码的整洁和重用。但是处理数据只是ChannelHandler所做的事情之一，也可以压制I/O操作，例如写请求。所有这些都可以动态实现。ChannelHandler就是入站(InBound)和出站(OutBound)处理器。 ChannelInitializerChannelInitializer本身也是一个特殊的Inbound处理器，用来初始化channel。 ChannelHandlerContext每创建一个ChannelHandler，随之也会闯进一个ChannelHandlerContext。 ChannelPipeline里面真实存放的对象实际上是ChannelHandlerContext，ChannelHandlerContext里又维护了ChannelHannlder。所以ChannelHandlerContext实际上是ChannelPipeline和ChannelHandler的桥梁，它提供了api可以获取Channel对象，和与之关联的ChannelHandler对象、ChannelPipeline对象。 Reactor模式在Netty中的体现 通过这个图我们可以看到，客户端向BossGroup发起连接请求，BossGroup本身监听的是一个OP_ACCEPT事件（NioServerSocketChannel的构造方法中可以找到注册事件的代码），一旦OP_ACCEPT事件产生之后，select()方法就会返回SelectionKey的集合，那么SelectionKey本身也是包装了SocketChannel对象，就是与客户端真正建立连接的SocketChannel对象。对于Netty来说，会将SocketChannel包装成NIOSocketChannel，接着又将NIOSocketChannel注册到了WorkerGroup的selector中。WorkerGroup的Selector监听的是OP_READ，所以当数据发送过来的时候，就不再跟BossGroup打交道了，转而和WorkerGroup进行数据的传递。 下面这一段就是channelFactory通过反射创建NioServerSocketChannel对象的时候，注册了OP_ACCEPT事件，并进行一个包装。 1234public NioServerSocketChannel(ServerSocketChannel channel) &#123; super(null, channel, SelectionKey.OP_ACCEPT); config = new NioServerSocketChannelConfig(this, javaChannel().socket());&#125; NioServerSocketChannelConfig代码跟进去看，实际上是调用的父类的构造方法： 123public DefaultChannelConfig(Channel channel) &#123; this(channel, new AdaptiveRecvByteBufAllocator());&#125; AdaptiveRecvByteBufAllocator是干什么用的呢？它会根据反馈自动调整Channel所关联的buffer的大小。它的规则是如果上次读取的时候填满了Buffer，那么就会增加，反之连续2次没有填充满，就会减少。 123static final int DEFAULT_MINIMUM = 64;static final int DEFAULT_INITIAL = 1024;static final int DEFAULT_MAXIMUM = 65536; 这3个变量也定义了默认的Buffer的最小值，初始值和最大值。 其中有一个SIZE_TABLE在静态代码块中定义了初始化大小变化的一个数组，通过getSizeTableIndex来引用数组里面的值，动态调整Buffer大小就取数组里面的值来调整。内部类HandleImpl（记录每次读取的大小来猜测下一次的大小–动态调整）的父类MaxMessageHandle里有一个很重要的方法allocate()，通过平台来判断是使用directBuffer还是heapBuffer： 123456public ByteBuf ioBuffer(int initialCapacity) &#123; if (PlatformDependent.hasUnsafe()) &#123; return directBuffer(initialCapacity); &#125; return heapBuffer(initialCapacity);&#125; 其中堆内Buffer是Netty用数组自己实现的，而DirectBuffer最终可以跟踪到UnpooledUnsafeDirectByteBuf.allocateDirect()，又发现我们很熟悉的NIO代码： 123protected ByteBuffer allocateDirect(int initialCapacity) &#123; return ByteBuffer.allocateDirect(initialCapacity);&#125; ChannelChannel是针对于网络套接字的一个连接点，也可以认为它是一个可以执行I/O操作的组件。 它提供以下功能： 获取Channel当前的状态（open,connected) channel配置参数(buffer size) IO操作（read,write,connect,bind) 提供一个ChannelPipline，可以处理所有当前Channel关联的所有的I/O事件和请求。 EventLoopGroup 一个EventLoopGroup中会包含一个或多个EventLoop。 一个EventLoop在它的整个生命周期当中都只会与唯一一个Thread进行绑定。 所有由EventLoop所处理的各种I/O事件都将在它所关联的那个Thread上进行处理。 我们可以在SingleThreadEventExecutor.execute()发现这段代码。 一个Channel在它的整个生命周期中只会注册在一个EventLoop上。 一个EventLoop在运行过程当中，会被分配给一个或者多个Channel。 重要结论：在Netty中，Channel的实现一定是线程安全的；基于此，我们可以存储一个Channel的引用，并且在需要向远程端点发送数据时，通过这个引用来调用Channel相应的方法；即便当时有很多线程都在使用她也不会出现多线程问题；而且，消息一定会按照顺序发送出去。 我们在业务开发中，不要将长时间执行的耗时任务放入到EventLoop的执行队列中，因为它将会一直阻塞该线程所对应的所有Channel上的其他执行任务，如果我们需要进行阻塞调用或是耗时的操作（实际开发中很常见），那么我们就需要使用一个专门的EventExecutor（业务线程池）。 通常会有两种实现方式： 在ChannelHandler的回调方法中，使用自己定义的业务线程池，这样就可以实现异步调用。 借助于Netty提供的向ChannelPipeline添加ChannelHandler时调用的addLast方法来传递EventExecutor。 说明：默认情况下（调用addLast(handler))，ChannelHandler中的回调方法都是由I/O线程所执行，如果调用了ChannelPipeline addLast(EventExecutorGroup group, ChannelHandler... handlers)方法，那么ChannelHandler中的回调方法就是由参数中的group线程组来执行的。 JDK的Futuer和Netty的FutuerJDK所提供的Future只能通过手工方式检查执行结果，而这个操作是会阻塞的；Netty则对ChannelFuture进行了增强，通过ChannelFutureListener以回调的方式来获取执行结果，去除了手工检查阻塞的操作；值得注意的是：ChannelFutureListener的operationComplete方法是由I/O线程执行的，因此要注意的是不要在这里执行耗时的操作，否则需要通过另外的线程或线程池来执行。 ctx.write()和ctx.channel().write()的区别在Netty中有两种发送消息的方式，可以直接写到Channel中，也可以写到与ChannelHandler所关联的那个ChannelHandlerContext中。对于前一种方式来说，消息会从ChannelPipeline的末尾开始流动；对于后一种方式来说，消息将从ChannelPipeline中的下一个ChannelHandler开始流动。 结论： ChannelHandlerContext与ChannelHandler之间是关联绑定关系是永远都不会发生改变的，因此对其进行缓存是没有任何问题的。 对于与Channel的同名方法来说，ChannelHandlerContext方法将会产生更短的事件流，所以我们应该在可能的情况下利用这个特性来提升应用性能。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-分支]]></title>
    <url>%2F2017%2F09%2F21%2Fgit2%2F</url>
    <content type="text"><![CDATA[先介绍一下.gitignore .gitignore项目里面经常有一些文件是不需要纳入版本控制的，例如我编译后的class文件，或者idea产生的一些配置文件等。 1234567891011121314151617181920212223242526272829.gradle/build/!gradle/wrapper/gradle-wrapper.jar### STS ###.apt_generated.classpath.factorypath.project.settings.springBeans### IntelliJ IDEA ###.idea*.iws*.iml*.ipr### NetBeans ###nbproject/private/build/nbbuild/dist/nbdist/.nb-gradle/rebel.xml/out 上面是我的一个项目的配置，看得出来是有一些规则的，使用起来很简单，将这些规则保存到一个.gitignore中后，在项目里面这些文件或者文件夹里面的所有修改操作都不会被纳入到git版本控制中，也不会提交。 规则 *.a 忽略所有.a结尾的文件 !lib.a 但lib.a除外 /TODO 仅仅忽略项目根目录下的TODO文件，但不包括subdir/TODO build/ 忽略build/目录下的所有文件 doc/*.txt 会忽略doc/notes.txt但不包括doc/server/arch.txt Git分支分支就是当你正在开发的时候，你可以基于当前的工作副本新开一条开发线，2边都可以进行单独的开发，当开发到某一个阶段的时候，可以合并到一起，这样2个分支的修改内容都能保留。 ————— master————— new_branch git branch12➜ mygit git:(master) git branch* master 这个命令可以显示当前所在分支，当使用git init创建仓库的时候，git默认会为我们创建一个master分支。 创建分支创建一个新的分支并切换到新的分支上 123456➜ mygit git:(master) git branch new_branch➜ mygit git:(master) git branch* master new_branch➜ mygit git:(master) git checkout new_branchSwitched to branch 'new_branch' 这个时候2个分支上的文件还是一模一样的。 切换分支在new_branch分支上创建一个文件并提交，切换到master上查看，是没有这个文件的，这也说明2个不同分支上的文件是独立的，修改是互不影响的。 切换分支的命令：git checkout 123456789101112131415➜ mygit git:(new_branch) echo 'test new branch' &gt; test4.txt➜ mygit git:(new_branch) ✗ git add test4.txt ➜ mygit git:(new_branch) ✗ git commit -m 'add a new file'[new_branch 8013cc2] add a new file 1 file changed, 1 insertion(+) create mode 100644 test4.txt➜ mygit git:(new_branch) git statusOn branch new_branchnothing to commit, working tree clean➜ mygit git:(new_branch) lsmydir settings.properties test.txt test4.txt➜ mygit git:(new_branch) git checkout masterSwitched to branch 'master'➜ mygit git:(master) lsmydir settings.properties test.txt 如果想在2个分支之间来回切换，可以使用git checkout - 12345678➜ mygit git:(new_branch) git checkout -Switched to branch 'master'➜ mygit git:(master) git checkout -Switched to branch 'new_branch'➜ mygit git:(new_branch) git checkout -Switched to branch 'master'➜ mygit git:(master) git checkout -Switched to branch 'new_branch' 删除分支分支一样可以删除，命令：git branch -d new_branch 12➜ mygit git:(new_branch) git branch -d new_brancherror: Cannot delete branch 'new_branch' checked out at '/Users/xiaomai/code/zhanglong/git/mygit 报错了，为什么呢？因为你当前本来就在new_branch上，就好比你坐到一个凳子上怎么能把凳子挪走呢。 12345➜ mygit git:(new_branch) git checkout -Switched to branch 'master'➜ mygit git:(master) git branch -d new_brancherror: The branch 'new_branch' is not fully merged.If you are sure you want to delete it, run 'git branch -D new_branch'. 又出错了，说new_branch并没有合并过，不能删除，如果要删除要使用git branch -D new_branch。 因为我们之前创建了test4.txt这个文件，它是存在于new_branch分支上的，如果直接删除这个分支，那么在这个分支上所做的修改就会丢失了，也就是test4.txt这个文件就没了。 现在来测试一下： 12➜ mygit git:(master) git branch -D new_branchDeleted branch new_branch (was 8013cc2). 下面的新创建了一个new_branch2分支，但是这次却可以直接删除，因为这2个分支是一样的，没有区别，不需要合并。 1234567➜ mygit git:(master) git branch new_branch2➜ mygit git:(master) git checkout new_branch2 Switched to branch &apos;new_branch2&apos;➜ mygit git:(new_branch2) git checkout -Switched to branch &apos;master&apos;➜ mygit git:(master) git branch -d new_branch2 Deleted branch new_branch2 (was c7a801e). 创建并切换创建分支并切换，我们之前是用了2个命令，Git也提供了简便的方式，用一条命令来完成这个操作。 12➜ mygit git:(master) git checkout -b new_branch4Switched to a new branch 'new_branch4' 合并刚才创建分支后删除，有提示到说没有合并，我们再创建一个分支，并对文件做一些修改。 12345678910111213141516171819➜ mygit git:(new_branch4) vi test.txt ➜ mygit git:(new_branch4) ✗ git statusOn branch new_branch4Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: test.txtno changes added to commit (use "git add" and/or "git commit -a")➜ mygit git:(new_branch4) ✗ git add .➜ mygit git:(new_branch4) ✗ git commit -m 'add a new line in test'[new_branch4 41c3327] add a new line in test 1 file changed, 1 insertion(+)➜ mygit git:(new_branch4) git checkout -Switched to branch 'master'➜ mygit git:(master) git branch* master new_branch4 Git里合并的命令是git merge 12345➜ mygit git:(master) git merge new_branch4 Updating c7a801e..41c3327Fast-forward test.txt | 1 + 1 file changed, 1 insertion(+) 这个命令的意思是，将new_branch4分支上的修改合并到master上。 现在查看test.txt的文件内容，和我们在new_branch4上修改的内容一样了。 再验证一下合并之后能不能成功删除。 12➜ mygit git:(master) git branch -d new_branch4 Deleted branch new_branch4 (was 41c3327). 因为new_branch4分支上的修改已经合并（merge）到master上了，所以可以正常删除。 分支的原理 分支到底是什么呢，分支指的是一个commit对象链，它就像是一条工作记录线（或者时间线），这张图有4个commit，这4个commit共同组成了一个branch。 每一次提交都会记录上一次的commit id，我们看第一个提交的commit id是10dc7…，而第二次提交的parent属性正好是10dc7。而这也就把所有的commit连接起来了，在版本库只有一个分支的情况下，这些连接起来的线也就组成了分支的信息。 HEAD之前我们在使用git rest HEAD的时候，这个HEAD到底是什么意思呢？ HEAD指向的是当前分支，master指向提交，什么意思呢，如下图所示，master是指向第3个提交，而HEAD是指向的master分支。 下图就相当于在master分支上执行git checkout -b dev，创建并切换到dev分支：创建了一个叫dev的指针，它和master一样也指向同一个提交，并且HEAD指向当前分支上，也就是dev。 这个我们可以通过命令来实战验证一下。 1234➜ mygit git:(master) git checkout -b devSwitched to a new branch 'dev'➜ mygit git:(dev) cat .git/HEAD ref: refs/heads/dev 这张图又什么意思呢，相信很容易猜到了，就是在dev分支上产生了一次提交。 12345➜ mygit git:(dev) vi test.txt ➜ mygit git:(dev) ✗ git add .➜ mygit git:(dev) ✗ git commit -m &apos;add a new line in test.txt&apos;[dev bd1bf5d] add a new line in test.txt 1 file changed, 1 insertion(+) 下面的图意思是将dev合并到master分支上，因为master没有修改，所以这里不会产生冲突，直接将master的指针指向了第4个提交（这种没有冲突的提交也叫Fast-Forward） git log -3 先看一下dev分支的提交日志： 1234567891011commit bd1bf5d33911594452c43f990904f393f6037607 (HEAD -&gt; dev)Author: 帆 &lt;test@test.com&gt;Date: Thu Sep 21 15:14:45 2017 +0800 add a new line in test.txtcommit 41c3327a9cf94bcb85799f0253fba0707036292c (master)Author: 帆 &lt;test@test.com&gt;Date: Thu Sep 21 14:29:37 2017 +0800 add a new line in test 再看一下master分支的提交日志： 123456789commit 41c3327a9cf94bcb85799f0253fba0707036292c (HEAD -&gt; master)Author: 帆 &lt;test@test.com&gt;Date: Thu Sep 21 14:29:37 2017 +0800 add a new line in testcommit c7a801e2d02195a49d6741d61f1e6b471664b3f6Author: 帆 &lt;test@test.com&gt;Date: Thu Sep 21 11:06:54 2017 +0800 可以看到dev的第二个提交正好是master分支的第一个提交。 12345➜ mygit git:(master) git merge devUpdating 41c3327..bd1bf5dFast-forward test.txt | 1 + 1 file changed, 1 insertion(+) 现在执行merge操作，可以看到update 41c3327，也就是dev的第一条提交，正好是master没有的，结合刚才的图来看，也就是把master指向了41c3327。 看一下merge之后master分支上的git log -3 123456789101112131415161718commit bd1bf5d33911594452c43f990904f393f6037607 (HEAD -&gt; master, dev)Author: 帆 &lt;test@test.com&gt;Date: Thu Sep 21 15:14:45 2017 +0800 add a new line in test.txtcommit 41c3327a9cf94bcb85799f0253fba0707036292cAuthor: 帆 &lt;test@test.com&gt;Date: Thu Sep 21 14:29:37 2017 +0800 add a new line in testcommit c7a801e2d02195a49d6741d61f1e6b471664b3f6Author: 帆 &lt;test@test.com&gt;Date: Thu Sep 21 11:06:54 2017 +0800 gitignore(END) 下面再看看我们在master和dev分支上都作一些修改，然后合并。 1234567891011121314➜ mygit git:(master) echo 'hello java' &gt; test2.txt ➜ mygit git:(master) ✗ git add .➜ mygit git:(master) ✗ git commit -m 'create a file test2.txxt'[master bc84a7f] create a file test2.txxt 1 file changed, 1 insertion(+) create mode 100644 test2.txt➜ mygit git:(master) git checkout devSwitched to branch 'dev'➜ mygit git:(dev) ✗ echo 'hello kotlin' &gt; test2.txt ➜ mygit git:(dev) ✗ git add .➜ mygit git:(dev) ✗ git commit -m 'create file test2.txt'[dev def3afc] create file test2.txt 1 file changed, 1 insertion(+) create mode 100644 test2.txt 现在我们在master和dev上都创建了一个test2.txt，并且内容不一样。 现在master和dev指向的commit都不一样，现在合并： 1234➜ mygit git:(master) git merge devAuto-merging test2.txtCONFLICT (add/add): Merge conflict in test2.txtAutomatic merge failed; fix conflicts and then commit the result. 出现了冲突，git不知道如何处理这种冲突，需要我们自己解决，我们现在看一下test2.txt的内容是什么vi test2.txt 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADhello java=======hello kotlin&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev 上面是HEAD（当前分支master）的内容，下面的dev分支的内容，现在我们手动处理一下冲突，编辑保留我们需要留下的内容。 12345678910111213➜ mygit git:(master) ✗ vi test2.txt ➜ mygit git:(master) ✗ git statusOn branch masterYou have unmerged paths. (fix conflicts and run "git commit") (use "git merge --abort" to abort the merge)Unmerged paths: (use "git add &lt;file&gt;..." to mark resolution) both added: test2.txtno changes added to commit (use "git add" and/or "git commit -a") 现在执行git add test2.txt表示冲突已经解决，可以提交 123456789101112131415161718➜ mygit git:(master) ✗ git add test2.txt ➜ mygit git:(master) ✗ git statusOn branch masterAll conflicts fixed but you are still merging. (use "git commit" to conclude merge)Changes to be committed: modified: test2.txt➜ mygit git:(master) ✗ git commit[master b4bd64b] Merge branch 'dev'➜ mygit git:(master) git statusOn branch masternothing to commit, working tree clean➜ mygit git:(master) cat test2.txt hello javahello kotlin 现在再回过头来看看dev分支下test2.txt的内容： 1234➜ mygit git:(master) git checkout devSwitched to branch 'dev'➜ mygit git:(dev) cat test2.txt hello kotlin 内容还是跟之前的一样，如果这个时候我们执行git merge master，还会产生冲突吗？ 12345678➜ mygit git:(dev) git merge masterUpdating def3afc..b4bd64bFast-forward test2.txt | 1 + 1 file changed, 1 insertion(+)➜ mygit git:(dev) cat test2.txt hello javahello kotlin 没有冲突，fast-forward直接成功，为什么呢？ 因为在master上合并后，指针移动了2次，一次是dev的提交，一次是合并的提交。 现在在dev上再将master合并过来的时候，master的提交里已经包含了dev这次的提交，git检测到没有冲突，直接将指针指向了master的提交就可以了。 fast-forward 如果可能，合并分支时Git会使用fast-forward模式 在这种模式下，删除分支时会丢掉分支信息 合并时加上–no-ff参数会禁用fast-forward，这样会多出一个commit id （Merge branch ‘dev’） git merge --no-ff dev 查看log git log --graph 版本回退刚才已经讲解了分支的原理，那么现在再来理解一下版本回退，应该就很容易想到了，无非就是移动分支的指针某到一个提交上。 回退到上一版本 git reset --hard HEAD^这个命令表示回退到HEAD的上一个版本，多一个^就多回退一个版本。 git reset --hard HEAD~2表示回退到HEAD的某一个版本之前，后面的数字就是回退的数量。 git reset --hard commit id 返回到某一个版本 git reflog 如果现在已经在历史的某一个提交上，用git log是看不到之后的commit id的。那么就可以使用git reflog查看，git reflog显示的是操作日志。 12345678910ca07f7b (HEAD -&gt; master) HEAD@&#123;0&#125;: reset: moving to ca07f7b0d2246f HEAD@&#123;1&#125;: reset: moving to HEAD0d2246f HEAD@&#123;2&#125;: reset: moving to HEAD0d2246f HEAD@&#123;3&#125;: reset: moving to HEAD^^3240097 HEAD@&#123;4&#125;: reset: moving to HEAD^ca07f7b (HEAD -&gt; master) HEAD@&#123;5&#125;: commit: add another line3240097 HEAD@&#123;6&#125;: commit: add another lined8f57a9 HEAD@&#123;7&#125;: commit: add a new line0d2246f HEAD@&#123;8&#125;: commit (initial): initial commit(END)]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-源码分析(一)]]></title>
    <url>%2F2017%2F09%2F13%2Fnetty6%2F</url>
    <content type="text"><![CDATA[Netty源码分析跟着代码的编写和运行流程来看看Netty的源码和原理，这里面包含了大量NIO的知识，所以我们要对NIO的基础知识要有掌握，不然看Netty的源码是很难受的。那么我们从哪里开始阅读源码呢，既然不知道从何下手，就从运行的例子一个一个往下看吧。 12345678910111213141516171819public class MyServer &#123; public static void main(String[] args) throws InterruptedException &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .handler(new LoggingHandler(LogLevel.WARN)) .childHandler(new MyServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8899).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; NioEventLoopGroup还记得我们编写Netty服务端的时候，第一行代码就是EventLoopGroup bossGroup = new NioEventLoopGroup();，bossGroup是作为转发、分发任务的，而workerGroup才是真正执行任务的。 EventLoopGroup底层是一个死循环，它会不停的去侦测输入输出的事件进行处理，处理完成后进行任务的执行。 EventLoopGroup有一个参数，表示线程数量，如果不传的话，在MultithreadEventLoopGroup里有设置它的默认值是Math.max(1, SystemPropertyUtil.getInt( &quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2));的计算结果，像bossGroup一般都会设置成1，因为分配任务的group，只需要一个线程就足以。 最后创建线程的代码在MultithreadEventExecutorGroup的构造方法里面。children[i] = newChild(executor, args); 所以NioEventLoopGroup在初始化的时候就只是赋值和初始化属性，什么操作也没有做。 ServerBootstrapBootstrap sub-class which allows easy bootstrap of ServerChannel。意思就是ServerBootstrap就是用来启动ServerChannel的一个类。 group()接下来看ServerBootstrap.group()方法，先把父类AbstractBootstrap的group属性设置了，然后再设置自己的childGroup属性。 1234567891011public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) &#123; super.group(parentGroup); if (childGroup == null) &#123; throw new NullPointerException("childGroup"); &#125; if (this.childGroup != null) &#123; throw new IllegalStateException("childGroup set already"); &#125; this.childGroup = childGroup; return this;&#125; channel()用于通过Class对象创建一个channel对象，源码是通过ReflectiveChannelFactory的反射代码调用无参构造方法创建的对象。实际上是设置了一个channelFactory属性，只有在调用bind()方法的时候才会真正创建对象。 NioServerSocketChannelServerSocketChannel implementation which uses NIO selector based implementation to accept new connections. ServerSocketChannel的实现，使用了基于NIO selector的实现接受连接。 这个就是说NIO使用Selector基于事件的连接是一样的。 childHandler()服务于用channel的请求，实现为赋值到childHandler的属性中。 bind()在这个方法里才真正创建了一个新的channel，并且绑定到上面。 最后调用到doBind方法，在netty中，do开头的基本都是私有方法。 final ChannelFuture regFuture = initAndRegister(); 在initAndRegister()里就调用了channelFactory.newChannel()（ReflectiveChannelFactory）来创建Channel对象。 12345678@Overridepublic T newChannel() &#123; try &#123; return clazz.getConstructor().newInstance(); &#125; catch (Throwable t) &#123; throw new ChannelException("Unable to create Channel from class " + clazz, t); &#125;&#125; 那么我们传入的是NioServerSocketChannel,构造方法就调用到了provider.openServerSocketChannel();，这个就是Java NIO的代码了。 123 public NioServerSocketChannel() &#123; this(newSocket(DEFAULT_SELECTOR_PROVIDER));&#125; newSocket方法还提到了在github上的一个问题https://github.com/netty/netty/issues/2308。这里就出现了Netty调用了NIO的方法，像SelectionKey注册了OP_ACCEPT事件，表示可以接受连接了。并且在父类中调用了`ch.configureBlocking(false);`设置为非阻塞。**AbstractNioChannel**也包含了一个**SelectableChannel**的引用，这个其实就是对NIO的Channel的一个包装就提现出来了。 1234public NioServerSocketChannel(ServerSocketChannel channel) &#123; super(null, channel, SelectionKey.OP_ACCEPT); config = new NioServerSocketChannelConfig(this, javaChannel().socket());&#125; 在获得Channel对象后，马上调用了init方法进行初始化。 12channel = channelFactory.newChannel();init(channel); init方法本身也是做了很多事情，其中有一些option和attr的设置，还有就是Netty在这里也加了一个Handler。 123456789101112131415161718p.addLast(new ChannelInitializer&lt;Channel&gt;() &#123; @Override public void initChannel(final Channel ch) throws Exception &#123; final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) &#123; pipeline.addLast(handler); &#125; ch.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); &#125; &#125;); &#125;&#125;); 这个有没有眼熟，我们自己写的childHanlder()注册的不就是ChannelInitializer的子类吗。还有ChannelHandler handler = config.handler();实际上就是serverBootstrap.handler()注册的自定义的Handler，是提供给bossGroup使用的，在这里被添加到ChannelPipeline的末端。 bind(8899)返回了一个ChannelFuture，ChannelFuture最终是继承了java.util.concurrent.Future，返回Future的都是异步方法，结果只能通过get()方法获取，get()方法是阻塞的，会阻塞直到异步方法运行完成返回结果。 netty自己写了一个Future，并加了几个方法来区别JDK只有isDone的不足，在JDK中，异步任务完成、取消、异常，isDone方法都会返回true，而netty为了更加细化这个状态，做了如下处理。 12345678910111213141516* +---------------------------+* | Completed successfully |* +---------------------------+* +----&gt; isDone() = true |* +--------------------------+ | | isSuccess() = true |* | Uncompleted | | +===========================+* +--------------------------+ | | Completed with failure |* | isDone() = false | | +---------------------------+* | isSuccess() = false |----+----&gt; isDone() = true |* | isCancelled() = false | | | cause() = non-null |* | cause() = null | | +===========================+* +--------------------------+ | | Completed by cancellation |* | +---------------------------+* +----&gt; isDone() = true |* | isCancelled() = true |* +---------------------------+ 除了状态的处理，还添加了addListener方法，这个方法会在任务运行完成的时候通知并回调，所以用户能更加准确的判断何时调用get()方法。 Netty建议我们使用addListener，而不要使用await()，因为addListener是非阻塞的，await()会阻塞直到I/O完成。不要在ChannelHanlder中调用await()方法，因为ChannelHanlder的方法通常是被事件处理器调用的，如果await()被I/O操作线程的事件处理器调用，那么I/O操作就会一直阻塞造成死锁。比如在channelRead()中调用await()方法，等待I/O操作完成，而I/O操作又在等待channelRead()完成，就成了死锁，这种情况应该调用: 123456future.addListener(new ChannelFutureListener() &#123; public void operationComplete(ChannelFuture future) &#123; // Perform post-closure operation // ... &#125;&#125;); Reactor模式Doug Lea反应器模式，Netty整体架构是Reactor模式的完整体现。提到Reactor模式，就不得不拿出大神Doug Lea(Java并发包的作者)的文档：《Scalable IO in Java》，内容不多，里面涉及到传统IO的写法，NIO的设计思想。这个文档非常重要，一定要熟读。 大多数的网络服务都是下面的流程： 读取请求 对请求进行解码 处理服务（业务逻辑） 编码相应 发送响应 传统的网络服务设计方式如上图所示：客户端有多个，服务端每接受到一个请求就创建一个线程进行一系列的处理…. 1234567891011121314151617181920212223242526272829class Server implements Runnable &#123; public void run() &#123; try &#123; ServerSocket ss = new ServerSocket(PORT); while (!Thread.interrupted()) new Thread(new Handler(ss.accept())).start(); // or, single-threaded, or a thread pool &#125; catch (IOException ex) &#123; /* ... */ &#125; &#125; static class Handler implements Runnable &#123; final Socket socket; Handler(Socket s) &#123; socket = s; &#125; public void run() &#123; try &#123; byte[] input = new byte[MAX_INPUT]; socket.getInputStream().read(input); byte[] output = process(input); socket.getOutputStream().write(output); &#125; catch (IOException ex) &#123; /* ... */ &#125; &#125; private byte[] process(byte[] cmd) &#123; /* ... */ &#125; &#125;&#125; 这种方式最大的问题就是线程太多了，如果线程持续上升，线程之间的切换非常耗费资源，服务器就支撑不了。 Reactor通过分发恰当的处理器来响应IO事件（类似于AWT） handler是非阻塞的 通过将handler绑定到事件上来管理（类似于AWT的addActionListener） 客户端保持不变，这里多了一个Reactor角色，它去检测客户端发起的请求和连接，将客户端的请求派发给特定的handler。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Reactor implements Runnable &#123; final Selector selector; final ServerSocketChannel serverSocket; Reactor(int port) throws IOException &#123; selector = Selector.open(); serverSocket = ServerSocketChannel.open(); serverSocket.socket().bind( new InetSocketAddress(port)); serverSocket.configureBlocking(false); SelectionKey sk = serverSocket.register(selector, SelectionKey.OP_ACCEPT); sk.attach(new Acceptor()); &#125; /* Alternatively, use explicit SPI provider: SelectorProvider p = SelectorProvider.provider(); selector = p.openSelector(); serverSocket = p.openServerSocketChannel(); */ public void run() &#123; // normally in a new Thread try &#123; while (!Thread.interrupted()) &#123; selector.select(); Set selected = selector.selectedKeys(); Iterator it = selected.iterator(); while (it.hasNext()) dispatch((SelectionKey) (it.next()); selected.clear(); &#125; &#125; catch (IOException ex) &#123; /* ... */ &#125; &#125; void dispatch(SelectionKey k) &#123; Runnable r = (Runnable) (k.attachment()); if (r != null) r.run(); &#125; class Acceptor implements Runnable &#123; // inner public void run() &#123; try &#123; SocketChannel c = serverSocket.accept(); if (c != null) new Handler(selector, c); catch(IOException ex)&#123; /* ... */ &#125; &#125; &#125; &#125;&#125; sk.attach(obj)可以放一个对象进去，在后面可以用attachment()取出来，这里放进去的是Acceptor。 Reactor本身是不做任何处理的，run()方法里事件发生的时候，调用了dispatch()方法，交由Acceptor来分发，Handler实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546final class Handler implements Runnable &#123; final SocketChannel socket; final SelectionKey sk; ByteBuffer input = ByteBuffer.allocate(MAXIN); ByteBuffer output = ByteBuffer.allocate(MAXOUT); static final int READING = 0, SENDING = 1; int state = READING; Handler(Selector sel, SocketChannel c) throws IOException &#123; socket = c; c.configureBlocking(false); // Optionally try first read now sk = socket.register(sel, 0); sk.attach(this); sk.interestOps(SelectionKey.OP_READ); sel.wakeup(); &#125; boolean inputIsComplete() &#123; /* ... */ &#125; boolean outputIsComplete() &#123; /* ... */ &#125; void process() &#123; /* ... */ &#125; public void run() &#123; try &#123; if (state == READING) read(); else if (state == SENDING) send(); &#125; catch (IOException ex) &#123; /* ... */ &#125; &#125; void read() throws IOException &#123; socket.read(input); if (inputIsComplete()) &#123; process(); state = SENDING; // Normally also do first write now sk.interestOps(SelectionKey.OP_WRITE); &#125; &#125; void send() throws IOException &#123; socket.write(output); if (outputIsComplete()) sk.cancel(); &#125;&#125; 这个handler可以对应到Netty中Netty提供的handler或者我们自己写的handler。因为最早注册的是OP_ACCEPT，所以这个handler要注册OP_READ。其中sel.wakeup();的意思是如果之前有select()方法阻塞了，那么让select()方法立即返回，如果当前没有select()方法阻塞的话，那么下一次调用select()会立即返回。然后执行run()方法，是通过判断状态的方式来决定是写还是读 ，这个在Netty3中就是需要这样实现handler代码的，需要自己判断状态来决定业务逻辑。Netty4已经改成各种回调了，比如channelRead，channelActive等。 文档接着又描述了多线程版本的设计，增加多个Reactor线程，提高Reactor分发的速度，以及使用线程池来处理请求 下图是多个Reactor的图示，这个图对应到Netty我们可以理解为mainReactor对应bossGroup，subReactor对应workerGroup。 reactor-siemens《reactor-siemens》是发布于1995年的论文，。这个理论也现在也没有过时。 这是论文里面的一张图，跟Doug Lea的图实际上是一个意思，虽然名字不一样，下面解释一下这个图里面的元素的意思。 Reactor模式一共有5种角色构成： Handle（句柄或是描述符）：本质上表示一种资源，是由操作系统提供的；该资源用于表示一个个的事件，比如说文件描述符，针对网络编程中的Socket描述符。事件既可以来自于外部，也可以来自于内部；外部事件比如说客户端的连接请求，客户端发送过来的数据等；内部事件比如说操作系统产生的定时器事件等。它本质上就是一个文件描述符。Handle是事件产生的发源地。 Synchronous Event Demultiplexer（同步事件分离器）：它本身是一个系统调用，用于等待事件的发生（事件可能是一个，也可能是多个）。调用方在调用它的时候以后会被阻塞，一直阻塞到同步事件分离器上有事件产生为止。对于Linux来说，同步事件分离器指的就是常用的I/O多路复用的机制，比如说select，poll，epoll等。在Java NIO领域中，同步事件分离器对应的组件就是Selector；对应的阻塞方法就是select方法。 Event Hanlder（事件处理器）：本身由多个回调方法构成，这些回调方法构成了与应用相关的对于某个事件的反馈机制。Netty相比于Java NIO来说，在事件处理器这个角色上进行了一个升级，它为我们开发者提供了大量的回调方法，供我们在特定事件产生的时候实现相应的业务方法进行业务逻辑的处理。 Contrete Event Handler（具体事件处理器）：是事件处理器的实现。它本身实现了事件处理器所提供的各个回调方法，从而实现了特定于业务的逻辑。它本质上就是我们所编写的一个个的处理器实现。 Initiation Dispatcher（初始分发器）：它实际上就是Reactor角色。它本身定义了一些规范，这些规范用于控制事件的调度方式，同时又提供了应用进行事件处理器的注册、删除、等设施。它本身是整个时间处理器的核心所在，Initiation Dispatcher会通过同步事件分离器来等待时间的发生。一旦事件发生，Initiation Dispatcher首先会分离出每一个事件，然后调用事件处理器，最后调用相关的回调方法来处理这些事件。 Reactor模式的流程： 当应用向Initiation Dispatcher注册具体的事件处理器时，应用会标识出该事件处理器希望Initiation Dispatcher在某个事件发生时向其通知的该事件，该事件与Handle关联。 Initiation Dispatcher会要求每个事件处理器向其传递内部的Handle。该Handle向操作系统标识了事件处理器。 当所有的事件处理器注册完毕后，应用汇调用handle_events方法来启动Initiation Dispatcher的事件循环。这时，Initiation Dispatcher会将每个注册的事件管理器的Handle合并起来，并使用同步事件分离器等待这些事件的发生。比如说，TCP协议层会使用select同步事件分离器操作来等待客户端发送的数据到达已经连接的socket handle上。 当与某个事件源对应的Handle变为ready状态时（比如说，TCP socket变为等待读状态时），同步事件分离器就会通知Initiation Dispatcher。 Initiation Dispatcher会触发事件处理器的回调方法，从而相应这个处于ready状态的Handle。当事件发生时，Initiation Dispatcher会将事件源激活的Handle作为[key]来寻找并分发恰当的事件处理器回调方法。 Initiation Dispatcher会回调事件处理器的handle_events回调方法来执行特定于应用的功能（开发者自己所编写的功能），从而响应这个事件。所发生的事件类型可以作为该方法参数并被该方法内部使用来执行额外的特定于服务的分离与分发。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NIO-零拷贝]]></title>
    <url>%2F2017%2F09%2F12%2Fnio7%2F</url>
    <content type="text"><![CDATA[NIO 零拷贝OIO在读取文件传输的时候，在操作系统中发生了多次上下文的切换和多次的数据拷贝。 BIO在读取文件的流程图 NIO零拷贝读取文件的流程图，实际上这个也有一次拷贝，就是从kernel内核空间拷贝到socket buffer中。 下面再写个零拷贝的例子深刻理解一下，首先是OIO的方式。 OldServer12345678910111213141516171819202122232425262728293031323334import java.io.DataInputStream;import java.io.IOException;import java.net.ServerSocket;import java.net.Socket;/** * @author yangfan * @date 2017/09/12 */public class OldServer &#123; public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(8899); while (true) &#123; Socket socket = serverSocket.accept(); DataInputStream dataInputStream = new DataInputStream(socket.getInputStream()); try &#123; byte[] byteArry = new byte[4096]; while (true) &#123; int readCount = dataInputStream.read(byteArry, 0, byteArry.length); if (-1 == readCount) &#123; break; &#125; &#125; &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125; &#125;&#125; OldClient12345678910111213141516171819202122232425262728293031323334353637383940import java.io.DataOutputStream;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;import java.net.Socket;/** * @author yangfan * @date 2017/09/12 */public class OldClient &#123; public static void main(String[] args) throws IOException &#123; Socket socket = new Socket("localhost", 8899); // 这里选择一个比较大一点的文件，否则看不出来效果 String fileName = "/Users/xiaomai/software/工具/CleanMyMac 3.5.1.dmg"; InputStream inputStream = new FileInputStream(fileName); DataOutputStream dataOutputStream = new DataOutputStream(socket.getOutputStream()); byte[] buffer = new byte[4096]; long readCount; long total = 0; long startTime = System.currentTimeMillis(); while ((readCount = inputStream.read(buffer)) &gt;=0) &#123; total += readCount; dataOutputStream.write(buffer); &#125; System.out.println("发送总字节数：" + total + "，耗时：" + (System.currentTimeMillis() - startTime)); inputStream.close(); dataOutputStream.close(); &#125;&#125; 下面是NIO的方式： NewIOServer123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.io.IOException;import java.net.InetSocketAddress;import java.net.ServerSocket;import java.nio.ByteBuffer;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;/** * @author yangfan * @date 2017/09/12 */public class NewIOServer &#123; public static void main(String[] args) throws IOException &#123; InetSocketAddress address = new InetSocketAddress(8899); ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); ServerSocket serverSocket = serverSocketChannel.socket(); // 复用客户端连接的端口号，断开后保留一段时间 serverSocket.setReuseAddress(true); serverSocket.bind(address); ByteBuffer byteBuffer = ByteBuffer.allocate(4096); while (true) &#123; SocketChannel socketChannel = serverSocketChannel.accept(); socketChannel.configureBlocking(true); int readCount = 0; while (-1 != readCount) &#123; try &#123; readCount = socketChannel.read(byteBuffer); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; // 循环读取后将position设置为0 byteBuffer.rewind(); &#125; &#125; &#125;&#125; NewIOClient123456789101112131415161718192021222324252627/** * @author yangfan * @date 2017/09/13 */public class NewIOClient &#123; public static void main(String[] args) throws Exception &#123; SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress("localhost", 8899)); socketChannel.configureBlocking(true); // 这里选择一个比较大一点的文件，否则看不出来效果 String fileName = "/Users/xiaomai/software/工具/CleanMyMac 3.5.1.dmg"; FileChannel fileChannel = new FileInputStream(fileName).getChannel(); long startTime = System.currentTimeMillis(); // 一行代码解决传输 // 这行代码是用的零拷贝的方式来进行的文件传输，交给操作系统直接将文件系统缓存传输给了目标Channel // 而没有拷贝进用户空间的过程 long transferCount = fileChannel.transferTo(0, fileChannel.size(), socketChannel); System.out.println("发送总字节数：" + transferCount + ", 耗时：" + (System.currentTimeMillis() - startTime)); fileChannel.close(); &#125;&#125; 通过简单的测试我们也可以发现，NIO方式的速度确实快了很多。 从Linux2.4开始，还提供了scatter/gather的方式使速度更上一层，实现了真正意义上的零拷贝。 将文件拷贝到kernel缓冲区后，只将地址和长度等必要信息拷贝到socket buffer中，等到要发送文件的时候，从socket buffer中读取文件长度和地址，从kernel中读取真正的文件，这是一种gather操作，然后把数据直接发送到了服务器端，跟之前的对比，省去了从kernel内核空间拷贝到socket buffer的过程。]]></content>
      <categories>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-简介基本操作]]></title>
    <url>%2F2017%2F08%2F28%2Fgit1%2F</url>
    <content type="text"><![CDATA[用git已经差不多3年了，回顾一下。 https://git-scm.com/ 这个是Git的官网，Git是一款免费、开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。 Git的读音为/gɪt/。 Git是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。 Torvalds 开始着手开发 Git 是为了作为一种过渡方案来替代 BitKeeper，后者之前一直是 Linux 内核开发人员在全球使用的主要源代码工具。开放源码社区中的有些人觉得BitKeeper 的许可证并不适合开放源码社区的工作，因此 Torvalds 决定着手研究许可证更为灵活的版本控制系统。尽管最初 Git 的开发是为了辅助 Linux 内核开发的过程，但是我们已经发现在很多其他自由软件项目中也使用了 Git。例如 很多 Freedesktop 的项目迁移到了 Git 上。 githubgithub是一个面向开源及私有软件项目的托管平台，因为只支持git 作为唯一的版本库格式进行托管，故名github。github于2008年4月10日正式上线，除了git代码仓库托管及基本的 Web管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。目前，其注册用户已经超过350万，托管版本数量也是非常之多，其中不乏知名开源项目 Ruby on Rails、jQuery、python 等。 gitlabGitLab是一个利用Ruby on Rails开发的开源应用程序，实现一个自托管的Git项目仓库，可通过Web界面进行访问公开的或者私人项目。它拥有与GitHub类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序（Wall）进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。GitLab要求服务器端采用Gitolite搭建（为了方便安装，现已经用gitlab-shell代替Gitolite） Git常用命令 获得版本库 git init git clone 版本管理 git add git commit git rm 查看信息 git help git log git diff 远程协作 git pull git push 执行git init，初始化一个空的git仓库，目录下会生成一个.git目录，用ls -al可查看。 创建一个文件：touch test.txt git status执行git status 1234567891011➜ mygit git:(master) ✗ git statusOn branch masterInitial commitUntracked files: (use "git add &lt;file&gt;..." to include in what will be committed) test.txtnothing added to commit but untracked files present (use "git add" to track) 表示当前处于master分支上，并且还没有提交过，test.txt没有被追踪（意思test.txt没有纳入到git的版本控制系统中），然后执行git add test.txt加入到暂存区中。 git add12345678910➜ mygit git:(master) ✗ git add test.txt ➜ mygit git:(master) ✗ git status On branch masterInitial commitChanges to be committed: (use "git rm --cached &lt;file&gt;..." to unstage) new file: test.txt git rm –cached现在test.txt文件就已经被添加到git暂存区了，可以被提交。根据上面的输出提示，我们可以执行git rm --cached &lt;file&gt;将文件还原到被修改的状态，现在来试一下。 12345678910111213➜ mygit git:(master) ✗ git rm --cached test.txt rm 'test.txt'➜ mygit git:(master) ✗ git status On branch masterInitial commitUntracked files: (use "git add &lt;file&gt;..." to include in what will be committed) test.txtnothing added to commit but untracked files present (use "git add" to track) 现在文件又从暂存区恢复到了被修改的状态，接下来我们添加后来测试一下git的提交,git的commit是要求提交必须填写注释的。 git commit1mygit git:(master) ✗ git commit 123456789# Please enter the commit message for your changes. Lines starting# with &apos;#&apos; will be ignored, and an empty message aborts the commit.# On branch master## Initial commit## Changes to be committed:# new file: test.txt# 这里出来一个vi的界面填写注释，如果直接保存退出会终止提交并提示以下内容。 12➜ mygit git:(master) ✗ git commitAborting commit due to empty commit message. 现在输入注释后进行提交： 1234567➜ mygit git:(master) ✗ git commit[master (root-commit) db19d2d] initial commit 1 file changed, 1 insertion(+) create mode 100644 test.txt➜ mygit git:(master) git status On branch masternothing to commit, working tree clean 还有一种更加简便的方式：将当前目录下有修改的文件进行add操作并提交：git commit -am &#39;add another line&#39; git log如果要查看之前的提交，可以用git log命令进行查看。 1234567➜ mygit git:(master) git logcommit db19d2de7917bc55dc8a7d05545e12f7e00c8325 (HEAD -&gt; master)Author: yangfan &lt;hyyangfan@gmail.com&gt;Date: Fri Sep 15 10:58:10 2017 +0800 initial commit(END) git configGit的提交id（commit id）是一个摘要值，这个摘要值实际上是个sha1计算出来的。 对于user.name和user.email来说，有3个地方可以设置 /etc/gitconfig (几乎不会使用) git config --system ~/.gitconfig (很常用) git config --global 针对于特定项目的，.git/config文件中 git config --local 1234567➜ mygit git:(master) git configusage: git config [&lt;options&gt;]Config file location --global use global config file --system use system config file --local use repository config file 设置一下: 1234567891011121314➜ .git git:(master) git config --local user.name '帆'➜ .git git:(master) git config --local user.email 'test@test.com'➜ .git git:(master) git config --listuser.name=yangfanuser.email=hyyangfan@gmail.comcommit.template=/Users/xiaomai/.stCommitMsgcore.repositoryformatversion=0core.filemode=truecore.bare=falsecore.logallrefupdates=truecore.ignorecase=truecore.precomposeunicode=trueuser.name=帆user.email=test@test.com git config --list展示了设置的属性，现在再来看.git/config文件，就发生了变化。 1234567891011➜ .git git:(master) cat config[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[user] name = 帆 email = test@test.com 现在再改一下test.txt文件： git checkout1234567891011121314➜ mygit git:(master) vi test.txt ➜ mygit git:(master) ✗ git status On branch masterChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: test.txtno changes added to commit (use "git add" and/or "git commit -a")➜ mygit git:(master) ✗ git checkout -- test.txt ➜ mygit git:(master) git status On branch masternothing to commit, working tree clean git checkout -- test.txt可以恢复文件的修改，这个命令要慎用，没有提交的话用了这个命令修改的找不回来了。 12345➜ mygit git:(master) vi test.txt ➜ mygit git:(master) ✗ git add test.txt ➜ mygit git:(master) ✗ git commit -m "second commit"[master 112c95f] second commit 1 file changed, 1 insertion(+) git commit -m 后面的-m是填写注释的简便方式。 现在多提交了一次，查看一下提交日志。 12345678910111213➜ mygit git:(master) git logcommit 112c95fdbf9e8361e68b99d8bc9f658923461dc1 (HEAD -&gt; master)Author: 帆 &lt;test@test.com&gt;Date: Fri Sep 15 11:30:05 2017 +0800 second commitcommit db19d2de7917bc55dc8a7d05545e12f7e00c8325Author: yangfan &lt;hyyangfan@gmail.com&gt;Date: Fri Sep 15 10:58:10 2017 +0800 initial commit(END) git rm现在新创建一个test2.txt并提交，那我们将这个文件从已经提交的库里删除应该怎么做呢？ 1234567891011➜ mygit git:(master) git rm test2.txt rm 'test2.txt'➜ mygit git:(master) ✗ git statusOn branch masterChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) deleted: test2.txt➜ mygit git:(master) ✗ lstest.txt 执行上面的命令，我们可以看到，不光是git status显示文件已经被删除，ls命令也看不到了。 1234567891011121314151617181920➜ mygit git:(master) ✗ git reset HEAD test2.txtUnstaged changes after reset:D test2.txt➜ mygit git:(master) ✗ git statusOn branch masterChanges not staged for commit: (use "git add/rm &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) deleted: test2.txtno changes added to commit (use "git add" and/or "git commit -a")➜ mygit git:(master) ✗ lstest.txt➜ mygit git:(master) ✗ git checkout -- test2.txt➜ mygit git:(master) git statusOn branch masternothing to commit, working tree clean➜ mygit git:(master) lstest.txt test2.txt 上面又测试一系列命令，用reset和checkout命令恢复了删除的文件。 下面进行真正的删除，并提交。 123456789101112131415➜ mygit git:(master) git rm test2.txt rm 'test2.txt'➜ mygit git:(master) ✗ git statusOn branch masterChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) deleted: test2.txt➜ mygit git:(master) ✗ git commit -m "delete test2.txt"[master 5f69c8b] delete test2.txt 1 file changed, 1 deletion(-) delete mode 100644 test2.txt➜ mygit git:(master) ls test.txt 再演示系统使用系统命令rm删除文件的情况，不再需要调用reset就可以用checkout恢复。 123456789101112131415161718192021222324➜ mygit git:(master) echo 'hello world' &gt; test2.txt➜ mygit git:(master) ✗ lstest.txt test2.txt➜ mygit git:(master) ✗ git add test2.txt ➜ mygit git:(master) ✗ git commit -m 'create a file'[master 1a7fc91] create a file 1 file changed, 1 insertion(+) create mode 100644 test2.txt➜ mygit git:(master) rm test2.txt ➜ mygit git:(master) ✗ git statusOn branch masterChanges not staged for commit: (use "git add/rm &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) deleted: test2.txtno changes added to commit (use "git add" and/or "git commit -a")➜ mygit git:(master) ✗ git checkout -- test2.txt➜ mygit git:(master) git statusOn branch masternothing to commit, working tree clean➜ mygit git:(master) lstest.txt test2.txt 不过也可以由此看出，使用git rm，可以直接提交删除文件。而使用系统命令rm，需要调用git add将删除文件的动作纳入到暂存区才能提交。 git rm： 删除了一个文件 将被删除的文件纳入到暂存区（stage，index） 若想恢复被删除的文件，需要进行两个动作： git reset HEAD test2.txt，将待删除的问价你从暂存区恢复到工作区 git checkout -- test2.txt，将工作区中的修改丢弃掉 rm： 将test2.txt删除，这时被删除的文件并未纳入暂存区中。 git mv重命名 1234567891011➜ mygit git:(master) git mv test.txt test2.txt➜ mygit git:(master) ✗ git statusOn branch masterChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) renamed: test.txt -&gt; test2.txt➜ mygit git:(master) ✗ git commit -m 'rename a file'[master 7030bca] rename a file 1 file changed, 0 insertions(+), 0 deletions(-) rename test.txt =&gt; test2.txt (100%) git add .如果一次修改了多个文件，可以使用git add .命令将所有修改纳入到暂存区。 git commit –amend如果不小心写错了提交消息怎么办？ 123456789101112➜ mygit git:(master) ✗ git commit -m "不小心写错了这条消息"On branch masterChanges not staged for commit: modified: test2.txtno changes added to commit➜ mygit git:(master) ✗ git commit --amend -m '再次修正'[master 021a9de] 再次修正 Date: Wed Sep 20 16:48:51 2017 +0800 2 files changed, 1 insertion(+), 2 deletions(-) delete mode 100644 test.txt➜ mygit git:(master) ✗ git log]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 编码和解码]]></title>
    <url>%2F2017%2F08%2F27%2Fnio6%2F</url>
    <content type="text"><![CDATA[做过web开发的人一定遇到过中文乱码的问题，一般情况下我们都是直接把所有的编码都设置成UTF-8，可是并没有真正的去理解为什么会导致乱码的问题，这里一起研究一下编码和解码究竟是怎么一回事。 这段代码很简单，就是读取一个文件的内容，然后输出到另外一个文件里面。 12345678910111213141516171819202122232425262728293031323334353637/** * Java的编解码 * @author yangfan * @date 2017/08/27 */public class NioTest13 &#123; public static void main(String[] args) throws IOException &#123; String inputFile = "NioTest13_In.txt"; String outputFile = "NioTest13_Out.txt"; // 把一个文件拷贝到另一个文件 RandomAccessFile inputRandomAccessFile = new RandomAccessFile(inputFile, "r"); RandomAccessFile outputRandomAccessFile = new RandomAccessFile(outputFile, "rw"); long inputLength = new File(inputFile).length(); FileChannel inputFileChannel = inputRandomAccessFile.getChannel(); FileChannel outputFileChannel = outputRandomAccessFile.getChannel(); MappedByteBuffer inputData = inputFileChannel.map(FileChannel.MapMode.READ_ONLY, 0, inputLength); Charset charset = Charset.forName("utf-8"); CharsetDecoder decoder = charset.newDecoder(); CharsetEncoder encoder = charset.newEncoder(); CharBuffer charBuffer = decoder.decode(inputData); ByteBuffer outputData = encoder.encode(charBuffer); outputFileChannel.write(outputData); inputRandomAccessFile.close(); outputRandomAccessFile.close(); &#125;&#125; 但这里有很关键的几行代码。 123Charset charset = Charset.forName("utf-8");CharsetDecoder decoder = charset.newDecoder();CharsetEncoder encoder = charset.newEncoder(); NioTest13_In.txt读取的内容为： 12hello,world你好 程序执行后，得到了正确的结果。 无论是用程序还是用工具保存的文件，最后都是以字节的形式保存在硬盘上的，那么在保存的时候，是有一个字符编码。我们可以用一些工具查看某个文件的编码是什么，比如mac上有一个叫enca的工具，可以执行brew install enca进行安装。 12enca NioTest13_In.txt Universal transformation format 8 bits; UTF-8 我们之前的代码Charset.forName(&quot;utf-8&quot;);表示用utf8来对文件编解码，读取的文件和输出的文件，都是utf8格式的。 现在对代码进行一点改动，把编码格式改成Charset charset = Charset.forName(&quot;iso-8859-1&quot;);,查看输出的文件内容，发现竟然没有乱码，而且用工具查看编码格式依然是utf8，这是怎么回事呢? 下面分别解释一下我们常见的几种编码: ASCII (American Standard Code for Information Interchange, 美国信息交换标准代码) 7 bit表示一个字符，共计可以表示128种字符。 IOS-8859-1，基于ASCII的扩展 8 bit表示一个字符，即用一个字节（byte）(8 bit)来表示一个字符，共计可以表示256个字符。 gb2312，常见汉字编码 2个字节表示1个汉字 GBK，包括生僻字的编码 GB18030，最完整的汉字编码 BIG5，台湾繁体字编码 unicode，包括全球所有编码的字符节。 采用2个字节表示一个字符。 utf-Unicode Translation Format. 因为unicode太占存储空间了，所以又诞生了utf8,utf16等格式，unicode本身是一种编码方式，而utf是一种存储方式；utf-8是unicode的实现方式之一。 utf-16LE(little endian), utf16-be(big endian) 大端和小端：unicode编码的文件开头有一个Zero Width No-break Space。如果是0xFEFF (BE), 0xFFFE(LE)，表示大端还是小端。 utf-8，变长字节表示形式，一般来说，utf-8会通过3个字节来表示一个中文。 utf8有个概念：BOM(Byte Order Mark)，就是文件头，在windows里用16进制打开文件能看到，但是mac和linux里是没有的。 那么刚才用ios-8859-1来读取，读取英文的部分没有问题，那么为什么中文也是正确的呢？你好，因为utf8是用3个字节表示一个中文，这2个字的字节表示为E4 BD A0 E5 A5 BD。那么用ios-8859-1解码的时候，先读取一个字节E4，这个肯定是乱码，但是在解码完成后，再进行编码的时候完整了放入了6个字节的数据，而输出的文件又是utf-8格式，所以我们查看文件的时候，又把字节进行了正确的解析。 如果我们解码和编码用的不同的字符集，那么肯定就会出错了。]]></content>
      <categories>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO 模拟聊天]]></title>
    <url>%2F2017%2F08%2F27%2Fnio5%2F</url>
    <content type="text"><![CDATA[接下来看看如何用NIO来做一个模拟聊天，通过这个例子来更加理解NIO的使用方式，下面展示了服务端代码，用命令进行测试，然后再用NIO的方式编写客户端进行测试，对NIO的服务端和客户端的开发，都更加深入的理解。 服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/** * 模拟聊天 * * @author yangfan * @date 2017/08/27 */public class NioServer &#123; /** * 保存客户端连接 */ private static Map&lt;String, SocketChannel&gt; clientMap = new HashMap&lt;&gt;(); public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); ServerSocket serverSocket = serverSocketChannel.socket(); serverSocket.bind(new InetSocketAddress(8899)); Selector selector = Selector.open(); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; try &#123; int nums = selector.select(); // 拿到触发OP_ACCEPT事件的SelectionKey集合，访问对应通道 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); selectionKeys.forEach(selectionKey -&gt; &#123; final SocketChannel client; try &#123; // 通道已连接 if (selectionKey.isAcceptable()) &#123; ServerSocketChannel server = (ServerSocketChannel) selectionKey.channel(); client = server.accept(); client.configureBlocking(false); // 连接后注册读取事件 client.register(selector, SelectionKey.OP_READ); // 每次有客户端连接后，生成一个ID并放入map中 String key = "[" + UUID.randomUUID().toString() + "]"; clientMap.put(key, client); &#125; // 通道有数据写入 else if (selectionKey.isReadable()) &#123; // 已经变成SocketChannel client = (SocketChannel) selectionKey.channel(); ByteBuffer readBuffer = ByteBuffer.allocate(1024); int count = client.read(readBuffer); String senderKey = null; for (Map.Entry&lt;String, SocketChannel&gt; entry : clientMap.entrySet()) &#123; if (client == entry.getValue()) &#123; senderKey = entry.getKey(); break; &#125; &#125; if (count &gt; 0) &#123; readBuffer.flip(); Charset charset = Charset.forName("utf-8"); String receivedMessage = String.valueOf(charset.decode(readBuffer).array()); System.out.println(client + ":" + receivedMessage); for (Map.Entry&lt;String, SocketChannel&gt; entry : clientMap.entrySet()) &#123; SocketChannel value = entry.getValue(); ByteBuffer writeBuffer = ByteBuffer.allocate(1024); writeBuffer.put((senderKey + ": " + receivedMessage).getBytes()); writeBuffer.flip(); value.write(writeBuffer); &#125; if (receivedMessage.equals("1")) &#123; selectionKeys.clear(); &#125; &#125; else if (count == -1) &#123; // 判断是否客户端断开了连接 clientMap.remove(senderKey); &#125; &#125; &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125;); // 每次读取完成状态后必须清除，因为每一个selectionKey的channel只能被读取一次 selectionKeys.clear(); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125; &#125;&#125; 这部分代码基本上NIO开发的模板式代码，服务端启动代码。 1234ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.configureBlocking(false);ServerSocket serverSocket = serverSocketChannel.socket();serverSocket.bind(new InetSocketAddress(8899)); open()一个Selector后，channel调用register将自己注册到selector上，并传入SelectionKey.OP_ACCEPT表示等待连接。 接下来selector.select();会阻塞，直到有客户端连接，程序才会继续往下走，selector.selectedKeys()返回有状态变化可以被使用的keys，每一个判断分支后对应的channel可以强转为对应的Channel。 比如代码中注册为OP_ACCEPT的是ServerSocketChannel，而注册为OP_READ的是一个SocketChannel。最后不要忘记将selectedKeys清空，否则下次循环进入，遗留下来的selectKey.channel()是获取不到对应的Channel的。 用命令先测试一下，结果如下，一方发送一条消息后，都收到了消息输出，并且带上了连接的时候生成的客户端ID。 客户端NIO客户端的开发，代码跟服务端的差不多，只是由ServerSocketChannel，换成了SocketChannel。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * NIO客户端 * * @author yangfan * @date 2017/08/27 */public class NioClient &#123; public static void main(String[] args) throws IOException &#123; try &#123; SocketChannel socketChannel = SocketChannel.open(); socketChannel.configureBlocking(false); Selector selector = Selector.open(); socketChannel.register(selector, SelectionKey.OP_CONNECT); socketChannel.connect(new InetSocketAddress("localhost", 8899)); while (true) &#123; selector.select(); Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); selectionKeys.forEach(selectionKey -&gt; &#123; try &#123; // 表示已经与服务端建立连接 if (selectionKey.isConnectable()) &#123; SocketChannel client = (SocketChannel) selectionKey.channel(); if (client.isConnectionPending()) &#123; client.finishConnect(); ByteBuffer writeBuffer = ByteBuffer.allocate(1024); writeBuffer.put((LocalDateTime.now() + " 连接成功").getBytes()); writeBuffer.flip(); client.write(writeBuffer); ExecutorService executorService = Executors.newSingleThreadExecutor(Executors.defaultThreadFactory()); executorService.submit(() -&gt; &#123; while (true) &#123; writeBuffer.clear(); InputStreamReader input = new InputStreamReader(System.in); BufferedReader br = new BufferedReader(input); String sendMessage = br.readLine(); writeBuffer.put(sendMessage.getBytes()); writeBuffer.flip(); client.write(writeBuffer); &#125; &#125;); &#125; // 发送完数据后，向selector注册读取事件，等待服务器的返回结果 client.register(selector, SelectionKey.OP_READ); &#125; else if (selectionKey.isReadable()) &#123; SocketChannel client = (SocketChannel) selectionKey.channel(); ByteBuffer readBuffer = ByteBuffer.allocate(1024); int count = client.read(readBuffer); if (count &gt; 0) &#123; String receivedMessage = new String(readBuffer.array(), 0, count); System.out.println(receivedMessage); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); selectionKeys.clear(); &#125; &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125;&#125; 启动客户端后输出： mac用户会看到后面有很多框框，感觉有点奇怪，是哪里出问题了呢？ 单独用一个例子来说明： 12345678910111213141516public class ChartSetTest &#123; public static void main(String[] args) &#123; ByteBuffer buffer = ByteBuffer.allocate(512); final byte[] msg = "中文".getBytes(); buffer.put(msg); buffer.flip(); System.out.println(new String(msg)); System.out.println(String.valueOf(StandardCharsets.UTF_8.decode(buffer).array())); &#125;&#125; 在这个例子中，真相就是buffer的数组在这里是6个字节，decode转换成中文以后，数组里就只有2个元素了，但是长度还是6，还有4个\u0000占位，可以在debug的时候看出来，这也就解释了前面的输出为什么会是那样了。]]></content>
      <categories>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO Selector]]></title>
    <url>%2F2017%2F08%2F23%2Fnio4%2F</url>
    <content type="text"><![CDATA[参考：http://blog.csdn.net/lianjiww/article/details/53540145 SelectorNIO的三大组件Channel、Buffer、Selector已经讲了2个了，接着我们一起探索一下Selector。 Selector（选择器）是Java NIO的一个组件，能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。仅用单个线程来处理多个Channels的好处是，只需要更少的线程来处理通道。对于操作系统来说，线程之间上下文切换的开销很大，而且每个线程都要占用系统的一些资源。因此，使用的线程越少越好。 Selector的创建通过调用Selector.open()方法创建一个Selector，例如： 1Selector selector = Selector.open(); Selector里持有3个集合: key set 注册在Selector的所有通道。 selected-key set 准备好进行操作的通道。 cancelled-key set 准备取消的通道。 注册通道通过SelectableChannel.register()方法来注册，例如： 12channel.configureBlocking(false);SelectionKey key = channel.register(selector, Selectionkey.OP_READ); register()方法会返回一个SelectionKey 对象，这个对象代表了注册到该Selector的通道。与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。register()方法的第二个参数是一个“interest集合”，意思是在通过Selector监听Channel时对什么事件感兴趣。可以监听四种不同类型的事件，这四种事件用SelectionKey的四个常量来表示： Connect事件-SelectionKey.OP_CONNECT Accept事件-SelectionKey.OP_ACCEPT Read事件-SelectionKey.OP_READ Write事件-SelectionKey.OP_WRITE SelectionKeyregister()方法返回的是一个SelectionKey对象，它包含了以下属性： interest集合interest集合是你所选择的感兴趣的事件集合。用“位与”操作interest 集合和给定的SelectionKey常量，可以确定某个给定的事件是否在interest 集合中，例如： 12int interestSet = selectionKey.interestOps();boolean isInterestedInAccept = (interestSet &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT; ready集合ready集合是通道已经准备就绪的操作的集合。从SelectionKey访问ready集合： 1int readySet = selectionKey.readyOps(); 可以用“位与”，来检测channel中什么事件或操作已经就绪，也可以使用以下四个方法，它们都会返回一个布尔类型： 1234selectionKey.isAcceptable();selectionKey.isConnectable();selectionKey.isReadable();selectionKey.isWritable(); Channel从SelectionKey访问Channel： 1Channel channel = selectionKey.channel(); Selector从SelectionKey访问Selector： 1Selector selector = selectionKey.selector(); 附加的对象可以将一个对象或者更多信息附加到SelectionKey上，这样就能方便的识别某个给定的通道。例如： 12selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在向Selector注册Channel的时候附加对象，例如： 1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 选择通道向Selector注册了通道以后，可以用select()方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。select()方法有几个重载： int select()阻塞到至少有一个通道在你注册的事件上就绪了。 int select(long timeout)和select()一样，除了最长会阻塞timeout毫秒。 int selectNow()不会阻塞，不管什么通道就绪都立刻返回。如果自从前一次选择操作后，没有通道变成可选择的，则返回零。 select()方法返回的int值表示自上次调用select()方法后有多少通道已经就绪。调用了select()方法，并且返回值表明有一个或更多个通道就绪了，然后可以调用selectedKeys()方法，访问就绪通道，例如： 1Set selectedKeys = selector.selectedKeys(); 实战12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * @author yangfan * @date 2017/08/23 */public class NioTest12 &#123; public static void main(String[] args) throws IOException &#123; int[] ports = new int[5]; ports[0] = 5000; ports[1] = 5001; ports[2] = 5002; ports[3] = 5003; ports[4] = 5004; Selector selector = Selector.open(); for (int i = 0; i &lt; ports.length; i++) &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 配置成非阻塞 serverSocketChannel.configureBlocking(false); ServerSocket serverSocket = serverSocketChannel.socket(); InetSocketAddress address = new InetSocketAddress(ports[i]); serverSocket.bind(address); // register()会触发一个SelectionKey被添加到Selector的keys中，可以用selector.keys()查看。 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); // System.out.println(selector.keys().size()); System.out.println("监听端口： " + ports[i]); &#125; while (true) &#123; int numbers = selector.select(); System.out.println("numbers: " + numbers); // 准备好的通道（有客户端连接了） Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iter = selectionKeys.iterator(); // 准备建立连接 while (iter.hasNext()) &#123; SelectionKey selectionKey = iter.next(); if (selectionKey.isAcceptable()) &#123; ServerSocketChannel serverSocketChannel = (ServerSocketChannel) selectionKey.channel(); SocketChannel socketChannel = serverSocketChannel.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector, SelectionKey.OP_READ); iter.remove(); System.out.println("获得客户端连接： " + socketChannel); &#125; else if (selectionKey.isReadable()) &#123; // 接收数据 SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); int bytesRead = 0; while (true) &#123; ByteBuffer byteBuffer = ByteBuffer.allocate(512); byteBuffer.clear(); int read = socketChannel.read(byteBuffer); if (read &lt;= 0) &#123; break; &#125; byteBuffer.flip(); socketChannel.write(byteBuffer); bytesRead += read; &#125; System.out.println("读取： " + bytesRead + ", 来自于: " + socketChannel); iter.remove(); &#125; &#125; &#125; &#125;&#125; 上面的例子可以看出来，所有的操作都是主动或者被动的对SelectionKey的OP状态改变进行的操作，所以异步编程里面事件无处不在。]]></content>
      <categories>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO DirectBuffer详解]]></title>
    <url>%2F2017%2F08%2F17%2Fnio3%2F</url>
    <content type="text"><![CDATA[DirectBuffer之前我们用ByteBuffer.allocate()看一下源码： 12345public static ByteBuffer allocate(int capacity) &#123; if (capacity &lt; 0) throw new IllegalArgumentException(); return new HeapByteBuffer(capacity, capacity);&#125; HeapByteBuffer是从堆上分配的内存空间创建的Buffer，实际上JDK还提供了另外一种方式ByteBuffer.allocateDirect()： 123public static ByteBuffer allocateDirect(int capacity) &#123; return new DirectByteBuffer(capacity);&#125; DirectByteBuffer创建的buffer是从直接内存中开辟的空间分配，我们叫做堆外内存，不会被gc回收，里面用到了很多没有开源的sun的api。 new DirectByteBuffer()，这个对象本身是在堆上创建的，但是源码里的base = unsafe.allocateMemory(size);则是在堆外内存中分配的，那么java堆上的数据是如何找到堆外的数据的呢，一定是保存了一个地址，找了一下发现如下变量： Buffer.java 123// Used only by direct buffers// NOTE: hoisted here for speed in JNI GetDirectBufferAddresslong address; 说放在Buffer这个类里是为了效率。 零拷贝如果使用HeapByteBuffer在进行文件读写的时候，所有的数据都在Java堆上，然而操作系统不是直接处理堆上的数据，而是把堆上的数据拷贝到操作系统里（Java内存模型之外）某一块内存空间中，然后再把数据和IO设备进行交互。意思用HeapByteBuffer进行IO操作的时候中间多了一次数据拷贝的过程。 而使用DirectByteBuffer，因为数据本来就在堆外内存中，所以跟IO设备交互的时候没有拷贝的过程，提升了效率，这有一个专有名词，也就是零拷贝。 以下内容转自知乎： DirectByteBuffer 自身是一个Java对象，在Java堆中；而这个对象中有个long类型字段address，记录着一块调用 malloc() 申请到的native memory。 HotSpot VM里的GC除了CMS之外都是要移动对象的，是所谓“compacting GC”。 如果要把一个Java里的 byte[] 对象的引用传给native代码，让native代码直接访问数组的内容的话，就必须要保证native代码在访问的时候这个 byte[] 对象不能被移动，也就是要被“pin”（钉）住。 可惜HotSpot VM出于一些取舍而决定不实现单个对象层面的object pinning，要pin的话就得暂时禁用GC——也就等于把整个Java堆都给pin住。HotSpot VM对JNI的Critical系API就是这样实现的。这用起来就不那么顺手。 所以 Oracle/Sun JDK / OpenJDK 的这个地方就用了点绕弯的做法。它假设把 HeapByteBuffer 背后的 byte[] 里的内容拷贝一次是一个时间开销可以接受的操作，同时假设真正的I/O可能是一个很慢的操作。 于是它就先把 HeapByteBuffer 背后的 byte[] 的内容拷贝到一个 DirectByteBuffer 背后的native memory去，这个拷贝会涉及 sun.misc.Unsafe.copyMemory() 的调用，背后是类似 memcpy() 的实现。这个操作本质上是会在整个拷贝过程中暂时不允许发生GC的，虽然实现方式跟JNI的Critical系API不太一样。（具体来说是 Unsafe.copyMemory() 是HotSpot VM的一个intrinsic方法，中间没有safepoint所以GC无法发生）。 然后数据被拷贝到native memory之后就好办了，就去做真正的I/O，把 DirectByteBuffer 背后的native memory地址传给真正做I/O的函数。这边就不需要再去访问Java对象去读写要做I/O的数据了。 MappedByteBufferDirectBuffer是继承于MappedByteBuffer的，内存映射文件是一种允许Java直接从内存访问的特殊文件，操作系统再负责将内存的改动写入的IO设备中。 12345678910111213141516171819/** * MappedByteBuffer * @author yangfan * @date 2017/08/17 */public class NioTest9 &#123; public static void main(String[] args) throws IOException &#123; RandomAccessFile randomAccessFile = new RandomAccessFile("NioTest9.txt", "rw"); FileChannel fileChannel = randomAccessFile.getChannel(); // 将文件映射到内存中，就可以在内存中直接修改文件了 MappedByteBuffer mappedByteBuffer = fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 5); mappedByteBuffer.put(0, (byte)'a'); mappedByteBuffer.put(3, (byte)'b'); randomAccessFile.close(); &#125;&#125; 上面的代码执行完成会直接修改NioTest9.txt中的内容。 FileLock文件锁这个用的不多，共享锁是只读，都只能读，排他是只能自己读写，别人不能读也不能写。 12345678910111213141516171819202122/** * 文件锁 * @author yangfan * @date 2017/08/17 */public class NioTest10 &#123; public static void main(String[] args) throws IOException &#123; RandomAccessFile randomAccessFile = new RandomAccessFile("NioTest10.txt", "rw"); FileChannel fileChannel = randomAccessFile.getChannel(); // 从索引3锁6个长度 // true表示共享锁，false表示排他锁 FileLock fileLock = fileChannel.lock(3, 6, true); System.out.println("valid: " + fileLock.isValid()); System.out.println("lock type: " + fileLock.isShared()); fileLock.release(); randomAccessFile.close(); &#125;&#125; Scattering &amp; Gathering之前的例子中在进行读写的时候，都是用的一个Buffer对象来完成的，Buffer的Scattering(散开)，可以接受传递一个Buffer的数组。比如我要把Channel中的信息读到Buffer中，那么channel里面有20个字节，传递一个Buffer数组，往里面读信息，第一个数组长度是10，第二个数组长度是5，第三个长度也是5，它会按顺序将第一个Buffer读满，再接着往第二个读，再读第三个。就是将一个Channel中的数据读取到多个Buffer中。而Gathering则是相反的，他是写操作，先将第一个Buffer写到Channel中，然后也是顺序写入后面的channel。 下面用一个网络IO的例子来说明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 关于Buffer的Scattering与Gathering * * @author yangfan * @date 2017/08/17 */public class NioTest11 &#123; public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); InetSocketAddress address = new InetSocketAddress(8899); serverSocketChannel.socket().bind(address); int messageLength = 2 + 3 + 4; ByteBuffer[] buffers = new ByteBuffer[3]; buffers[0] = ByteBuffer.allocate(2); buffers[1] = ByteBuffer.allocate(3); buffers[2] = ByteBuffer.allocate(4); SocketChannel socketChannel = serverSocketChannel.accept(); while (true) &#123; int bytesRead = 0; while (bytesRead &lt; messageLength) &#123; // 数组类型的读 long r = socketChannel.read(buffers); bytesRead += r; System.out.println("bytesRead: " + bytesRead); Stream.of(buffers) .map(buffer -&gt; "position: " + buffer.position() + ", limit: " + buffer.limit()) .forEach(System.out::println); &#125; Stream.of(buffers).forEach(Buffer::flip); long bytesWritten = 0; while (bytesWritten &lt; messageLength) &#123; long r = socketChannel.write(buffers); bytesWritten += r; &#125; Stream.of(buffers).forEach(Buffer::clear); System.out.println("bytesRead: " + bytesRead + "， byteWritten: " + bytesWritten + ", messageLength: " + messageLength); &#125; &#125;&#125; nc localhost 8899 telnet localhost 8899 这2个命令都可以进行刚才的程序测试。 123nc localhost 8899helloworhellowor 回车也算一个字节，所以输入hellowor后，程序马上回写了数据。控制台输出： 12345bytesRead: 9position: 2, limit: 2position: 3, limit: 3position: 4, limit: 4bytesRead: 9， byteWritten: 9, messageLength: 9 现在程序依旧在等待输入，我们继续输入12345helloahelloa 这里先输入一个hello+回车，再输入a+回车，再回车，进行了3次操作，也一共是9个字节，数据进行了回写，接下来看控制台的输出。 12345678910111213bytesRead: 6position: 2, limit: 2position: 3, limit: 3position: 1, limit: 4bytesRead: 8position: 2, limit: 2position: 3, limit: 3position: 3, limit: 4bytesRead: 9position: 2, limit: 2position: 3, limit: 3position: 4, limit: 4bytesRead: 9， byteWritten: 9, messageLength: 9 第一次输入hello+回车的时候，输入了6个字节，0、1索引的buffer读满了，2索引的buffer读取了一个位置，再敲入a+回车，2索引的Buffer还剩一个位置，此时再敲入回车，Buffer全部读满，Buffer开始进行写入操作。]]></content>
      <categories>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO Buffer详解]]></title>
    <url>%2F2017%2F08%2F15%2Fnio2%2F</url>
    <content type="text"><![CDATA[Bufferjava NIO中的Buffer用于和NIO通道进行交互。数据从通道读入到缓冲区，从缓冲区写入到通道中。缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。 之前代码里面用到了ByteBuffer，实际上ByteBuffer不只是可以放byte在里面，也可以放别的类型数据，但是取出来的时候必须跟放进去的类型顺序也保持一致，否则会报错。(BufferUnderflowException) put &amp; get12345678910111213141516171819202122public class NioTest5 &#123; public static void main(String[] args) &#123; ByteBuffer buffer = ByteBuffer.allocate(64); buffer.putInt(15); buffer.putLong(50000000L); buffer.putDouble(14.123123); buffer.putChar('好'); buffer.putShort((short) 2); buffer.putChar('的'); buffer.flip(); System.out.println(buffer.getInt()); System.out.println(buffer.getLong()); System.out.println(buffer.getDouble()); System.out.println(buffer.getChar()); System.out.println(buffer.getShort()); System.out.println(buffer.getChar()); &#125;&#125; Slice Buffer123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Buffer的分片--slice() * Slice Buffer与原有Buffer共享相同的底层数组 * @author yangfan * @date 2017/08/15 */public class NioTest6 &#123; public static void main(String[] args) &#123; ByteBuffer buffer = ByteBuffer.allocate(10); for (int i = 0; i &lt; buffer.capacity(); i++) &#123; buffer.put((byte)i); &#125; buffer.position(2); buffer.limit(6); // 会返回一个包含[2,6)原始数据的Buffer，任意修改2个Buffer的数据，对应都会发生变化 ByteBuffer sliceBuffer = buffer.slice(); for (int i = 0; i &lt; sliceBuffer.capacity(); i++) &#123; byte b = sliceBuffer.get(i); b *= 2; sliceBuffer.put(i , b); &#125; buffer.clear(); while (buffer.hasRemaining()) &#123; System.out.println(buffer.get()); &#125; // 输出下面结果// 0// 1// 4// 6// 8// 10// 6// 7// 8// 9 &#125;&#125; 只读Buffer12345678910111213141516171819202122232425262728/** * 只读Buffer,我们可以随时将一个普通Buffer调用asReadOnlyBuffer方法返回一个只读Buffer * 但不能将一个只读Buffer转换为读写Buffer * @author yangfan * @date 2017/08/15 */public class NioTest7 &#123; public static void main(String[] args) &#123; ByteBuffer buffer = ByteBuffer.allocate(10); //class java.nio.HeapByteBuffer System.out.println(buffer.getClass()); for (int i = 0; i &lt; buffer.capacity(); i++) &#123; buffer.put((byte) i); &#125; ByteBuffer readOnlyBuffer = buffer.asReadOnlyBuffer(); //class java.nio.HeapByteBufferR System.out.println(readOnlyBuffer.getClass()); readOnlyBuffer.position(0); // ReadOnlyBufferException readOnlyBuffer.put((byte) 2); &#125;&#125;]]></content>
      <categories>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO]]></title>
    <url>%2F2017%2F08%2F12%2Fnio1%2F</url>
    <content type="text"><![CDATA[Java NIO “对语言设计人员来说，创建好的输入／输出系统是一项特别困难的任务。”――《Think in Java》 下面一起来研究一下NIO的用法，先来一段示例代码： 123456789101112131415public class NioTest1 &#123; public static void main(String[] args) &#123; IntBuffer buffer = IntBuffer.allocate(10); for (int i = 0; i &lt; buffer.capacity(); i++) &#123; int randomNumber = new SecureRandom().nextInt(20); buffer.put(randomNumber); &#125; buffer.flip(); while (buffer.hasRemaining()) &#123; System.out.println(buffer.get()); &#125; &#125;&#125; 以上代码会输出10个随机数，输出很简单，但是包含的逻辑却很丰富。 java的io现在分为2种: java.io java.nio java.io中最为核心的一个概念是流(Stream)，面向流的编程，里面也大量运用了装饰模式，在java.io中。Java中一个流要么是输入流，要么是输出流，不可能同时既是输入流又是输出流。java.nio中拥有3个核心概念：Selector，Channel与Buffer。在java.nio中，我们是面向块（block）或是缓冲区（buffer）编程的。Buffer本身就是块内存，底层实现上，它实际上是个数组。数据的读、写都是通过Buffer来实现的。 除了数组之外，Buffer还提供了对于数据的结构化访问方式，并且可以追踪到系统的读写过程。Java中的7种原生数据类型都有各自对应的Buffer类型，如IntBuffer，LongBuffer，ByteBuffer及CharBuffer等等，并没有BooleanBuffer类型。 Channel指的是可以向其写入数据或是从中读取数据的对象，它类似于java.io中的Stream。所有数据的读写都是通过Buffer来进行的，永远不会出现直接向Channel写入数据的情况，或是直接从Channel读取数据的情况。与Stream不同的是，Channel是双向的，一个流只可能是InputStream或是OutputStream，Channel打开后则可以进行读取、写入或是读写。由于Channel是双向的，因此它能更好的反映出底层操作系统的真实情况；在linux系统中，底层操作系统的通道就是双向的。 实例1读取文件 NioTest2.txt 1hello world welcome 12345678910111213141516171819202122public class NioTest2 &#123; public static void main(String[] args) throws IOException &#123; FileInputStream fileInputStream = new FileInputStream("NioTest2.txt"); // 通过文件输入流可以获取到通道对象 FileChannel fileChannel = fileInputStream.getChannel(); // 无论读写都必须通过Buffer来操作 ByteBuffer byteBuffer = ByteBuffer.allocate(512); fileChannel.read(byteBuffer); // 之前是往Buffer里面写，现在进行Buffer的读，所以要调用flip()方法 byteBuffer.flip(); while (byteBuffer.hasRemaining()) &#123; byte b = byteBuffer.get(); System.out.println("Character: " + (char) b); &#125; fileInputStream.close(); &#125;&#125; 实例2写入文件 123456789101112131415161718192021public class NioTest3 &#123; public static void main(String[] args) throws Exception &#123; FileOutputStream fileOutputStream = new FileOutputStream("NioTest3.txt"); FileChannel fileChannel = fileOutputStream.getChannel(); ByteBuffer byteBuffer = ByteBuffer.allocate(512); byte[] messages = "hello world welcome, nihao".getBytes(); for (byte message : messages) &#123; byteBuffer.put(message); &#125; byteBuffer.flip(); fileChannel.write(byteBuffer); &#125;&#125; 上面2个例子，能看出一定的模式，就是数据一定是跟Buffer打交道的，然后再读或写到Channel中。 读： file-&gt;channel-&gt;buffer-&gt;filp()-&gt;打印内容写： 数据-&gt;buffer-&gt;flip()-&gt;channel-&gt;输出到文件 Buffer中的3个重要属性看实例中都调用了一句关键代码filp()，这个方法的源码如下： 123456public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this;&#125; 关于NIO Buffer中的3个重要状态属性的含义：position，limit与capacity。 Buffer的capacity是指它所包含的元素的数量，capacity不可能为负数，也不会变化。 Buffer的limit是指第一个不能被读或者写的元素的索引，并且永远不会超过capacity。 Buffer的position是说下一个将要被读或者写的索引，不能为负数，并且不会超过limit。 假设执行Buffer.allocate(6),下面看内存划分情况。 0 1 2 3 4 5 6 ↑ ↑ position capacity limit 最后一个是虚拟的位置。 现在往Buffer中放入2个元素,记住position是下一个可以被读或者写的位置，所以应该是在索引为2的地方。 0 1 2 3 4 5 6 ↑ ↑ position capacity limit 再放2个元素。 0 1 2 3 4 5 6 ↑ ↑ position capacity limit 现在调用flip()，看看会发生什么事。 先把position归位,然后limit变为之前position的位置。 0 1 2 3 4 5 6 ↑ ↑ ↑ position limit capacity 而且position是不会大于limit的，所以hasRemaining()的实现其实也很简单，当position和limit相等的时候，表示Buffer的东西已经被读取完毕。 123public final boolean hasRemaining() &#123; return position &lt; limit;&#125; 实例312345678910111213141516171819202122232425262728293031323334353637public class NioTest4 &#123; public static void main(String[] args) throws IOException &#123; FileInputStream inputStream = new FileInputStream("input.txt"); FileOutputStream outputStream = new FileOutputStream("output.txt"); FileChannel inputChannel = inputStream.getChannel(); FileChannel outputChannel = outputStream.getChannel(); ByteBuffer buffer = ByteBuffer.allocate(512); while (true) &#123; // 如果删除这行代码，第一次运行后position和limit是相等的，那么这个时候是不能再写入数据的了，所以read会返回0，导致死循环不停的重复写入数据到文件中 buffer.clear(); // 先读取数据,返回值是每次读取的字节 int read = inputChannel.read(buffer); System.out.println("read: " + read); // 第一次读取完成后，调用clear()将position设置为0，limit归位。 // 那么这个时候返回的read应该是-1，因为Channel里面已经没有数据了 if (-1 == read) &#123; break; &#125; // 翻转 // position为0，limit为之前的position buffer.flip(); // 写入数据 outputChannel.write(buffer); &#125; inputChannel.close(); outputChannel.close(); &#125;&#125; 通过NIO读取文件涉及到3个步骤： 从FileInputStream获取到FileChannel对象。 创建Buffer。 将数据从Channel读取到Buffer中。 绝对方法与相对方法绝对方法与相对方法的含义： 相对方法：limit值与position值会在操作时被考虑到。 绝对方法：完全忽略掉limit值和position值。]]></content>
      <categories>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot-admin]]></title>
    <url>%2F2017%2F08%2F12%2Fspring-boot-admin%2F</url>
    <content type="text"><![CDATA[Spring Boot AdminKotlin版本 Java版本 简介简单的后台开发模板框架，具备用户管理，菜单管理和角色管理3个功能，权限控制到按钮层级。 采用JWT+Spring Security进行权限验证和会话保持项目基于Spring Boot+Mybatis+BootStrap+DataTables 前端代码不是我自己写的，后端有Kotlin和Java两个版本，Kotlin对Java生态的兼容比较好，我是先做的Java版，只花了一点时间就把Java版本’复制‘成了Kotlin版本。 相关框架 Spring boot Mybatis druid lombok backbone.js bootstrap datatables 修改application-dev.yml里的数据库连接执行mysql -uroot -p 数据库 &lt; dmc.sql导入数据库脚本。 直接RunDMCApplication启动后访问：http://localhost:10000 帐号：admin密码：111111 页面展示]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-gRPC介绍和使用]]></title>
    <url>%2F2017%2F07%2F23%2Fnetty5%2F</url>
    <content type="text"><![CDATA[gRPCDefine your service using Protocol Buffers, a powerful binary serialization toolset and language gRPC是基于Protobuf开发的RPC框架，简化了protobuf的开发，提供了服务端和客户端网络交互这一块的代码。 Demo照着 https://grpc.io/docs/quickstart/java.html 测试一下官方的Demo。 记得要把Update a gRPC service部分做了。 gRPC整合Gradle与代码生成https://github.com/grpc/grpc-java这个是gRPC-java项目，先引入gRPC的依赖。 123compile 'io.grpc:grpc-netty:1.4.0'compile 'io.grpc:grpc-protobuf:1.4.0'compile 'io.grpc:grpc-stub:1.4.0' 然后配置gradle的grpc插件 1234567891011121314151617181920212223242526272829apply plugin: 'java'apply plugin: 'com.google.protobuf'buildscript &#123; repositories &#123; mavenCentral() &#125; dependencies &#123; // ASSUMES GRADLE 2.12 OR HIGHER. Use plugin version 0.7.5 with earlier // gradle versions classpath 'com.google.protobuf:protobuf-gradle-plugin:0.8.1' &#125;&#125;protobuf &#123; protoc &#123; artifact = "com.google.protobuf:protoc:3.2.0" &#125; plugins &#123; grpc &#123; artifact = 'io.grpc:protoc-gen-grpc-java:1.4.0' &#125; &#125; generateProtoTasks &#123; all()*.plugins &#123; grpc &#123;&#125; &#125; &#125;&#125; 后面直接用gradle的任务就可以生成代码了。 gRPC提供了3种传输层的实现 gRPC comes with three Transport implementations: The Netty-based transport is the main transport implementation based on Netty. It is for both the client and the server. The OkHttp-based transport is a lightweight transport based on OkHttp. It is mainly for use on Android and is for client only. The inProcess transport is for when a server is in the same process as the client. It is useful for testing. https://github.com/google/protobuf-gradle-plugin The Gradle plugin that compiles Protocol Buffer (aka. Protobuf) definition files (*.proto) in your project. There are two pieces of its job: It assembles the Protobuf Compiler (protoc) command line and use it to generate Java source files out of your proto files. It adds the generated Java source files to the input of the corresponding Java compilation unit (sourceSet in a Java project; variant in an Android project), so that they can be compiled along with your Java sources. 实战配置好后，进行一个演示 在src/main/proto新建一个文件Student.proto gradle插件默认从src/main/proto找proto源文件进行代码生成，这里有提到，而且这个路径的配置是可以修改的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647syntax = &quot;proto3&quot;;package com.sail.proto;option java_package = &quot;com.sail.proto&quot;;option java_outer_classname = &quot;StudentProto&quot;;option java_multiple_files = true;service StudentService &#123; rpc GetRealNameByUsername(MyRequest) returns (MyResponse) &#123;&#125; rpc GetStudentsByAge(StudentRequest) returns (stream StudentResponse) &#123;&#125; rpc GetStudentsWrapperByAges(stream StudentRequest) returns (StudentResponseList) &#123;&#125; rpc BiTalk(stream StreamRequest) returns (stream StreamResponse) &#123;&#125;&#125;message MyRequest &#123; string username = 1;&#125;message MyResponse &#123; string realname = 2;&#125;message StudentRequest &#123; int32 age = 1;&#125;message StudentResponse &#123; string name = 1; int32 age = 2; string city = 3;&#125;message StudentResponseList &#123; repeated StudentResponse studentResponse = 1;&#125;message StreamRequest &#123; string request_info = 1;&#125;message StreamResponse &#123; string response_info = 1;&#125; 然后执行gradle generateProto，生成的代码默认是放在/build目录下，我们手动拷贝到src/main/java。 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package com.sail.grpc;import com.sail.proto.*;import io.grpc.stub.StreamObserver;import java.util.UUID;/** * @author yangfan * @date 2017/08/01 */public class StudentServiceImpl extends StudentServiceGrpc.StudentServiceImplBase &#123; @Override public void getRealNameByUsername(MyRequest request, StreamObserver&lt;MyResponse&gt; responseObserver) &#123; System.out.println("接收到客户端信息： " + request.getUsername()); responseObserver.onNext(MyResponse.newBuilder().setRealname("张三").build()); responseObserver.onCompleted(); &#125; /** * 接收StudentRequest参数 * 返回stream的StudentResponse */ @Override public void getStudentsByAge(StudentRequest request, StreamObserver&lt;StudentResponse&gt; responseObserver) &#123; System.out.println("接收到客户端信息：" + request.getAge()); responseObserver.onNext(StudentResponse.newBuilder().setName("张三").setAge(20).setCity("北京").build()); responseObserver.onNext(StudentResponse.newBuilder().setName("李四").setAge(30).setCity("天津").build()); responseObserver.onNext(StudentResponse.newBuilder().setName("王五").setAge(40).setCity("成都").build()); responseObserver.onNext(StudentResponse.newBuilder().setName("赵六").setAge(50).setCity("深圳").build()); responseObserver.onCompleted(); &#125; /** * 接收stream的StudentRequest参数 * 返回StudentResponseList */ @Override public StreamObserver&lt;StudentRequest&gt; getStudentsWrapperByAges(StreamObserver&lt;StudentResponseList&gt; responseObserver) &#123; return new StreamObserver&lt;StudentRequest&gt;() &#123; @Override public void onNext(StudentRequest value) &#123; System.out.println("onNext: " + value.getAge()); &#125; @Override public void onError(Throwable t) &#123; System.out.println(t.getMessage()); &#125; @Override public void onCompleted() &#123; StudentResponse studentResponse = StudentResponse.newBuilder().setName("张三").setAge(20).setCity("西安").build(); StudentResponse studentResponse2 = StudentResponse.newBuilder().setName("李四").setAge(30).setCity("成都").build(); StudentResponseList studentResponseList = StudentResponseList.newBuilder() .addStudentResponse(studentResponse).addStudentResponse(studentResponse2).build(); responseObserver.onNext(studentResponseList); responseObserver.onCompleted(); &#125; &#125;; &#125; /** * 双向流式数据传递 */ @Override public StreamObserver&lt;StreamRequest&gt; biTalk(StreamObserver&lt;StreamResponse&gt; responseObserver) &#123; return new StreamObserver&lt;StreamRequest&gt;() &#123; @Override public void onNext(StreamRequest value) &#123; System.out.println(value.getRequestInfo()); responseObserver.onNext(StreamResponse.newBuilder().setResponseInfo(UUID.randomUUID().toString()).build()); &#125; @Override public void onError(Throwable t) &#123; System.out.println(t.getMessage()); &#125; @Override public void onCompleted() &#123; responseObserver.onCompleted(); &#125; &#125;; &#125;&#125; 服务器端12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.sail.grpc;import io.grpc.Server;import io.grpc.ServerBuilder;import java.io.IOException;/** * @author yangfan * @date 2017/08/01 */public class GrpcServer &#123; private Server server; private void start() throws IOException &#123; this.server = ServerBuilder.forPort(8899).addService(new StudentServiceImpl()).build().start(); System.out.println("server started！"); // 这里在关闭JVM的时候会执行JVM回调钩子 Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; System.out.println("关闭jvm"); GrpcServer.this.stop(); &#125;)); System.out.println("执行到这里"); &#125; private void stop() &#123; if (server != null) &#123; this.server.shutdown(); &#125; &#125; private void awaitTermination() throws InterruptedException &#123; if (server != null) &#123; this.server.awaitTermination(); &#125; &#125; public static void main(String[] args) throws InterruptedException, IOException &#123; GrpcServer grpcServer = new GrpcServer(); grpcServer.start(); grpcServer.awaitTermination(); &#125;&#125; 客户端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package com.sail.grpc;import com.sail.proto.*;import io.grpc.ManagedChannel;import io.grpc.ManagedChannelBuilder;import io.grpc.stub.StreamObserver;import java.time.LocalDateTime;import java.util.Iterator;/** * @author yangfan * @date 2017/08/01 */public class GrpcClient &#123; public static void main(String[] args) &#123; ManagedChannel managedChannel = ManagedChannelBuilder.forAddress("localhost", 8899) .usePlaintext(true).build(); StudentServiceGrpc.StudentServiceBlockingStub blockingStub = StudentServiceGrpc.newBlockingStub(managedChannel); StudentServiceGrpc.StudentServiceStub stub = StudentServiceGrpc.newStub(managedChannel); MyResponse myResponse = blockingStub.getRealNameByUsername(MyRequest.newBuilder().setUsername("zhangsan").build()); System.out.println(myResponse.getRealname()); System.out.println("----------------"); Iterator&lt;StudentResponse&gt; iter = blockingStub.getStudentsByAge(StudentRequest.newBuilder().setAge(20).build()); while (iter.hasNext()) &#123; StudentResponse studentResponse = iter.next(); System.out.println(studentResponse.getName() + ", " + studentResponse.getAge() + ", " + studentResponse.getCity()); &#125; System.out.println("----------------"); // getStudentsWrapperByAges的调用代码 StreamObserver&lt;StudentResponseList&gt; studentResponseListStreamObserver = new StreamObserver&lt;StudentResponseList&gt;() &#123; @Override public void onNext(StudentResponseList value) &#123; value.getStudentResponseList().forEach(studentResponse -&gt; &#123; System.out.println(studentResponse.getName() + ", " + studentResponse.getAge() + ", " + studentResponse.getCity()); System.out.println("*******"); &#125;); &#125; @Override public void onError(Throwable t) &#123; System.out.println(t.getMessage()); &#125; @Override public void onCompleted() &#123; System.out.println("completed!"); &#125; &#125;; // 只要客户端是以流式发送请求，那么一定是异步的 StreamObserver&lt;StudentRequest&gt; studentRequestStreamObserver = stub.getStudentsWrapperByAges(studentResponseListStreamObserver); // 发送多条数据 studentRequestStreamObserver.onNext(StudentRequest.newBuilder().setAge(20).build()); studentRequestStreamObserver.onNext(StudentRequest.newBuilder().setAge(30).build()); studentRequestStreamObserver.onNext(StudentRequest.newBuilder().setAge(40).build()); studentRequestStreamObserver.onNext(StudentRequest.newBuilder().setAge(50).build()); studentRequestStreamObserver.onCompleted(); // 以上代码是没有输出结果的，因为stub是异步的，所以当执行完onCompleted的时候程序就已经结束了，还没有来得及发送请求 // 现在加入以下代码，让程序多运行一会 try &#123; Thread.sleep(50000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 双向数据流的调用 StreamObserver&lt;StreamRequest&gt; requestStreamObserver = stub.biTalk(new StreamObserver&lt;StreamResponse&gt;() &#123; @Override public void onNext(StreamResponse value) &#123; System.out.println(value.getResponseInfo()); &#125; @Override public void onError(Throwable t) &#123; System.out.println(t.getMessage()); &#125; @Override public void onCompleted() &#123; System.out.println("onCompleted!"); &#125; &#125;); for (int i = 0; i &lt; 10; i++) &#123; requestStreamObserver.onNext(StreamRequest.newBuilder().setRequestInfo(LocalDateTime.now().toString()).build()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-Apache Thrift介绍和使用]]></title>
    <url>%2F2017%2F07%2F09%2Fnetty4%2F</url>
    <content type="text"><![CDATA[Thrift Thrift最初由Facebook研发，主要用于各个服务之间的RPC通信，支持跨语言，常用的语言比如C++，Java，Python，PHP，Ruby，Erlang，Perl，Haskell，C#，Cocoa，JavaScript，Node.js，SmallTalk and OCaml都支持。 Thrift是一个典型的CS（客户端/服务端）结构，客户端和服务端可以使用不同语言开发。既然客户端和服务端能使用不同的语言开发，那么一定就要有一种中间语言来关联客户端和服务端的语言，这种语言就是IDL（Interface Description Language）。 Thrift不支持无符号类型，因为很多变成语言不存在无符号类型，比如Java Thrift数据类型 byte：有符号字节 i16：16位有符号整数 i32：32位有符号整数 i64：64位有符号整数 double：64位有符号整数 double：64位浮点数 string：字符串 Thrift容器类型 集合中的元素可以是除了service之外的任何类型，包括exception list：一些列由T类型的数据组成的有序列表，元素可以重复 set：一系列由T类型的数据组成的无序集合，元素不可重复 map：一个字典结构，key为K类型，value为V类型，相当于Java中的HashMap Thrift工作原理如何实现多语言之间的通信？ 数据传输使用socket（多重语言均支持），数据再以特定的格式（string等）发送，接受方语言进行解析。 定义thrift的文件，由thrift文件（IDL）生成双方语言的接口、model，在生成的model以及接口中会有解码编码的代码 结构体（struct）就像C语言一样，Thrift支持struct类型，目的就是将一些数据聚合在一起，方便传输管理。struct的定义形式如下： 12345struct Prople &#123; 1:string name; 2:i32 age; 3:string gender;&#125; 枚举枚举的定义形式和Java的Enum定义类似： 1234enum Gender &#123; MALE, FEMALE&#125; 异常（exception）Thrift支持自定义exception，规则与struct一样 1234exception RequestException &#123; 1:i32 code; 2:string reasone;&#125; 服务（service）Thrift定义服务相当于Java中创建Interface一样，创建的service经过代码生成命令之后就会生成客户端和服务端的框架代码。定义形式如下： 1234service HelloWorldService &#123; //service中定义的函数，相当于Java interface中定义的方法 string doAction(1:string name, 2:i32 age);&#125; 类型定义Thrift支持类似C++一样的typedef定义： 12typedef i32 inttypedef i64 long 常量（const）thrift也支持常量定义，使用const关键字： 12const i32 MAX_RETRIES_TIME = 10const string MY_WEBSIZE = &quot;http://facebook.com&quot; 命名空间 Thrift的命名空间相当于Java中的package的意思，主要目的是组织代码。thrift使用关键namespace 定义命名空间： namespace java com.test.thrift.demo 格式是：namespace 语言名 路径 文件包含Thrift支持文件包含，相当于C/C++中的include，Java中的import。使用关键字include定义： include &quot;global.thrift&quot; 注释Thrift注释方式支持shell风格的注释，支持C/C++风格的注释，即#和//开头的语句都当作注释，/**/包裹的语句也是注释。 可选与必选Thrift提供两个关键字required，optional，分别用于表示对应的字段是必填的还是可以选的 1234struct People &#123; 1:required string name; 2:optional i32 age;&#125; 生成代码 了解了如何定义thrift文件之后，我们需要用定义好的thrift文件生成我们需要的目标语言的源码 首先需要定义thrift接口描述文件 参见data.thrift data.thrift 12345678910111213141516171819202122232425namespace java thrift.generatedtypedef i16 shorttypedef i32 inttypedef i64 longtypedef bool booleantypedef string Stringstruct Person &#123; 1: optional String username, 2: optional int age, 3: optional boolean married&#125;exception DataException &#123; 1: optional String message, 2: optional String callStack, 3: optional String date&#125;service PersonService &#123; Person getPersonByUsername(1: required String username) throws (1: DataException dataException), void savePerson(1: required Person person) throws (1: DataException dataException)&#125; 执行生成代码命令 thrift --gen java src/thrift/data.thrift 记得导入&#39;org.apache.thrift:libthrift:0.10.0&#39; 生成了3个类，分别是Person，DataException，PersonService。 然后我们来写一个service的实现类 PersonServiceImpl 1234567891011121314151617181920212223242526272829303132package com.sail.thrift;import thrift.generated.DataException;import thrift.generated.Person;import thrift.generated.PersonService;/** * @author yangfan * @date 2017/07/09 */public class PersonServiceImpl implements PersonService.Iface &#123; @Override public Person getPersonByUsername(String username) throws DataException, org.apache.thrift.TException &#123; Person person = new Person(); person.setUsername(username); person.setAge(20); person.setMarried(false); return person; &#125; @Override public void savePerson(Person person) throws DataException, org.apache.thrift.TException &#123; System.out.println("Got Client Param: "); System.out.println(person.getUsername()); System.out.println(person.getAge()); System.out.println(person.isMarried()); &#125;&#125; 测试接下来编写客户端和服务端代码进行测试。 ThriftServer.java 1234567891011121314151617public class ThriftServer &#123; public static void main(String[] args) throws Exception &#123; TNonblockingServerSocket socket = new TNonblockingServerSocket(8899); THsHaServer.Args arg = new THsHaServer.Args(socket).minWorkerThreads(2).maxWorkerThreads(4); PersonService.Processor&lt;PersonServiceImpl&gt; processor = new PersonService.Processor&lt;&gt;(new PersonServiceImpl()); arg.protocolFactory(new TCompactProtocol.Factory()); arg.transportFactory(new TFramedTransport.Factory()); arg.processorFactory(new TProcessorFactory(processor)); TServer server = new THsHaServer(arg); System.out.println("Thrift Server Started"); server.serve(); &#125;&#125; ThriftClient.java 123456789101112131415161718192021222324252627282930313233public class ThriftClient &#123; public static void main(String[] args) &#123; TTransport transport = new TFramedTransport(new TSocket("localhost", 8899), 600); TProtocol protocol = new TCompactProtocol(transport); PersonService.Client client = new PersonService.Client(protocol); try &#123; transport.open(); Person person = client.getPersonByUsername("张三"); System.out.println(person.getUsername()); System.out.println(person.getAge()); System.out.println(person.isMarried()); Person person2 = new Person(); person2.setUsername("李四"); person2.setAge(30); person2.setMarried(true); client.savePerson(person2); &#125; catch (Exception e) &#123; throw new RuntimeException(e.getMessage(), e); &#125; finally &#123; transport.close(); &#125; &#125;&#125; 然后分别启动服务端和客户端 服务端输出： 12345Thrift Server StartedGot Client Param: 李四30true 客户端输出: 12345Received 1张三20falseReceived 2 Thrift传输格式 TBinaryProtocol-二进制格式 TCompactProtocal-压缩格式 TJSONProtocol-JSON格式 TSimpleJSONProtocol-提供JSON只写协议，生成的文件很容易通过脚本语言解析。 TDebugProtocol-使用易懂的可读的文本格式，以便于debug Thrift数据传输方式 TSocket-阻塞式Socket TFramedTransport-以frame为单位进行传输，非阻塞式服务中使用。 TFileTransport-以文件形式进行传输。 TMemortyTransport-将内存用于I/O，Java实现时内部实际使用了简单的ByteArrayOuputStream。 TZlibTransport-使用zlib进行压缩，与其他传输方式联合使用。当前无Java实现。 Thrift支持的服务模型 TSimpleServer-简单的单线程服务模型，常用于测试 TThreadPoolServer-多线程服务模型，使用标准的阻塞式IO TNonblockingServer-多线程服务模型，使用非阻塞式IO（需使用TFramedTransport数据传输方式） THsHaServer-ThsHa引入了线程池去处理，其模型把读写任务房到线程池去处理；Half-sync/Half-async的处理模式，Half-aysnc是在处理IO时间上（accpect/read/write io），Half-sync是用于handler对rpc的同步处理 Thrift对多语言的支持Python作为Client在data.thrift中增加一行代码 12namespace java thrift.generatednamespace py py.thrift.generated 然后执行命令生成python代码thrift --gen py src/thrift/data.thrift 在Java项目中，我们的依赖是通过gradle引入的，这里python我们需要下载thrift的包，然后进入lib/py目录，执行里面的python脚本。 sudo python setup.py install 生成的依赖位于：/Library/Python/2.7/site-packages/thrift-0.10.0-py2.7-macosx-10.12-intel.egg 然后编写客户端的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243# coding=utf-8__author__ = 'sail'from py.thrift.generated import PersonServicefrom py.thrift.generated import ttypesfrom thrift import Thriftfrom thrift.transport import TSocketfrom thrift.transport import TTransportfrom thrift.protocol import TCompactProtocolimport sysreload(sys)sys.setdefaultencoding('utf8')try: tSocket = TSocket.TSocket("localhost", 8899) tSocket.setTimeout(600) transport = TTransport.TFramedTransport(tSocket) protocol = TCompactProtocol.TCompactProtocol(transport) client = PersonService.Client(protocol) transport.open() person = client.getPersonByUsername("张三") print person.username print person.age print person.married print '----------------' newPerson = ttypes.Person() newPerson.username = "lisi" newPerson.age = 30 newPerson.married = True client.savePerson(newPerson)except Thrift.TException, tx: print '%s' % tx.message 运行后输出以下结果 123张三20False Python作为Server编写py_server.py代码: 123456789101112131415161718192021222324# coding=utf-8from py.thrift.generated import PersonServicefrom PersonServiceImpl import PersonServiceImplfrom thrift import Thriftfrom thrift.transport import TTransportfrom thrift.transport import TSocketfrom thrift.protocol import TCompactProtocolfrom thrift.server import TServertry: personServiceHandler = PersonServiceImpl() processor = PersonService.Processor(personServiceHandler) serverSocket = TSocket.TServerSocket(port=8899) transportFactory = TTransport.TFramedTransportFactory() protocolFactory = TCompactProtocol.TCompactProtocolFactory() server = TServer.TSimpleServer(processor, serverSocket, transportFactory, protocolFactory) server.serve()except Thrift.TException, ex: print '%s' % ex.message 然后启动python的服务端，再分别执行Java和Python的客户端，都得到了正确的响应。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-Google Protobuf介绍和使用]]></title>
    <url>%2F2017%2F06%2F15%2Fnetty3%2F</url>
    <content type="text"><![CDATA[Google Protobufhttps://developers.google.com/protocol-buffers/ 接下来看一下Google Protobuf的使用方式。Protobuf的主要作用是用来进行RPC的传输。它跟Apache Thrift属于同一个领域的框架，都可以用来序列化和反序列化数据进行传输。 RMI介绍目前Java中有一门比较成熟，同时也是EJB的标准的技术叫做RMI(remote method invocation)。RMI限制了只能基于Java调用。这种跨机器的调用，是客户端序列化后字节码再通过网络进行传输到服务端，服务端再反序列化数据进行代码调用。这就涉及到2个概念， client: stub(序列化生成代码) server: skeleton(反序列化) 序列化与反序列化，也叫做编码与解码。 RPC介绍Remote Procedure Call, 远程过程调用，很多RPC框架是跨语言的。 定义一个接口说明文件(idl)：描述了对象（结构体）、对象成员、接口方法等一系列信息。 通过RPC框架所提供的编译器，将接口说明文件编译成具体语言文件。 在客户端与服务端分别引入RPC编译器所生产的文件，即可像调用本地方法一样调用远程方法。 Protocol Buffers Protocol buffers are Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages. Protocol buffers是谷歌提供的语言中立、平台中立，用于序列化结构数据的可扩展的机制，就像XML一样，但是它的体积更小，它的速度更快、更简单。数据结构你只需要定义一次就可以了，然后就可以使用生成的各种语言源代码去轻松的读写你的各种结构化数据。 下载https://github.com/google/protobuf/releases 先下载环境，注意不要下错了，因为我是MAC，所以我这里下载protoc-3.3.0-osx-x86_64.zip，配置好环境变量后可执行protoc命令。 Demohttps://developers.google.com/protocol-buffers/docs/javatutorial 然后我们照着官方的demo来写一个。 首先需要编写一个Student.proto文件。 1234567891011121314syntax = &quot;proto2&quot;;package com.sail.protobuf;option optimize_for = SPEED;option java_package = &quot;com.sail.protobuf&quot;;option java_outer_classname = &quot;DataInfo&quot;;message Student &#123; required string name = 1; optional int32 age = 2; optional string address = 3;&#125; 然后执行命令生成代码，就得到一个DataInfo.java protoc --java_out=src/main/java src/main/protobuf/Student.proto 测试现在测试一下这个类的使用 123456789101112131415public class ProtoBufTest &#123; public static void main(String[] args) throws InvalidProtocolBufferException &#123; DataInfo.Student student = DataInfo.Student.newBuilder() .setName("张三").setAge(20).setAddress("北京").build(); byte[] student2ByteArray = student.toByteArray(); DataInfo.Student student2 = DataInfo.Student.parseFrom(student2ByteArray); System.out.println(student2.getName()); System.out.println(student2.getAge()); System.out.println(student2.getAddress()); &#125;&#125; Netty Demo在上述代码中，我们看到正确输出了结果。然后配合Netty使用。Netty代码跟之前的套路都是一样的，还是Handler不同。 服务端代码TestServer.java 123456789101112131415161718192021public class TestServer &#123; public static void main(String[] args) throws InterruptedException &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) // handler是针对于bossGroup的 .handler(new LoggingHandler(LogLevel.INFO)) // childHandler是针对于workerGroup的 .childHandler(new TestServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8899).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; TestServerInitializer.java 12345678910111213public class TestServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new ProtobufVarint32FrameDecoder()); pipeline.addLast(new ProtobufDecoder(MyDataInfo.Person.getDefaultInstance())); pipeline.addLast(new ProtobufVarint32LengthFieldPrepender()); pipeline.addLast(new ProtobufEncoder()); pipeline.addLast(new TestServerHandler()); &#125;&#125; TestServerHandler.java 12345678910public class TestServerHandler extends SimpleChannelInboundHandler&lt;MyDataInfo.Person&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, MyDataInfo.Person msg) throws Exception &#123; System.out.println(msg.getName()); System.out.println(msg.getAge()); System.out.println(msg.getAddress()); &#125;&#125; 客户端代码TestClient.java 1234567891011121314151617public class TestClient &#123; public static void main(String[] args) throws InterruptedException &#123; // 客户端只需要一个EventLoopGroup EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup).channel(NioSocketChannel.class).handler(new TestClientInitializer()); ChannelFuture channelFuture = bootstrap.connect("localhost", 8899).sync(); channelFuture.channel().closeFuture().sync(); &#125;finally &#123; eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125; TestClientInitializer.java 12345678910111213public class TestClientInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new ProtobufVarint32FrameDecoder()); pipeline.addLast(new ProtobufDecoder(MyDataInfo.Person.getDefaultInstance())); pipeline.addLast(new ProtobufVarint32LengthFieldPrepender()); pipeline.addLast(new ProtobufEncoder()); pipeline.addLast(new TestClientHandler()); &#125;&#125; TestClientHandler.java 123456789101112131415public class TestClientHandler extends SimpleChannelInboundHandler&lt;MyDataInfo.Person&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, MyDataInfo.Person msg) throws Exception &#123; &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; MyDataInfo.Person person = MyDataInfo.Person.newBuilder().setName("张三").setAge(20).setAddress("成都").build(); ctx.channel().writeAndFlush(person); &#125;&#125; 测试先启动服务端，再启动客户端。服务端直接输出了 123张三20成都 另一种方法但是发现我们这个代码非常有局限性，因为代码里写了MyDataInfo.Person.getDefaultInstance,如果要传输其他消息怎么办呢，或者不止一个消息，这个就非常不灵活，下面就介绍一种更灵活的方式。 1pipeline.addLast(new ProtobufDecoder(MyDataInfo.Person.getDefaultInstance())); proto重新定义一下proto文件 12345678910111213141516171819202122232425262728293031323334353637383940414243syntax = "proto2";package com.sail.protobuf;option optimize_for = SPEED;option java_package = "com.sail.netty.sixthexample";option java_outer_classname = "MyDataInfo";message MyMessage &#123; enum DataType &#123; PersonType = 1; DogType = 2; CatType = 3; &#125; required DataType data_type = 1; oneof dataBody &#123; Person person = 2; Dog dog = 3; Cat cat = 4; &#125;&#125;message Person &#123; optional string name = 1; optional int32 age = 2; optional string address = 3;&#125;message Dog &#123; optional string name = 1; optional int32 age = 2;&#125;message Cat &#123; optional string name = 1; optional string city = 2;&#125; 上面出现了oneof If you have a message with many optional fields and where at most one field will be set at the same time, you can enforce this behavior and save memory by using the oneof feature. 这是官方的解释，意思就是说如果有很多个optional但是同一时间内只有一个有值，那么就可以用oneof的方式来提升性能节约内存。我们也正是利用这种方式来用同一个消息进行不同的值传递。 对之前的代码进行一些改动 服务端TestServerHandler.java 123456789101112131415161718192021222324252627282930public class TestServerHandler extends SimpleChannelInboundHandler&lt;MyDataInfo.MyMessage&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, MyDataInfo.MyMessage msg) throws Exception &#123; MyDataInfo.MyMessage.DataType dataType = msg.getDataType(); switch (dataType) &#123; case PersonType: MyDataInfo.Person person = msg.getPerson(); System.out.println(person.getName()); System.out.println(person.getAge()); System.out.println(person.getAddress()); break; case DogType: MyDataInfo.Dog dog = msg.getDog(); System.out.println(dog.getName()); System.out.println(dog.getAge()); break; case CatType: MyDataInfo.Cat cat = msg.getCat(); System.out.println(cat.getName()); System.out.println(cat.getCity()); break; &#125; &#125;&#125; TestServerInitializer.java主要修改了ProtobufDecoder的类型。 12345678910111213public class TestServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new ProtobufVarint32FrameDecoder()); pipeline.addLast(new ProtobufDecoder(MyDataInfo.MyMessage.getDefaultInstance())); pipeline.addLast(new ProtobufVarint32LengthFieldPrepender()); pipeline.addLast(new ProtobufEncoder()); pipeline.addLast(new TestServerHandler()); &#125;&#125; 客户端TestClientHandler.java 1234567891011121314151617181920212223242526272829303132public class TestClientHandler extends SimpleChannelInboundHandler&lt;MyDataInfo.MyMessage&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, MyDataInfo.MyMessage msg) throws Exception &#123; &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; int randomInt = new Random().nextInt(3); MyDataInfo.MyMessage myMessage; if (0 == randomInt) &#123; myMessage = MyDataInfo.MyMessage.newBuilder() .setDataType(MyDataInfo.MyMessage.DataType.PersonType) .setPerson(MyDataInfo.Person.newBuilder().setName("张三").setAge(20).setAddress("成都").build()).build(); &#125; else if (1 == randomInt) &#123; myMessage = MyDataInfo.MyMessage.newBuilder() .setDataType(MyDataInfo.MyMessage.DataType.DogType) .setDog(MyDataInfo.Dog.newBuilder().setName("一只狗").setAge(2).build()).build(); &#125; else &#123; myMessage = MyDataInfo.MyMessage.newBuilder() .setDataType(MyDataInfo.MyMessage.DataType.CatType) .setCat(MyDataInfo.Cat.newBuilder().setName("七七").setCity("成都").build()).build(); &#125; ctx.channel().writeAndFlush(myMessage); &#125;&#125; TestClientInitializer.java 12345678910111213public class TestClientInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new ProtobufVarint32FrameDecoder()); pipeline.addLast(new ProtobufDecoder(MyDataInfo.MyMessage.getDefaultInstance())); pipeline.addLast(new ProtobufVarint32LengthFieldPrepender()); pipeline.addLast(new ProtobufEncoder()); pipeline.addLast(new TestClientHandler()); &#125;&#125; 测试然后启动服务端，再多启动几次客户端，得到如下输出: 123456789七七成都一只狗2一只狗2张三20成都]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-WebSocket介绍和使用]]></title>
    <url>%2F2017%2F06%2F12%2Fnetty2%2F</url>
    <content type="text"><![CDATA[随着互联网的发展，传统的HTTP协议已经很难满足Web应用日益复杂的需求了。近年来，随着HTML5的诞生，WebSocket协议被提出，它实现了浏览器与服务器的全双工通信，扩展了浏览器与服务端的通信功能，使服务端也能主动向客户端发送数据。 我们知道，传统的HTTP协议是无状态的，每次请求（request）都要由客户端（如 浏览器）主动发起，服务端进行处理后返回response结果，而服务端很难主动向客户端发送数据；这种客户端是主动方，服务端是被动方的传统Web模式 对于信息变化不频繁的Web应用来说造成的麻烦较小，而对于涉及实时信息的Web应用却带来了很大的不便，如带有即时通信、实时数据、订阅推送等功能的应用。在WebSocket规范提出之前，开发人员若要实现这些实时性较强的功能，经常会使用折衷的解决方法：轮询（polling）和Comet技术。其实后者本质上也是一种轮询，只不过有所改进。 轮询是最原始的实现实时Web应用的解决方案。轮询技术要求客户端以设定的时间间隔周期性地向服务端发送请求，频繁地查询是否有新的数据改动。明显地，这种方法会导致过多不必要的请求，浪费流量和服务器资源。 Comet技术又可以分为长轮询和流技术。长轮询改进了上述的轮询技术，减小了无用的请求。它会为某些数据设定过期时间，当数据过期后才会向服务端发送请求；这种机制适合数据的改动不是特别频繁的情况。流技术通常是指客户端使用一个隐藏的窗口与服务端建立一个HTTP长连接，服务端会不断更新连接状态以保持HTTP长连接存活；这样的话，服务端就可以通过这条长连接主动将数据发送给客户端；流技术在大并发环境下，可能会考验到服务端的性能。 这两种技术都是基于请求-应答模式，都不算是真正意义上的实时技术；它们的每一次请求、应答，都浪费了一定流量在相同的头部信息上，并且开发复杂度也较大。 伴随着HTML5推出的WebSocket，真正实现了Web的实时通信，使B/S模式具备了C/S模式的实时通信能力。WebSocket的工作流程是这样的：浏览器通过JavaScript向服务端发出建立WebSocket连接的请求，在WebSocket连接建立成功后，客户端和服务端就可以通过 TCP连接传输数据。因为WebSocket连接本质上是TCP连接，不需要每次传输都带上重复的头部数据，所以它的数据传输量比轮询和Comet技术小了很多。 示例下面来看一下示例代码，套路还是和之前是一样的，只不过Initializer注册的Handler发生了变化。 MyServer.java 123456789101112131415161718192021public class MyServer &#123; public static void main(String[] args) throws InterruptedException &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) // handler是针对于bossGroup的 .handler(new LoggingHandler(LogLevel.INFO)) // childHandler是针对于workerGroup的 .childHandler(new WebSocketChannelInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8899).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; WebSocketChannelInitializer.java 12345678910111213141516public class WebSocketChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new HttpServerCodec()); pipeline.addLast(new ChunkedWriteHandler()); // 把各段HTTP的请求合并为一个FullHttpRequest pipeline.addLast(new HttpObjectAggregator(8192)); pipeline.addLast(new WebSocketServerProtocolHandler("/ws")); pipeline.addLast(new TextWebSocketFrameHandler()); &#125;&#125; TextWebSocketFrameHandler.java 注意这里SimpleChannelInboundHandler的泛型用TextWebSocketFrame 123456789101112131415161718192021222324252627public class TextWebSocketFrameHandler extends SimpleChannelInboundHandler&lt;TextWebSocketFrame&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, TextWebSocketFrame msg) throws Exception &#123; System.out.println("收到消息：" + msg.text()); ctx.channel().writeAndFlush(new TextWebSocketFrame("服务器时间：" + LocalDateTime.now())); &#125; @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("handlerAdded: " + ctx.channel().id().asLongText()); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("handlerRemoved: " + ctx.channel().id().asLongText()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("异常发生"); ctx.close(); &#125;&#125; 最后再来看一下客户端代码，我们来进行一下测试。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;WebSocket客户端&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form onsubmit="return false;"&gt; &lt;textarea name="message" style="width:400px;height:200px"&gt;&lt;/textarea&gt; &lt;button onclick="send(this.form.message.value)"&gt;发送数据&lt;/button&gt; &lt;h3&gt;服务端输出：&lt;/h3&gt; &lt;textarea id="responseText" style="width:400px;height:300px"&gt;&lt;/textarea&gt; &lt;button onclick="document.getElementById('responseText').value=''"&gt;清空内容&lt;/button&gt;&lt;/form&gt;&lt;script type="text/javascript"&gt; var socket; if (window.WebSocket) &#123; socket = new WebSocket("ws://localhost:8899/ws"); socket.onmessage = function (event) &#123; var ta = document.getElementById("responseText"); ta.value = ta.value + "\n" + event.data; &#125; socket.onopen = function (event) &#123; var ta = document.getElementById("responseText"); ta.value = "连接开启！"; &#125; socket.onclose = function (event) &#123; var ta = document.getElementById("responseText"); ta.value = ta.value + "连接关闭!"; &#125; &#125; else &#123; alert('浏览器不支持WebSocket!'); &#125; function send(message) &#123; if (!window.WebSocket) &#123; return; &#125; if (socket.readyState == WebSocket.OPEN) &#123; socket.send(message); &#125;else &#123; alert("连接尚未开启！") &#125; &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 运行起来观察一下。 提示连接开启，说明我们的websocket已经连接到了服务端，然后服务端也打印出来handlerAdded的通道ID。 发送一条消息给服务端试试。 从浏览器的界面和开发工具看，我们收到了服务发送给客户端的消息，而且谷歌的开发工具在websocket协议下，还多了一个FRAME来显示浏览器与服务器WebSocket交互的所有数据。服务端的控制台也打印出了浏览器发送给服务端的消息。 通过例子我们了解到Netty通过什么样的方式提供了对WebSocket的支持，为我们简化了大量的代码。希望通过这个例子，我们能更好的理解WebSocket的使用。 其实除了浏览器，现在IOS和Android也有第三方的工具可以来使用WebSocket连接。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-介绍和入门例子]]></title>
    <url>%2F2017%2F05%2F07%2Fnetty1%2F</url>
    <content type="text"><![CDATA[Netty简介http://netty.io Netty is an asynchronous event-driven network application frameworkfor rapid development of maintainable high performance protocol servers &amp; clients. 本系列源码在https://github.com/sail-y/netty Netty能做什么？ 可以像tomcat一样做一个http服务器。 socket开发。 支持长连接开发，例如websocket。 例子1: Http先忘记以前学过的servlet框架，netty并没有实现servlet的规范。 不多说，我们用netty先来写第一个例子，先跑起来试试看，后面再细说。 123456789101112131415161718192021222324252627282930313233343536package com.sail.netty.firstexample;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;/** * @author yangfan * @date 2017/05/20 */public class TestServer &#123; public static void main(String[] args) throws Exception &#123; // 分发任务 EventLoopGroup bossGroup = new NioEventLoopGroup(); // 实际处理的 EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; // 启动服务 ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .childHandler(new TestServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8899).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 12345678910111213141516171819202122package com.sail.netty.firstexample;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.http.HttpServerCodec;/** * @author yangfan * @date 2017/05/20 */public class TestServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast("httpServerCodec", new HttpServerCodec()); pipeline.addLast("testHttpServerHandler", new TestHttpServerHandler()); &#125;&#125; 1234567891011121314151617181920212223242526package com.sail.netty.firstexample;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.handler.codec.http.*;import io.netty.util.CharsetUtil;/** * @author yangfan * @date 2017/05/20 */public class TestHttpServerHandler extends SimpleChannelInboundHandler&lt;HttpObject&gt;&#123; @Override protected void channelRead0(ChannelHandlerContext ctx, HttpObject msg) throws Exception &#123; ByteBuf content = Unpooled.copiedBuffer("Hello World", CharsetUtil.UTF_8); FullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK, content); response.headers().set(HttpHeaderNames.CONTENT_TYPE, "text/plain"); response.headers().set(HttpHeaderNames.CONTENT_LENGTH, content.readableBytes()); ctx.writeAndFlush(response); &#125;&#125; 虽然看起来复杂，但是几乎所有的netty程序的代码流程都是这样的。定义好boss和worker的group-&gt;在childHandler定义ServerInitializer-&gt;在initChannel中定义通道处理器-&gt;实现通道处理器相应的回调方法。 注意loop group是死循环，必须手动停止，接下来我们来测试。 运行TestServer. 命令行测试 1curl localhost:8899 接着我们看一下netty处理器的生命周期，改一下TestHttpServerHandler的代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.sail.netty.firstexample;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.handler.codec.http.*;import io.netty.util.CharsetUtil;import java.net.URI;/** * @author yangfan * @date 2017/05/20 */public class TestHttpServerHandler extends SimpleChannelInboundHandler&lt;HttpObject&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, HttpObject msg) throws Exception &#123; if (msg instanceof HttpRequest) &#123; HttpRequest httpRequest = (HttpRequest) msg; System.out.println("请求方法名：" + httpRequest.method().name()); URI uri = new URI(httpRequest.uri()); if ("/favicon.ico".equals(uri.getPath())) &#123; System.out.println("请求favicon.ico"); return; &#125; ByteBuf content = Unpooled.copiedBuffer("Hello World", CharsetUtil.UTF_8); FullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK, content); response.headers().set(HttpHeaderNames.CONTENT_TYPE, "text/plain"); response.headers().set(HttpHeaderNames.CONTENT_LENGTH, content.readableBytes()); ctx.writeAndFlush(response); &#125; &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelActive"); super.channelActive(ctx); &#125; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelRegistered"); super.channelRegistered(ctx); &#125; @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("handlerAdded"); super.handlerAdded(ctx); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelInactive"); super.channelInactive(ctx); &#125; @Override public void channelUnregistered(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelUnregistered"); super.channelUnregistered(ctx); &#125;&#125; 这个代码用curl和浏览器访问得到的结果是不一样的，因为netty没有遵循servlet的规范，所以有些地方我们得自己处理。 这是curl工具的输出： 这是浏览器第一次访问的输出： 这是浏览器第二次访问的输出： 那是因为curl每次请求完之后就断了。而在http1.1协议下，浏览器有一个keep-alive功能来决定服务端收到请求什么时候关闭这个连接。所以我在多等了一会后，服务器还是自动关闭了连接，控制台还是输出了channelInactive和channelUnregistered 例子2: Socket代码演示 server先写server端代码 MyServer.java 12345678910111213141516171819202122232425262728293031package com.sail.netty.secondexample;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;/** * 和第一个例子并没有太大的区别 * @author yangfan * @date 2017/05/22 */public class MyServer &#123; public static void main(String[] args) throws InterruptedException &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .childHandler(new MyServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8899).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; MyServerHandler.java 1234567891011121314151617181920212223242526272829package com.sail.netty.secondexample;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import java.util.UUID;/** * @author yangfan * @date 2017/05/22 */public class MyServerHandler extends SimpleChannelInboundHandler&lt;String&gt;&#123; @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; System.out.println(ctx.channel().remoteAddress() + ", " + msg); ctx.channel().writeAndFlush("from server: " + UUID.randomUUID()); &#125; /** * 如果出现异常，关闭连接 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; MyServerInitializer .java 123456789101112131415161718192021222324252627282930313233package com.sail.netty.secondexample;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.LengthFieldBasedFrameDecoder;import io.netty.handler.codec.LengthFieldPrepender;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;import io.netty.util.CharsetUtil;/** * * @author yangfan * @date 2017/05/22 */public class MyServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; /** * 针对socket开发，我们处理的方式发生了一些变化 */ @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0,4,0,4)); pipeline.addLast(new LengthFieldPrepender(4)); pipeline.addLast(new StringDecoder(CharsetUtil.UTF_8)); pipeline.addLast(new StringEncoder(CharsetUtil.UTF_8)); // 最后添加我们自己的处理器 pipeline.addLast(new MyServerHandler()); &#125;&#125; client客户端代码 MyClient.java 12345678910111213141516171819202122232425262728package com.sail.netty.secondexample;import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioSocketChannel;/** * @author yangfan * @date 2017/05/22 */public class MyClient &#123; public static void main(String[] args) throws InterruptedException &#123; // 客户端只需要一个EventLoopGroup EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup).channel(NioSocketChannel.class).handler(new MyClientInitializer()); ChannelFuture channelFuture = bootstrap.connect("localhost", 8899).sync(); channelFuture.channel().closeFuture().sync(); &#125;finally &#123; eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125; MyClientHandler 123456789101112131415161718192021222324252627282930313233343536package com.sail.netty.secondexample;import io.netty.channel.ChannelHandler;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.util.concurrent.EventExecutorGroup;import java.time.LocalDateTime;/** * @author yangfan * @date 2017/05/22 */public class MyClientHandler extends SimpleChannelInboundHandler&lt;String&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; System.out.println(ctx.channel().remoteAddress()); System.out.println("client output: " + msg); ctx.writeAndFlush("from clinet: " + LocalDateTime.now()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; /** * 如果不重写这个方法，运行程序后并不会触发数据的传输，因为双方都在等待read，所以要先发送一次消息。 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.channel().writeAndFlush("1"); &#125;&#125; MyClientInitializer.java 12345678910111213141516171819202122232425262728293031323334package com.sail.netty.secondexample;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.LengthFieldBasedFrameDecoder;import io.netty.handler.codec.LengthFieldPrepender;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;import io.netty.util.CharsetUtil;/** * @author yangfan * @date 2017/05/22 */public class MyClientInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; /** * 除了最后一行，跟服务端代码是一样的 * @param ch * @throws Exception */ @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0,4,0,4)); pipeline.addLast(new LengthFieldPrepender(4)); pipeline.addLast(new StringDecoder(CharsetUtil.UTF_8)); pipeline.addLast(new StringEncoder(CharsetUtil.UTF_8)); // 最后添加我们自己的处理器 pipeline.addLast(new MyClientHandler()); &#125;&#125; 例子3：聊天下面做一个简单的消息广播，服务端1个，客户端3个，每个客户端上线的时候，服务端就对每个客户端广播上线的消息，下线同理。客户端发送消息的时候，服务端也会对每一个客户端进行广播。 MyChatServer.java 1234567891011121314151617181920212223242526package com.sail.netty.thirdexample;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;public class MyChatServer &#123; public static void main(String[] args) throws InterruptedException &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .childHandler(new MyChatServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8899).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; MyChatServerHandler.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.sail.netty.thirdexample;import io.netty.channel.Channel;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.channel.group.ChannelGroup;import io.netty.channel.group.DefaultChannelGroup;import io.netty.util.concurrent.GlobalEventExecutor;public class MyChatServerHandler extends SimpleChannelInboundHandler&lt;String&gt; &#123; private static ChannelGroup channelGroup = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE); @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; Channel channel = ctx.channel(); channelGroup.forEach(ch -&gt; &#123; if (channel != ch) &#123; ch.writeAndFlush(channel.remoteAddress() + " 发送的消息：" + msg + "\n"); &#125; else &#123; ch.writeAndFlush("【自己】 " + msg + "\n"); &#125; &#125;); &#125; /** * 如果出现异常，关闭连接 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; /** * 每一个客户端连接的时候，就保存它。 */ @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); // 在添加之前做一个广播 channelGroup.writeAndFlush("【服务器】 - " + channel.remoteAddress() + " 加入\n"); channelGroup.add(channel); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); // 在移除之前做一个广播 channelGroup.writeAndFlush("【服务器】 - " + channel.remoteAddress() + " 离开\n"); // 这个代码在handlerRemoved的时候会自动的从group中移除，所以我们不用写。 // channelGroup.remove(channel); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); System.out.println(channel.remoteAddress() + " 上线"); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); System.out.println(channel.remoteAddress() + " 下线"); &#125;&#125; MyChatServerInitializer.java 1234567891011121314151617181920212223package com.sail.netty.thirdexample;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.DelimiterBasedFrameDecoder;import io.netty.handler.codec.Delimiters;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;import io.netty.util.CharsetUtil;public class MyChatServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new DelimiterBasedFrameDecoder(4096, Delimiters.lineDelimiter())); pipeline.addLast(new StringDecoder(CharsetUtil.UTF_8)); pipeline.addLast(new StringEncoder(CharsetUtil.UTF_8)); pipeline.addLast(new MyChatServerHandler()); &#125;&#125; MyChatClient.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.sail.netty.thirdexample;import com.sail.netty.secondexample.MyClientInitializer;import io.netty.bootstrap.Bootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioSocketChannel;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;/** * @author yangfan * @date 2017/05/22 */public class MyChatClient &#123; public static void main(String[] args) throws Exception &#123; // 客户端只需要一个EventLoopGroup EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup).channel(NioSocketChannel.class).handler(new MyChatClientInitializer()); Channel channel = bootstrap.connect("localhost", 8899).sync().channel(); // channel.closeFuture().sync(); // 写一个死循环不断的去读取用户在控制台的输入 BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); for (;;) &#123; channel.writeAndFlush(br.readLine() + "\r\n"); &#125; &#125;finally &#123; eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125; MyChatClientHandler.java 123456789101112131415161718package com.sail.netty.thirdexample;import io.netty.channel.ChannelHandler;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.util.concurrent.EventExecutorGroup;/** * @author yangfan * @date 2017/05/22 */public class MyChatClientHandler extends SimpleChannelInboundHandler&lt;String&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; System.out.println(msg); &#125;&#125; MyChatClientInitializer.java 1234567891011121314151617181920212223242526package com.sail.netty.thirdexample;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.DelimiterBasedFrameDecoder;import io.netty.handler.codec.Delimiters;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;import io.netty.util.CharsetUtil;/** * @author yangfan * @date 2017/05/22 */public class MyChatClientInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new DelimiterBasedFrameDecoder(4096, Delimiters.lineDelimiter())); pipeline.addLast(new StringDecoder(CharsetUtil.UTF_8)); pipeline.addLast(new StringEncoder(CharsetUtil.UTF_8)); pipeline.addLast(new MyChatClientHandler()); &#125;&#125; 例子4：heartbeat（心跳）下面演示一个集群中常见的心跳用netty的实现方式。 MyServer.java 123456789101112131415161718192021222324252627282930313233343536package com.sail.netty.forthexample;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;import io.netty.handler.logging.LogLevel;import io.netty.handler.logging.LoggingHandler;/** * 心跳检测的Netty实现 * @author yangfan * @date 2017/05/22 */public class MyServer &#123; public static void main(String[] args) throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) // handler是针对于bossGroup的 .handler(new LoggingHandler(LogLevel.INFO)) // childHandler是针对于workerGroup的 .childHandler(new MyServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8899).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; MyServerHandler.java 12345678910111213141516171819202122232425262728293031323334353637package com.sail.netty.forthexample;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import io.netty.handler.timeout.IdleStateEvent;/** * @author yangfan * @date 2017/05/22 */public class MyServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; if (evt instanceof IdleStateEvent) &#123; IdleStateEvent event = (IdleStateEvent) evt; String eventType = null; switch (event.state()) &#123; case READER_IDLE: eventType = "读空闲"; break; case WRITER_IDLE: eventType = "写空闲"; break; case ALL_IDLE: eventType = "读写空闲"; break; &#125; System.out.println(ctx.channel().remoteAddress() + " 超时事件： " + eventType); ctx.channel().close(); &#125; &#125;&#125; MyServerInitializer.java 123456789101112131415161718192021222324252627package com.sail.netty.forthexample;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.timeout.IdleStateHandler;import java.util.concurrent.TimeUnit;/** * @author yangfan * @date 2017/05/22 */public class MyServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; /** * IdleStateHandler可以检测连接的3种状态 */ @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new IdleStateHandler(5, 7, 3, TimeUnit.SECONDS)); pipeline.addLast(new MyServerHandler()); &#125;&#125;]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-JODA实战]]></title>
    <url>%2F2017%2F04%2F08%2Fjdk8-10%2F</url>
    <content type="text"><![CDATA[Java 8日期/时间（ Date/Time）API是开发人员最受追捧的变化之一，Java从一开始就没有对日期时间处理的一致性方法，因此日期/时间API也是除Java核心API以外另一项倍受欢迎的内容。 为什么我们需要新的Java日期/时间API？ 在开始研究Java 8日期/时间API之前，让我们先来看一下为什么我们需要这样一个新的API。在Java中，现有的与日期和时间相关的类存在诸多问题，其中有： Java的日期/时间类的定义并不一致，在java.util和java.sql的包中都有日期类，此外用于格式化和解析的类在java.text包中定义。 java.util.Date同时包含日期和时间，而java.sql.Date仅包含日期，将其纳入java.sql包并不合理。另外这两个类都有相同的名字，这本身就是一个非常糟糕的设计。 对于时间、时间戳、格式化以及解析，并没有一些明确定义的类。对于格式化和解析的需求，我们有java.text.DateFormat抽象类，但通常情况下，SimpleDateFormat类被用于此类需求。 所有的日期类都是可变的，因此他们都不是线程安全的，这是Java日期类最大的问题之一。 日期类并不提供国际化，没有时区支持，因此Java引入了java.util.Calendar和java.util.TimeZone类，但他们同样存在上述所有的问题。 在现有的日期和日历类中定义的方法还存在一些其他的问题，但以上问题已经很清晰地表明：Java需要一个健壮的日期/时间类。这也是为什么Joda Time在Java日期/时间需求中扮演了高质量替换的重要角色。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100// 获取当前时间LocalDate localDate = LocalDate.now();System.out.println(localDate);System.out.println(localDate.getYear() + ", " + localDate.getMonthValue() + ", " + localDate.getDayOfMonth());System.out.println("------------");// 生成日期LocalDate localDate2 = LocalDate.of(2017, 4, 8);System.out.println(localDate2);System.out.println("------------");// 只关注月份和日期LocalDate localDate3 = LocalDate.of(2010, 3, 25);MonthDay monthDay = MonthDay.of(localDate3.getMonth(), localDate3.getDayOfMonth());MonthDay monthDay2 = MonthDay.from(LocalDate.of(2011, 3, 25));System.out.println(monthDay.equals(monthDay2));System.out.println("------------");// 时分秒LocalTime time = LocalTime.now();System.out.println(time);LocalTime time2 = time.plusHours(3).plusMinutes(20);System.out.println(time2);System.out.println("------------");LocalDate localDate1 = localDate.plus(2, ChronoUnit.WEEKS);System.out.println(localDate1);System.out.println("------------");LocalDate localDate4 = localDate.minus(2, ChronoUnit.MONTHS);System.out.println(localDate4);System.out.println("------------");Clock clock = Clock.systemDefaultZone();System.out.println(clock.millis());System.out.println("------------");LocalDate localDate5 = LocalDate.now();LocalDate localDate6 = LocalDate.of(2017, 4, 25);System.out.println(localDate5.isAfter(localDate6));System.out.println(localDate5.isBefore(localDate6));System.out.println(localDate5.equals(localDate6));System.out.println("------------");Set&lt;String&gt; availableZoneIds = ZoneId.getAvailableZoneIds();System.out.println(availableZoneIds);System.out.println("------------");ZoneId zoneId = ZoneId.of("Asia/Shanghai");LocalDateTime localDateTime = LocalDateTime.now();System.out.println(localDateTime);ZonedDateTime zonedDateTime = ZonedDateTime.of(localDateTime, zoneId);System.out.println(zonedDateTime);System.out.println("------------");YearMonth yearMonth = YearMonth.now();System.out.println(yearMonth);System.out.println(yearMonth.lengthOfMonth());System.out.println(yearMonth.isLeapYear());System.out.println("------------");YearMonth yearMonth1 = YearMonth.of(2016, 2);System.out.println(yearMonth1);System.out.println(yearMonth1.lengthOfMonth());System.out.println(yearMonth1.lengthOfYear());System.out.println(yearMonth1.isLeapYear());System.out.println("------------");LocalDate localDate7 = LocalDate.now();LocalDate localDate8 = LocalDate.of(2017, 3, 25);Period period = Period.between(localDate7, localDate8);System.out.println(period.getYears());System.out.println(period.getMonths());System.out.println(period.getDays());System.out.println("------------");System.out.println(Instant.now()); 时间差123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// 2014-03-18LocalDate date = LocalDate.of(2014, 3, 18);int year = date.getYear();Month month = date.getMonth();int day = date.getDayOfMonth();DayOfWeek dow = date.getDayOfWeek();int len = date.lengthOfMonth();// 闰年boolean leap = date.isLeapYear();System.out.println(leap);LocalDate today = LocalDate.now();// TemporalField方式获取year = date.get(ChronoField.YEAR);// 时间// 13:45:20LocalTime time = LocalTime.of(13, 45, 20);int hour = time.getHour();int minute = time.getMinute();int second = time.getSecond();// 从字符串转换date = LocalDate.parse("2014-03-18");time = LocalTime.parse("13:45:20");// LocalDateTime// 2014-03-18T13:45:20LocalDateTime dt1 = LocalDateTime.of(2014, Month.MARCH, 18, 13, 45, 20);LocalDateTime dt2 = LocalDateTime.of(date, time);LocalDateTime dt3 = date.atTime(13, 45, 20);LocalDateTime dt4 = date.atTime(time);LocalDateTime dt5 = time.atDate(date);// 机器嗦理解的时间类InstantInstant.ofEpochSecond(0);// 2秒后加上一千秒Instant.ofEpochSecond(2, 1_000);Instant.now().toEpochMilli();// 所有类都实现了Temporal接口， Temporal接口定义了如何读取和操纵为时间建模的对象的值// Duration类主要用于以秒和纳秒恒衡量的时间的长短Duration d1 = Duration.between(time, time.plusHours(1));System.out.println(d1.getSeconds());// Period则以年、月或日的方式对多个时间单位建模Period tenDays = Period.between(LocalDate.of(2014, 3, 8), LocalDate.of(2014, 3, 18));System.out.println(tenDays.getDays());// Duration和Period还有一些工厂类Duration threeMinutes = Duration.ofMinutes(3);threeMinutes = Duration.of(3, ChronoUnit.MINUTES);Period nineDays = Period.ofDays(9);Period threeWeeks = Period.ofWeeks(3);Period twoYearsSixMonthsOneDay = Period.of(2, 6, 1);System.out.println(dt1.plus(twoYearsSixMonthsOneDay));// 取今天的0点LocalDateTime localDateTime = LocalDate.now().atTime(0, 0, 0);long beginOfDay = localDateTime.toEpochSecond(ZoneOffset.of("+08:00"));System.out.println(localDateTime);System.out.println(beginOfDay); 计算&amp;格式化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// with是直接修改对应的属性LocalDate date1 = LocalDate.of(2014, 3, 18);// 2011-03-18LocalDate date2 = date1.withYear(2011);// 2011-03-25LocalDate date3 = date2.withDayOfMonth(25);// 2011-09-25LocalDate date4 = date3.with(ChronoField.MONTH_OF_YEAR, 9);// plus和minus是相对方式修改属性// 2014-03-18date1 = LocalDate.of(2014, 3, 18);// 2014-03-25date2 = date1.plusWeeks(1);// 2011-03-25date3 = date2.minusYears(3);// 2011-09-25date4 = date3.plus(6, ChronoUnit.MONTHS);// 利用TemporalAdjuster进行复杂处理date1 = LocalDate.of(2014, 3, 18);// 下一个星期天 2014-03-23date2 = date1.with(nextOrSame(DayOfWeek.SUNDAY));// 这个月最后一天 2014-03-31date3 = date2.with(lastDayOfMonth());// 只取工作日date3 = date2.with(temporal -&gt; &#123; DayOfWeek dow = DayOfWeek.of(temporal.get(DAY_OF_WEEK)); int dayToAdd = 1; if (dow == DayOfWeek.FRIDAY) &#123; dayToAdd = 3; &#125; else if (dow == DayOfWeek.SATURDAY) &#123; dayToAdd = 2; &#125; return temporal.plus(dayToAdd, DAYS);&#125;);TemporalAdjuster nextWorkingDay = ofDateAdjuster(temporal -&gt; &#123; DayOfWeek dow = DayOfWeek.of(temporal.get(DAY_OF_WEEK)); int dayToAdd = 1; if (dow == DayOfWeek.FRIDAY) &#123; dayToAdd = 3; &#125; else if (dow == DayOfWeek.SATURDAY) &#123; dayToAdd = 2; &#125; return temporal.plus(dayToAdd, DAYS);&#125;);date2.with(nextWorkingDay);// 格式化操作LocalDate date = LocalDate.of(2014, 3, 18);String s1 = date.format(DateTimeFormatter.BASIC_ISO_DATE);String s2 = date.format(DateTimeFormatter.ISO_LOCAL_DATE);date1 = LocalDate.parse("20140318", DateTimeFormatter.BASIC_ISO_DATE);date2 = LocalDate.parse("2014-03-18", DateTimeFormatter.ISO_LOCAL_DATE);DateTimeFormatter italianFormatter = DateTimeFormatter.ofPattern("d. MMMM yyyy", Locale.ITALIAN);date1 = LocalDate.of(2014, 3, 18);// 18. marzo 2014String formattedDate = date.format(italianFormatter);date2 = LocalDate.parse(formattedDate, italianFormatter);italianFormatter = new DateTimeFormatterBuilder().appendText(ChronoField.DAY_OF_MONTH) .appendLiteral(". ") .appendText(ChronoField.MONTH_OF_YEAR) .appendLiteral(" ") .appendText(ChronoField.YEAR) .parseCaseInsensitive() .toFormatter(Locale.ITALIAN); 时区12345678910111213141516171819ZoneId romeZone = ZoneId.of("Europe/Rome");LocalDate date = LocalDate.of(2014, Month.MARCH, 18);ZonedDateTime zdt1 = date.atStartOfDay(romeZone);LocalDateTime dateTime = LocalDateTime.of(2014, Month.MARCH, 18, 13, 45);ZonedDateTime zdt2 = dateTime.atZone(romeZone);Instant instant = Instant.now();ZonedDateTime zdt3 = instant.atZone(romeZone);// Instant instantFromDateTime = dateTime.toInstant(romeZone);LocalDateTime timeFromInstant = LocalDateTime.ofInstant(instant, romeZone);// 时区偏差ZoneOffset newYourkOffset = ZoneOffset.of("-05:00");OffsetDateTime dateTimeInNewYork = OffsetDateTime.of(dateTime, newYourkOffset);MinguoDate minguoDate = MinguoDate.now();System.out.println(minguoDate);]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-Stream流底层原理和实现[转]]]></title>
    <url>%2F2017%2F04%2F08%2Fjdk8-9%2F</url>
    <content type="text"><![CDATA[这篇文章发现别人写的好货，直接转过来看看。http://www.cnblogs.com/CarpenterLee/archive/2017/03/28/6637118.html Stream Pipelines前面我们已经学会如何使用Stream API，用起来真的很爽，但简洁的方法下面似乎隐藏着无尽的秘密，如此强大的API是如何实现的呢？比如Pipeline是怎么执行的，每次方法调用都会导致一次迭代吗？自动并行又是怎么做到的，线程个数是多少？本节我们学习Stream流水线的原理，这是Stream实现的关键所在。 首先回顾一下容器执行Lambda表达式的方式，以ArrayList.forEach()方法为例，具体代码如下：12345678// ArrayList.forEach()public void forEach(Consumer&lt;? super E&gt; action) &#123; ... for (int i=0; modCount == expectedModCount &amp;&amp; i &lt; size; i++) &#123; action.accept(elementData[i]);// 回调方法 &#125; ...&#125; 我们看到ArrayList.forEach()方法的主要逻辑就是一个for循环，在该for循环里不断调用action.accept()回调方法完成对元素的遍历。这完全没有什么新奇之处，回调方法在Java GUI的监听器中广泛使用。Lambda表达式的作用就是相当于一个回调方法，这很好理解。 Stream API中大量使用Lambda表达式作为回调方法，但这并不是关键。理解Stream我们更关心的是另外两个问题：流水线和自动并行。使用Stream或许很容易写入如下形式的代码： 12345int longestStringLengthStartingWithA = strings.stream() .filter(s -&gt; s.startsWith("A")) .mapToInt(String::length) .max(); 上述代码求出以字母A开头的字符串的最大长度，一种直白的方式是为每一次函数调用都执一次迭代，这样做能够实现功能，但效率上肯定是无法接受的。类库的实现着使用流水线（Pipeline）的方式巧妙的避免了多次迭代，其基本思想是在一次迭代中尽可能多的执行用户指定的操作。为讲解方便我们汇总了Stream的所有操作。 Stream操作分类中间操作(Intermediate operations)无状态(Stateless)unordered() filter() map() mapToInt() mapToLong() mapToDouble() flatMap() flatMapToInt() flatMapToLong() flatMapToDouble() peek()有状态(Stateful)distinct() sorted() sorted() limit() skip() 结束操作(Terminal operations)非短路操作forEach() forEachOrdered() toArray() reduce() collect() max() min() count()短路操作(short-circuiting)anyMatch() allMatch() noneMatch() findFirst() findAny() Stream上的所有操作分为两类：中间操作和结束操作，中间操作只是一种标记，只有结束操作才会触发实际计算。中间操作又可以分为无状态的(Stateless)和有状态的(Stateful)，无状态中间操作是指元素的处理不受前面元素的影响，而有状态的中间操作必须等到所有元素处理之后才知道最终结果，比如排序是有状态操作，在读取所有元素之前并不能确定排序结果；结束操作又可以分为短路操作和非短路操作，短路操作是指不用处理全部元素就可以返回结果，比如找到第一个满足条件的元素。之所以要进行如此精细的划分，是因为底层对每一种情况的处理方式不同。 一种直白的实现方式 仍然考虑上述求最长字符串的程序，一种直白的流水线实现方式是为每一次函数调用都执一次迭代，并将处理中间结果放到某种数据结构中（比如数组，容器等）。具体说来，就是调用filter()方法后立即执行，选出所有以A开头的字符串并放到一个列表list1中，之后让list1传递给mapToInt()方法并立即执行，生成的结果放到list2中，最后遍历list2找出最大的数字作为最终结果。程序的执行流程如如所示： 这样做实现起来非常简单直观，但有两个明显的弊端： 迭代次数多。迭代次数跟函数调用的次数相等。 频繁产生中间结果。每次函数调用都产生一次中间结果，存储开销无法接受。 这些弊端使得效率底下，根本无法接受。如果不使用Stream API我们都知道上述代码该如何在一次迭代中完成，大致是如下形式： 1234567int longest = 0;for(String str : strings)&#123; if(str.startsWith("A"))&#123;// 1. filter(), 保留以A开头的字符串 int len = str.length();// 2. mapToInt(), 转换成长度 longest = Math.max(len, longest);// 3. max(), 保留最长的长度 &#125;&#125; 采用这种方式我们不但减少了迭代次数，也避免了存储中间结果，显然这就是流水线，因为我们把三个操作放在了一次迭代当中。只要我们事先知道用户意图，总是能够采用上述方式实现跟Stream API等价的功能，但问题是Stream类库的设计者并不知道用户的意图是什么。如何在无法假设用户行为的前提下实现流水线，是类库的设计者要考虑的问题。 Stream流水线解决方案我们大致能够想到，应该采用某种方式记录用户每一步的操作，当用户调用结束操作时将之前记录的操作叠加到一起在一次迭代中全部执行掉。沿着这个思路，有几个问题需要解决： 用户的操作如何记录？ 操作如何叠加？ 叠加之后的操作如何执行？ 执行后的结果（如果有）在哪里？ &gt;&gt; 操作如何记录 注意这里使用的是“操作(operation)”一词，指的是“Stream中间操作”的操作，很多Stream操作会需要一个回调函数（Lambda表达式），因此一个完整的操作是&lt;数据来源，操作，回调函数&gt;构成的三元组。Stream中使用Stage的概念来描述一个完整的操作，并用某种实例化后的PipelineHelper来代表Stage，将具有先后顺序的各个Stage连到一起，就构成了整个流水线。跟Stream相关类和接口的继承关系图示。 还有IntPipeline, LongPipeline, DoublePipeline没在图中画出，这三个类专门为三种基本类型（不是包装类型）而定制的，跟ReferencePipeline是并列关系。图中Head用于表示第一个Stage，即调用调用诸如Collection.stream()方法产生的Stage，很显然这个Stage里不包含任何操作；StatelessOp和StatefulOp分别表示无状态和有状态的Stage，对应于无状态和有状态的中间操作。 Stream流水线组织结构示意图如下： 图中通过Collection.stream()方法得到Head也就是stage0，紧接着调用一系列的中间操作，不断产生新的Stream。这些Stream对象以双向链表的形式组织在一起，构成整个流水线，由于每个Stage都记录了前一个Stage和本次的操作以及回调函数，依靠这种结构就能建立起对数据源的所有操作。这就是Stream记录操作的方式。 &gt;&gt; 操作如何叠加以上只是解决了操作记录的问题，要想让流水线起到应有的作用我们需要一种将所有操作叠加到一起的方案。你可能会觉得这很简单，只需要从流水线的head开始依次执行每一步的操作（包括回调函数）就行了。这听起来似乎是可行的，但是你忽略了前面的Stage并不知道后面Stage到底执行了哪种操作，以及回调函数是哪种形式。换句话说，只有当前Stage本身才知道该如何执行自己包含的动作。这就需要有某种协议来协调相邻Stage之间的调用关系。 这种协议由Sink接口完成，Sink接口包含的方法如下表所示： 方法名作用void begin(long size)开始遍历元素之前调用该方法，通知Sink做好准备。void end()所有元素遍历完成之后调用，通知Sink没有更多的元素了。boolean cancellationRequested()是否可以结束操作，可以让短路操作尽早结束。void accept(T t)遍历元素时调用，接受一个待处理元素，并对元素进行处理。Stage把自己包含的操作和回调方法封装到该方法里，前一个Stage只需要调用当前Stage.accept(T t)方法就行了。 有了上面的协议，相邻Stage之间调用就很方便了，每个Stage都会将自己的操作封装到一个Sink里，前一个Stage只需调用后一个Stage的accept()方法即可，并不需要知道其内部是如何处理的。当然对于有状态的操作，Sink的begin()和end()方法也是必须实现的。比如Stream.sorted()是一个有状态的中间操作，其对应的Sink.begin()方法可能创建一个乘放结果的容器，而accept()方法负责将元素添加到该容器，最后end()负责对容器进行排序。对于短路操作，Sink.cancellationRequested()也是必须实现的，比如Stream.findFirst()是短路操作，只要找到一个元素，cancellationRequested()就应该返回true，以便调用者尽快结束查找。Sink的四个接口方法常常相互协作，共同完成计算任务。实际上Stream API内部实现的的本质，就是如何重载Sink的这四个接口方法。 有了Sink对操作的包装，Stage之间的调用问题就解决了，执行时只需要从流水线的head开始对数据源依次调用每个Stage对应的Sink.{begin(), accept(), cancellationRequested(), end()}方法就可以了。一种可能的Sink.accept()方法流程是这样的： 1234void accept(U u)&#123; 1. 使用当前Sink包装的回调函数处理u 2. 将处理结果传递给流水线下游的Sink&#125; Sink接口的其他几个方法也是按照这种[处理-&gt;转发]的模型实现。下面我们结合具体例子看看Stream的中间操作是如何将自身的操作包装成Sink以及Sink是如何将处理结果转发给下一个Sink的。先看Stream.map()方法： 1234567891011121314151617// Stream.map()，调用该方法将产生一个新的Streampublic final &lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super P_OUT, ? extends R&gt; mapper) &#123; ... return new StatelessOp&lt;P_OUT, R&gt;(this, StreamShape.REFERENCE, StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) &#123; @Override /*opWripSink()方法返回由回调函数包装而成Sink*/ Sink&lt;P_OUT&gt; opWrapSink(int flags, Sink&lt;R&gt; downstream) &#123; return new Sink.ChainedReference&lt;P_OUT, R&gt;(downstream) &#123; @Override public void accept(P_OUT u) &#123; R r = mapper.apply(u);// 1. 使用当前Sink包装的回调函数mapper处理u downstream.accept(r);// 2. 将处理结果传递给流水线下游的Sink &#125; &#125;; &#125; &#125;;&#125; 上述代码看似复杂，其实逻辑很简单，就是将回调函数mapper包装到一个Sink当中。由于Stream.map()是一个无状态的中间操作，所以map()方法返回了一个StatelessOp内部类对象（一个新的Stream），调用这个新Stream的opWripSink()方法将得到一个包装了当前回调函数的Sink。 再来看一个复杂一点的例子。Stream.sorted()方法将对Stream中的元素进行排序，显然这是一个有状态的中间操作，因为读取所有元素之前是没法得到最终顺序的。抛开模板代码直接进入问题本质，sorted()方法是如何将操作封装成Sink的呢？sorted()一种可能封装的Sink代码如下： 123456789101112131415161718192021222324252627282930313233// Stream.sort()方法用到的Sink实现class RefSortingSink&lt;T&gt; extends AbstractRefSortingSink&lt;T&gt; &#123; private ArrayList&lt;T&gt; list;// 存放用于排序的元素 RefSortingSink(Sink&lt;? super T&gt; downstream, Comparator&lt;? super T&gt; comparator) &#123; super(downstream, comparator); &#125; @Override public void begin(long size) &#123; ... // 创建一个存放排序元素的列表 list = (size &gt;= 0) ? new ArrayList&lt;T&gt;((int) size) : new ArrayList&lt;T&gt;(); &#125; @Override public void end() &#123; list.sort(comparator);// 只有元素全部接收之后才能开始排序 downstream.begin(list.size()); if (!cancellationWasRequested) &#123;// 下游Sink不包含短路操作 list.forEach(downstream::accept);// 2. 将处理结果传递给流水线下游的Sink &#125; else &#123;// 下游Sink包含短路操作 for (T t : list) &#123;// 每次都调用cancellationRequested()询问是否可以结束处理。 if (downstream.cancellationRequested()) break; downstream.accept(t);// 2. 将处理结果传递给流水线下游的Sink &#125; &#125; downstream.end(); list = null; &#125; @Override public void accept(T t) &#123; list.add(t);// 1. 使用当前Sink包装动作处理t，只是简单的将元素添加到中间列表当中 &#125;&#125; 上述代码完美的展现了Sink的四个接口方法是如何协同工作的： 首先beging()方法告诉Sink参与排序的元素个数，方便确定中间结果容器的的大小； 之后通过accept()方法将元素添加到中间结果当中，最终执行时调用者会不断调用该方法，直到遍历所有元素； 最后end()方法告诉Sink所有元素遍历完毕，启动排序步骤，排序完成后将结果传递给下游的Sink； 如果下游的Sink是短路操作，将结果传递给下游时不断询问下游cancellationRequested()是否可以结束处理。 &gt;&gt; 叠加之后的操作如何执行 Sink完美封装了Stream每一步操作，并给出了[处理-&gt;转发]的模式来叠加操作。这一连串的齿轮已经咬合，就差最后一步拨动齿轮启动执行。是什么启动这一连串的操作呢？也许你已经想到了启动的原始动力就是结束操作(Terminal Operation)，一旦调用某个结束操作，就会触发整个流水线的执行。 结束操作之后不能再有别的操作，所以结束操作不会创建新的流水线阶段(Stage)，直观的说就是流水线的链表不会在往后延伸了。结束操作会创建一个包装了自己操作的Sink，这也是流水线中最后一个Sink，这个Sink只需要处理数据而不需要将结果传递给下游的Sink（因为没有下游）。对于Sink的[处理-&gt;转发]模型，结束操作的Sink就是调用链的出口。 我们再来考察一下上游的Sink是如何找到下游Sink的。一种可选的方案是在PipelineHelper中设置一个Sink字段，在流水线中找到下游Stage并访问Sink字段即可。但Stream类库的设计者没有这么做，而是设置了一个Sink AbstractPipeline.opWrapSink(int flags, Sink downstream)方法来得到Sink，该方法的作用是返回一个新的包含了当前Stage代表的操作以及能够将结果传递给downstream的Sink对象。为什么要产生一个新对象而不是返回一个Sink字段？这是因为使用opWrapSink()可以将当前操作与下游Sink（上文中的downstream参数）结合成新Sink。试想只要从流水线的最后一个Stage开始，不断调用上一个Stage的opWrapSink()方法直到最开始（不包括stage0，因为stage0代表数据源，不包含操作），就可以得到一个代表了流水线上所有操作的Sink，用代码表示就是这样： 12345678910// AbstractPipeline.wrapSink()// 从下游向上游不断包装Sink。如果最初传入的sink代表结束操作，// 函数返回时就可以得到一个代表了流水线上所有操作的Sink。final &lt;P_IN&gt; Sink&lt;P_IN&gt; wrapSink(Sink&lt;E_OUT&gt; sink) &#123; ... for (AbstractPipeline p=AbstractPipeline.this; p.depth &gt; 0; p=p.previousStage) &#123; sink = p.opWrapSink(p.previousStage.combinedFlags, sink); &#125; return (Sink&lt;P_IN&gt;) sink;&#125; 现在流水线上从开始到结束的所有的操作都被包装到了一个Sink里，执行这个Sink就相当于执行整个流水线，执行Sink的代码如下： 12345678910// AbstractPipeline.copyInto(), 对spliterator代表的数据执行wrappedSink代表的操作。final &lt;P_IN&gt; void copyInto(Sink&lt;P_IN&gt; wrappedSink, Spliterator&lt;P_IN&gt; spliterator) &#123; ... if (!StreamOpFlag.SHORT_CIRCUIT.isKnown(getStreamAndOpFlags())) &#123; wrappedSink.begin(spliterator.getExactSizeIfKnown());// 通知开始遍历 spliterator.forEachRemaining(wrappedSink);// 迭代 wrappedSink.end();// 通知遍历结束 &#125; ...&#125; 上述代码首先调用wrappedSink.begin()方法告诉Sink数据即将到来，然后调用spliterator.forEachRemaining()方法对数据进行迭代（Spliterator是容器的一种迭代器，参阅），最后调用wrappedSink.end()方法通知Sink数据处理结束。逻辑如此清晰。 &gt;&gt; 执行后的结果在哪里最后一个问题是流水线上所有操作都执行后，用户所需要的结果（如果有）在哪里？首先要说明的是不是所有的Stream结束操作都需要返回结果，有些操作只是为了使用其副作用(Side-effects)，比如使用Stream.forEach()方法将结果打印出来就是常见的使用副作用的场景（事实上，除了打印之外其他场景都应避免使用副作用），对于真正需要返回结果的结束操作结果存在哪里呢？ 特别说明：副作用不应该被滥用，也许你会觉得在Stream.forEach()里进行元素收集是个不错的选择，就像下面代码中那样，但遗憾的是这样使用的正确性和效率都无法保证，因为Stream可能会并行执行。大多数使用副作用的地方都可以使用归约操作.md)更安全和有效的完成。 12345678// 错误的收集方式ArrayList&lt;String&gt; results = new ArrayList&lt;&gt;();stream.filter(s -&gt; pattern.matcher(s).matches()) .forEach(s -&gt; results.add(s)); // Unnecessary use of side-effects!// 正确的收集方式List&lt;String&gt;results = stream.filter(s -&gt; pattern.matcher(s).matches()) .collect(Collectors.toList()); // No side-effects! 回到流水线执行结果的问题上来，需要返回结果的流水线结果存在哪里呢？这要分不同的情况讨论，下表给出了各种有返回结果的Stream结束操作。 返回类型对应的结束操作booleananyMatch() allMatch() noneMatch()OptionalfindFirst() findAny()归约结果reduce() collect()数组toArray() 对于表中返回boolean或者Optional的操作（Optional是存放 一个 值的容器）的操作，由于只返回一个值，只需要在对应的Sink中记录这个值，等到执行结束时返回就可以了。 对于归约操作，最终结果放在用户调用时指定的容器中（容器类型通过收集器.md#收集器)指定）。collect(), reduce(), max(), min()都是归约操作，虽然max()和min()也是返回一个Optional，但事实上底层是通过调用reduce().md#多面手reduce)方法实现的。 对于返回是数组的情况，毫无疑问的结果会放在数组当中。这么说当然是对的，但在最终返回数组之前，结果其实是存储在一种叫做Node的数据结构中的。Node是一种多叉树结构，元素存储在树的叶子当中，并且一个叶子节点可以存放多个元素。这样做是为了并行执行方便。关于Node的具体结构，我们会在下一节探究Stream如何并行执行时给出详细说明。 结语本文详细介绍了Stream流水线的组织方式和执行过程，学习本文将有助于理解原理并写出正确的Stream代码，同时打消你对Stream API效率方面的顾虑。如你所见，Stream API实现如此巧妙，即使我们使用外部迭代手动编写等价代码，也未必更加高效。]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-Collectors工厂类源码探索]]></title>
    <url>%2F2017%2F02%2F26%2Fjdk8-8%2F</url>
    <content type="text"><![CDATA[Collectors静态工厂类对于Collectors静态工厂类来说，其实现一共分为两种情况： 通过CollectorImpl来实现。 通过reducing方法来实现, reducing方法本身又是通过CollectorImpl实现的。 12345public static &lt;T&gt; Collector&lt;T, ?, List&lt;T&gt;&gt; toList() &#123; return new CollectorImpl&lt;&gt;((Supplier&lt;List&lt;T&gt;&gt;) ArrayList::new, List::add, (left, right) -&gt; &#123; left.addAll(right); return left; &#125;, CH_ID);&#125; 这个方法现在来看也很简单了，3个参数，supplier，accumulator，combiner，然后characteristics是CH_ID，就是IDENTITY_FINISH，那么也就不需要finisher方法了。 toList是实现是ArrayList，如果我们需要LinkedList怎么办呢。我们需要用toCollection方法，接收一个Supplier，指定为LinkedList:new就可以了。 12345public static &lt;T, C extends Collection&lt;T&gt;&gt; Collector&lt;T, ?, C&gt; toCollection(Supplier&lt;C&gt; collectionFactory) &#123; return new CollectorImpl&lt;&gt;(collectionFactory, Collection&lt;T&gt;::add, (r1, r2) -&gt; &#123; r1.addAll(r2); return r1; &#125;, CH_ID);&#125; 下面的joining就是用了finisher，因为A和R类型不一样(StringBuilder和String)，需要用finishder来转换一下。 123456public static Collector&lt;CharSequence, ?, String&gt; joining() &#123; return new CollectorImpl&lt;CharSequence, StringBuilder, String&gt;( StringBuilder::new, StringBuilder::append, (r1, r2) -&gt; &#123; r1.append(r2); return r1; &#125;, StringBuilder::toString, CH_NOID);&#125; collectingAndThen的实现就是把FINISH_IDENTITY的特性给移除了，然后再给finisher()加一个andThen操作，对结果再次进行一个转换。 1234567891011121314151617public static&lt;T,A,R,RR&gt; Collector&lt;T,A,RR&gt; collectingAndThen(Collector&lt;T,A,R&gt; downstream, Function&lt;R,RR&gt; finisher) &#123; Set&lt;Collector.Characteristics&gt; characteristics = downstream.characteristics(); if (characteristics.contains(Collector.Characteristics.IDENTITY_FINISH)) &#123; if (characteristics.size() == 1) characteristics = Collectors.CH_NOID; else &#123; characteristics = EnumSet.copyOf(characteristics); characteristics.remove(Collector.Characteristics.IDENTITY_FINISH); characteristics = Collections.unmodifiableSet(characteristics); &#125; &#125; return new CollectorImpl&lt;&gt;(downstream.supplier(), downstream.accumulator(), downstream.combiner(), downstream.finisher().andThen(finisher), characteristics);&#125; mapping函数，就是在累加器执行之前，对参数进行一个类型转换，这也会导致返回类型改变。举例如下，首先根据人的城市分组， 1Map&lt;City, Set&lt;String&gt;&gt; lastNamesByCity = people.stream().collect(groupingBy(Person::getCity,mapping(Person::getLastName, toSet()))); 源码如下,结合上面的例子来看，就是toSet()本来应该是Set，但是因为mapping把累加器的类型由Person改成了String。12345678public static &lt;T, U, A, R&gt; Collector&lt;T, ?, R&gt; mapping(Function&lt;? super T, ? extends U&gt; mapper, Collector&lt;? super U, A, R&gt; downstream) &#123; BiConsumer&lt;A, ? super U&gt; downstreamAccumulator = downstream.accumulator(); return new CollectorImpl&lt;&gt;(downstream.supplier(), (r, t) -&gt; downstreamAccumulator.accept(r, mapper.apply(t)), downstream.combiner(), downstream.finisher(), downstream.characteristics());&#125; counting方法，就是初始值为0，每个元素映射成1，然后再加起来。后面的minBy,maxBy也都是通过reducing来实现的。 123public static &lt;T&gt; Collector&lt;T, ?, Long&gt; counting() &#123; return reducing(0L, e -&gt; 1L, Long::sum);&#125; summarizingInt方法，为什么第一个参数是一个数组呢，而不是直接一个值，因为累加器accumulator是BiConsumer，接受2个参数，不返回值，如果要进行累加的话，只能通过容器来累加，如果直接是一个值的话，没办法传递，也没有办法累加。 1234567public static &lt;T&gt; Collector&lt;T, ?, Integer&gt; summingInt(ToIntFunction&lt;? super T&gt; mapper) &#123; return new CollectorImpl&lt;&gt;( () -&gt; new int[1], (a, t) -&gt; &#123; a[0] += mapper.applyAsInt(t); &#125;, (a, b) -&gt; &#123; a[0] += b[0]; return a; &#125;, a -&gt; a[0], CH_NOID);&#125;]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-自定义收集器和注意事项]]></title>
    <url>%2F2017%2F02%2F26%2Fjdk8-7%2F</url>
    <content type="text"><![CDATA[自定义收集器之前我们简单说过了Collector接口，以及他的简单使用，现在我们来尝试自定义一个收集器，来进行更加深刻的理解。 Collector的5个方法： 123456789Supplier&lt;A&gt; supplier(); BiConsumer&lt;A, T&gt; accumulator();BinaryOperator&lt;A&gt; combiner();Function&lt;A, R&gt; finisher();// 收集器特性，只有3个值，CONCURRENT，UNORDERED，IDENTITY_FINISH// CONCURRENT标识同一个结果容器可以由多个线程多次调用。// UNORDERED标识收集器并不承诺保证流的顺序。// IDENTITY_FINISH标识finisher函数就是identity函数。Set&lt;Characteristics&gt; characteristics(); 了解以上接口方法后，我们来自己实现一个收集器： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class MySetCollector&lt;T&gt; implements Collector&lt;T, Set&lt;T&gt;, Set&lt;T&gt;&gt; &#123; /** * 产生结果容器 */ @Override public Supplier&lt;Set&lt;T&gt;&gt; supplier() &#123; System.out.println("supplier invoked!"); return HashSet::new; &#125; /** * 累加器 */ @Override public BiConsumer&lt;Set&lt;T&gt;, T&gt; accumulator() &#123; System.out.println("accumulator invoked!"); return Set&lt;T&gt;::add; &#125; /** * 用于并行流，将多个部分的执行结果合并起来 */ @Override public BinaryOperator&lt;Set&lt;T&gt;&gt; combiner() &#123; System.out.println("combiner invoked!"); return (set1, set2) -&gt; &#123; set1.addAll(set2); return set1; &#125;; &#125; /** * 合并后返回最终的结果类型 */ @Override public Function&lt;Set&lt;T&gt;, Set&lt;T&gt;&gt; finisher() &#123; System.out.println("finisher invoked!"); return Function.identity(); &#125; @Override public Set&lt;Characteristics&gt; characteristics() &#123; System.out.println("characteristics invoked"); return Collections.unmodifiableSet(EnumSet.of(IDENTITY_FINISH, UNORDERED)); &#125; public static void main(String[] args) &#123; List&lt;String&gt; list = Arrays.asList("hello", "world", "welcome"); Set&lt;String&gt; set = list.stream().collect(new MySetCollector&lt;&gt;()); System.out.println(set); &#125;&#125; 输出结果如下： 123456supplier invoked!accumulator invoked!combiner invoked!characteristics invokedcharacteristics invoked[world, hello, welcome] 首先说明一点，因为这些返回都是函数式接口，所以方法被调用了，也不意味着行为被执行了。 我们来看一下collect方法的源码。 12345678910111213141516public final &lt;R, A&gt; R collect(Collector&lt;? super P_OUT, A, R&gt; collector) &#123; A container; if (isParallel() &amp;&amp; (collector.characteristics().contains(Collector.Characteristics.CONCURRENT)) &amp;&amp; (!isOrdered() || collector.characteristics().contains(Collector.Characteristics.UNORDERED))) &#123; container = collector.supplier().get(); BiConsumer&lt;A, ? super P_OUT&gt; accumulator = collector.accumulator(); forEach(u -&gt; accumulator.accept(container, u)); &#125; else &#123; container = evaluate(ReduceOps.makeRef(collector)); &#125; return collector.characteristics().contains(Collector.Characteristics.IDENTITY_FINISH) ? (R) container : collector.finisher().apply(container);&#125; 我们的不是并行流，就直接看evaluate(ReduceOps.makeRef(collector));这一行代码，我们再进ReduceOps.makeRef看一下源码。 123456789101112131415161718192021222324252627282930313233343536public static &lt;T, I&gt; TerminalOp&lt;T, I&gt;makeRef(Collector&lt;? super T, I, ?&gt; collector) &#123; Supplier&lt;I&gt; supplier = Objects.requireNonNull(collector).supplier(); BiConsumer&lt;I, ? super T&gt; accumulator = collector.accumulator(); BinaryOperator&lt;I&gt; combiner = collector.combiner(); class ReducingSink extends Box&lt;I&gt; implements AccumulatingSink&lt;T, I, ReducingSink&gt; &#123; @Override public void begin(long size) &#123; state = supplier.get(); &#125; @Override public void accept(T t) &#123; accumulator.accept(state, t); &#125; @Override public void combine(ReducingSink other) &#123; state = combiner.apply(state, other.state); &#125; &#125; return new ReduceOp&lt;T, I, ReducingSink&gt;(StreamShape.REFERENCE) &#123; @Override public ReducingSink makeSink() &#123; return new ReducingSink(); &#125; @Override public int getOpFlags() &#123; return collector.characteristics().contains(Collector.Characteristics.UNORDERED) ? StreamOpFlag.NOT_ORDERED : 0; &#125; &#125;;&#125; 可以看到下面的代码就已经执行了这3个方法，来获取3个函数式接口。 123Supplier&lt;I&gt; supplier = Objects.requireNonNull(collector).supplier();BiConsumer&lt;I, ? super T&gt; accumulator = collector.accumulator();BinaryOperator&lt;I&gt; combiner = collector.combiner(); 那么为什么characteristics()方法调用了2次呢，看看上面的源码就知道，一处是在getOpFlags方法调用的，一处是在collect的最后一行代码里调用的。 如果Collections.unmodifiableSet(EnumSet.of(IDENTITY_FINISH, UNORDERED));把IDENTITY_FINISH去掉，那么就会调用finisher方法。 自定义收集器深度剖析和并行流陷阱接下里我们自定义的一个Collector目的是要把一个list转换成一个map，不过要求Supplier返回是一个Set。意思就是Collector&lt;T,A,R&gt;3个泛型参数，A和R的类型是不一样的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class MySetCollector2&lt;T&gt; implements Collector&lt;T, Set&lt;T&gt;, Map&lt;T, T&gt;&gt; &#123; /** * 产生结果容器 */ @Override public Supplier&lt;Set&lt;T&gt;&gt; supplier() &#123; System.out.println("supplier invoked!"); return HashSet::new; &#125; /** * 累加器 */ @Override public BiConsumer&lt;Set&lt;T&gt;, T&gt; accumulator() &#123; System.out.println("accumulator invoked!"); return (set, item) -&gt; &#123; System.out.println("accumulator: " + set + Thread.currentThread().getName()); set.add(item); &#125;; &#125; /** * 用于并行流，将多个部分的执行结果合并起来 */ @Override public BinaryOperator&lt;Set&lt;T&gt;&gt; combiner() &#123; System.out.println("combiner invoked!"); return (set1, set2) -&gt; &#123; set1.addAll(set2); return set1; &#125;; &#125; /** * 合并后返回最终的结果类型 */ @Override public Function&lt;Set&lt;T&gt;, Map&lt;T, T&gt;&gt; finisher() &#123; return set -&gt; &#123; Map&lt;T, T&gt; map = new HashMap&lt;&gt;(); set.forEach(item -&gt; map.put(item, item)); return map; &#125;; &#125; @Override public Set&lt;Characteristics&gt; characteristics() &#123; System.out.println("characteristics invoked"); // 这里就不能再写IDENTITY_FINISH了，因为A和R的类型不一样，如果写了会报一个强制转换的异常。 return Collections.unmodifiableSet(EnumSet.of(UNORDERED)); &#125; public static void main(String[] args) &#123; List&lt;String&gt; list = Arrays.asList("hello", "world", "welcome", "hello", "a", "b", "c", "d", "e", "f", "g"); Map&lt;String, String&gt; map = list.stream().collect(new MySetCollector2&lt;&gt;()); System.out.println(map); &#125;&#125; 输出结果： 1234567supplier invoked!accumulator invoked!combiner invoked!characteristics invokedcharacteristics invokedfinisher invoked&#123;a=a, b=b, world=world, c=c, d=d, e=e, f=f, g=g, hello=hello, welcome=welcome&#125; 上面写到不能再添加IDENTITY_FINISH，否则会出错，原因就在于之前我们解释的过的源码中的一行代码，就是collect的源码中。 123return collector.characteristics().contains(Collector.Characteristics.IDENTITY_FINISH) ? (R) container : collector.finisher().apply(container); 很明显，如果包含了IDENTITY_FINISH枚举，那么会执行(R) container。直接进行强制转换，在我们这里的container的e类型是Set,R是Map，那么这行代码就是把Set强制转换成一个Map，当然我们就会得到一个强制类型转换的异常。这也说明一个问题，characteristics并不能乱写，所以我们要理解这个函数的每一个枚举的含义，才能在开发中很好的运用。 接下来我们进行一个改造，把stream换成parallelStream。运行一次 12345678910111213141516171819characteristics invokedsupplier invoked!accumulator invoked!combiner invoked!characteristics invokedaccumulator: []ForkJoinPool.commonPool-worker-3accumulator: []ForkJoinPool.commonPool-worker-3accumulator: []ForkJoinPool.commonPool-worker-3accumulator: []ForkJoinPool.commonPool-worker-3accumulator: []ForkJoinPool.commonPool-worker-3accumulator: []ForkJoinPool.commonPool-worker-3accumulator: []ForkJoinPool.commonPool-worker-3accumulator: []ForkJoinPool.commonPool-worker-3accumulator: []mainaccumulator: []ForkJoinPool.commonPool-worker-1accumulator: []ForkJoinPool.commonPool-worker-2characteristics invokedfinisher invoked&#123;a=a, b=b, world=world, c=c, d=d, e=e, f=f, g=g, hello=hello, welcome=welcome&#125; 我们可以看到有多个线程进行了累加器的调用。我们再改一行代码 return Collections.unmodifiableSet(EnumSet.of(UNORDERED, CONCURRENT); 输出： 123456789101112131415161718characteristics invokedcharacteristics invokedsupplier invoked!accumulator invoked!accumulator: []ForkJoinPool.commonPool-worker-1accumulator: [welcome]ForkJoinPool.commonPool-worker-1accumulator: [a, welcome]ForkJoinPool.commonPool-worker-1accumulator: [a, hello, welcome]ForkJoinPool.commonPool-worker-1accumulator: [a, f, hello, welcome]ForkJoinPool.commonPool-worker-1accumulator: [a, f, g, hello, welcome]ForkJoinPool.commonPool-worker-1accumulator: [a, e, f, g, hello, welcome]ForkJoinPool.commonPool-worker-1accumulator: [a, b, e, f, g, hello, welcome]ForkJoinPool.commonPool-worker-1accumulator: [a, hello, welcome]mainaccumulator: [a, hello, welcome]ForkJoinPool.commonPool-worker-2accumulator: [a, hello, welcome]ForkJoinPool.commonPool-worker-3characteristics invokedfinisher invoked&#123;a=a, b=b, c=c, world=world, d=d, e=e, f=f, g=g, hello=hello, welcome=welcome&#125; 这有个很明显的差别，就是累加器输出的时候，打印set的值[]差别很大，这个不是偶然的，我们看CONCURRENT的api解释：允许有多个线程操作同一个结果容器，并且只能被用于无序(UNORDERED)的流。反过来想一下，如果没有加CONCURRENT特性，那么并行流就是有几个线程，就有几个结果容器被操作了， 我们多执行几次代码，可能会得到一个ConcurrentModificationException的异常，可是，如果我们把测试代码中打印set值的代码去掉，无论你执行多少次，也不会出现这个异常。我们看一下ConcurrentModificationException的说明： This exception may be thrown by methods that have detected concurrent modification of an object when such modification is not permissible. For example, it is not generally permissible for one thread to modify a Collection while another thread is iterating over it.意思很明确了，不允许一个线程在修改的时候，另一个线程同时又在迭代它。所以我们要在实际开发中，避免在累加器中对中间结果容器进行额外的操作。 那么如何证明加了CONCURRENT之后就只有一个中间结果容器，不加就就有多个中间结果容器呢(多个线程多个容器，所以不会有ConcurrentModificationException)，用combiner就能测试了，因为只有一个中间结果容器的话，combiner根本不会执行。 我们把代码改一下： 12345678910@Overridepublic BinaryOperator&lt;Set&lt;T&gt;&gt; combiner() &#123; System.out.println("combiner invoked!"); return (set1, set2) -&gt; &#123; System.out.println("set1: " + set1); System.out.println("set1: " + set2); set1.addAll(set2); return set1; &#125;;&#125; 不加CONCURRENT: 123456789101112131415161718192021222324252627282930313233343536373839characteristics invokedsupplier invoked!accumulator invoked!combiner invoked!characteristics invokedaccumulator: []ForkJoinPool.commonPool-worker-1 hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-2 hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-1 hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-3 hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-1 hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-3 hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-3 hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-2 hashcode: 0set1: [hello]set1: [hello]set1: [world]set1: [f]set1: [g]set1: [a]set1: [welcome]set1: [e]accumulator: []ForkJoinPool.commonPool-worker-3 hashcode: 0set1: [f, g]set1: [a, hello]set1: [world, hello]set1: [a, hello, welcome]accumulator: []ForkJoinPool.commonPool-worker-3 hashcode: 0accumulator: []main hashcode: 0set1: [c]set1: [d]set1: [b]set1: [c, d]set1: [b, c, d]set1: [e, f, g]set1: [a, world, hello, welcome]set1: [b, c, d, e, f, g]characteristics invokedfinisher invoked&#123;a=a, b=b, world=world, c=c, d=d, e=e, f=f, g=g, hello=hello, welcome=welcome&#125; 加了CONCURRENT之后： 123456789101112131415161718characteristics invokedcharacteristics invokedsupplier invoked!accumulator invoked!accumulator: []ForkJoinPool.commonPool-worker-1 hashcode: 0accumulator: [welcome]ForkJoinPool.commonPool-worker-1 hashcode: 1233099618accumulator: [a, welcome]ForkJoinPool.commonPool-worker-1 hashcode: 1233099715accumulator: []main hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-2 hashcode: 0accumulator: []ForkJoinPool.commonPool-worker-3 hashcode: 0accumulator: [a, world, hello, welcome]ForkJoinPool.commonPool-worker-2 hashcode: 1445580839accumulator: [a, hello, welcome]ForkJoinPool.commonPool-worker-1 hashcode: 1332262037accumulator: [a, world, e, hello, welcome]ForkJoinPool.commonPool-worker-2 hashcode: 1445580940accumulator: [a, world, hello, welcome]ForkJoinPool.commonPool-worker-3 hashcode: 1445580839accumulator: [a, world, e, f, hello, welcome]ForkJoinPool.commonPool-worker-1 hashcode: 1445581042characteristics invokedfinisher invoked&#123;a=a, b=b, world=world, c=c, d=d, e=e, f=f, g=g, hello=hello, welcome=welcome&#125; 可以看到的，加了CONCURRENT之后根本就没有执行combiner方法。所以我们可以再总结一下combiner的使用说明，就是在并行流，并且收集器的特性没有CONCURRENT特性的时候，combiner才会被调用。]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-比较器Comparator]]></title>
    <url>%2F2017%2F02%2F26%2Fjdk8-6%2F</url>
    <content type="text"><![CDATA[比较器详解关于比较器我们经常在排序的时候用到，按照某个属性排序。比如我们按照学生的成绩排序，如果成绩相等呢，我们再按照姓名排序。 在Java 8中，Comparator新增了不少默认方法以及静态方法，我们要把这些东西用起来。看看示例代码 123456789101112131415161718public class MyComparatorTest &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = Arrays.asList("nihao", "hello", "world", "welcome"); // 字符串长度升序 list.sort((item1, item2) -&gt; item1.length() - item2.length()); list.sort(Comparator.comparingInt(String::length)); //这行代码会报错 list.sort(Comparator.comparingInt(o -&gt; o.length()).reversed()); // 字符串长度降序 list.sort(Comparator.comparingInt(String::length).reversed()); System.out.println(list); &#125;&#125; 我们看一下为什么list.sort(Comparator.comparingInt(o -&gt; o.length()).reversed());会报错呢？其实改成list.sort(Comparator.comparingInt((String o) -&gt; o.length()).reversed());，我们主动加上lambda表达式的类型声明，也是可以的，这是因为Java的类型推断，只能推断一层，我们看comparingInt的参数泛型是&lt;? super T&gt;，而reversed()方法才是最后调用的一个方法，所以Java就只能推断出reversed()方法泛型是什么。而推断不出Comparator.comparingInt的参数泛型应该是什么了。所以这个例子如果我们去掉reversed方法，list.sort(Comparator.comparingInt(o -&gt; o.length());这样写也是可以正常运行的。 接着看thenComparing方法，解读一下文档，说的就是只在第一个比较器比较的结果是相等的时候，才会执行第二个执行比较器，如果不相等的话，那么第二个比较器是不会执行的。 1list.sort(Comparator.comparingInt(String::length).thenComparing(String.CASE_INSENSITIVE_ORDER));]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-Stream源码解析]]></title>
    <url>%2F2017%2F02%2F10%2Fjdk8-5%2F</url>
    <content type="text"><![CDATA[collect我们看一下Stream API里很重要的collect函数。 collect是一个收集器。 Collector作为collect方法的参数 Collector是一个接口，它是一个可变的汇聚操作，将输入元素累积到一个可变的结果容器中；它会在所有元素都处理完毕之后，将累积的结果转换为一个最终的表示（这是一个可选操作）；它支持串行与并行两种方式执行。 Collectors提供了关于Collector的常见汇聚实现，Collectors本身实际上是一个工厂。 为了确保串行与并行操作结果的等价性，Collector函数需要满足两个条件：identity（同一性）与associativity（结合性） a == combiner.apply(a, supplier.get()) 函数式编程最大的特点：表示做什么，而不是如何做。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class StreamTest1 &#123; public static void main(String[] args) &#123; Student student1 = new Student("zhangsan", 80); Student student2 = new Student("lisi", 90); Student student3 = new Student("wangwu", 100); Student student4 = new Student("zhaoliu", 90); Student student5 = new Student("zhaoliu", 90); List&lt;Student&gt; students = Arrays.asList(student1, student2, student3, student4, student5); List&lt;Student&gt; students1 = students.stream().collect(toList()); students1.forEach(System.out::println); System.out.println("----------"); System.out.println("count: " + students.stream().collect(counting())); System.out.println("count: " + students.stream().count()); System.out.println("----------"); students.stream().collect(minBy(Comparator.comparingInt(Student::getScore))).ifPresent(System.out::println); students.stream().collect(maxBy(Comparator.comparingInt(Student::getScore))).ifPresent(System.out::println); System.out.println(students.stream().collect(averagingInt(Student::getScore))); System.out.println(students.stream().collect(summingInt(Student::getScore))); System.out.println(students.stream().collect(summarizingInt(Student::getScore))); System.out.println("----------"); System.out.println(students.stream().map(Student::getName).collect(joining(", "))); System.out.println(students.stream().map(Student::getName).collect(joining(", ", "&lt;being&gt;", "&lt;end&gt;"))); System.out.println("----------"); Map&lt;Integer, Map&lt;String, List&lt;Student&gt;&gt;&gt; map = students.stream() .collect(groupingBy(Student::getScore, groupingBy(Student::getName))); System.out.println(map); System.out.println("----------"); Map&lt;Boolean, List&lt;Student&gt;&gt; map2 = students.stream().collect(partitioningBy(student -&gt; student.getScore() &gt; 80)); System.out.println(map2); System.out.println("----------"); Map&lt;Boolean, Map&lt;Boolean, List&lt;Student&gt;&gt;&gt; map3 = students.stream() .collect(partitioningBy(student -&gt; student.getScore() &gt; 80, partitioningBy(student -&gt; student.getScore() &gt; 90))); System.out.println(map3); System.out.println("----------"); Map&lt;Boolean, Long&gt; map4 = students.stream().collect(partitioningBy(student -&gt; student.getScore() &gt; 80, counting())); System.out.println(map4); System.out.println("----------"); Map&lt;String, Student&gt; map5 = students.stream().collect(groupingBy(Student::getName, collectingAndThen(minBy(Comparator.comparingInt(Student::getScore)), Optional::get))); System.out.println(map5); &#125; 用Collectors.toList举例 123456public static &lt;T&gt; Collector&lt;T, ?, List&lt;T&gt;&gt; toList() &#123; return new CollectorImpl&lt;&gt;((Supplier&lt;List&lt;T&gt;&gt;) ArrayList::new, List::add, (left, right) -&gt; &#123; left.addAll(right); return left; &#125;, CH_ID); &#125; CollectorImpl就是Collector的一个实现类，这个构造方法的参数我理解如下： 第一个参数Supplier，就是生成一个容器用来装需要收集的元素，这里是一个ArrayList 第二个参数是一个BiConsumer，这里叫做累加器，操作内容就是把流里的元素放进容器里 第三个参数combiner是一个BinaryOperator类型，只有在并发流的时候才会用到，意思就是并发的时候会有多个Supplier各自进行收集，最后combiner会把这些结果集合并在一起。]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-Stream详解]]></title>
    <url>%2F2017%2F02%2F05%2Fjdk8-4%2F</url>
    <content type="text"><![CDATA[StreamJava 8 中的 Stream 是对集合（Collection）对象功能的增强，它专注于对集合对象进行各种非常便利、高效的聚合操作（aggregate operation），或者大批量数据操作 (bulk data operation)。Stream API 借助于同样新出现的 Lambda 表达式，极大的提高编程效率和程序可读性。同时它提供串行和并行两种模式进行汇聚操作，并发模式能够充分利用多核处理器的优势，使用 fork/join 并行方式来拆分任务和加速处理过程。通常编写并行代码很难而且容易出错, 但使用 Stream API 无需编写一行多线程的代码，就可以很方便地写出高性能的并发程序。所以说，Java 8 中首次出现的 java.util.stream 是一个函数式语言+多核时代综合影响的产物。 Collection提供了新的stream()方法 流不存储值，通过管道的方式获取值 本质是函数式的，对流的操作会产生一个结果，不过并不会修改底层的数据源，集合可以作为流底层数据源 延迟查找，很多流操作（过滤、映射、排序等）都可以延迟实现 Stream 不是集合元素，它不是数据结构并不保存数据，它是有关算法和计算的，它更像一个高级版本的 Iterator。原始版本的 Iterator，用户只能显式地一个一个遍历元素并对其执行某些操作；高级版本的 Stream，用户只要给出需要对其包含的元素执行什么操作，比如 “过滤掉长度大于 10 的字符串”、“获取每个字符串的首字母”等，Stream 会隐式地在内部进行遍历，做出相应的数据转换。Stream 就如同一个迭代器（Iterator），单向，不可往复，数据只能遍历一次，遍历过一次后即用尽了，就好比流水从面前流过，一去不复返。而和迭代器又不同的是，Stream 可以并行化操作，迭代器只能命令式地、串行化操作。顾名思义，当使用串行方式去遍历时，每个 item 读完后再读下一个 item。而使用并行去遍历时，数据会被分成多个段，其中每一个都在不同的线程中处理，然后将结果一起输出。Stream 的并行操作依赖于 Java7 中引入的 Fork/Join 框架（JSR166y）来拆分任务和加速处理过程。流由3部分构成： 源 零个或多个中间操作 终止操作 流操作的分类： 惰性求值（中间操作） 及早求值（终止操作） 创建流的几种方式123456789101112public class StreamTest &#123; public static void main(String[] args) &#123; Stream stream = Stream.of("hello", "world", "hello world"); String[] strArray = new String[]&#123;"hello", "world", "hello world"&#125;; Stream stream1 = Stream.of(strArray); Stream stream2 = Arrays.stream(strArray); List&lt;String&gt; list = Arrays.asList(strArray); Stream stream3 = list.stream(); &#125;&#125; 流的简单使用1234567891011public class StreamTest2 &#123; public static void main(String[] args) &#123; IntStream.of(new int[]&#123;5, 6, 7&#125;).forEach(System.out::println); System.out.println("--------"); // 不包含8 IntStream.range(3, 8).forEach(System.out::println); System.out.println("--------"); // 包含8 IntStream.rangeClosed(3, 8).forEach(System.out::println); &#125;&#125; 进一步应用1234567public class StreamTest3 &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6); System.out.println(list.stream().map(i -&gt; 2 * i).reduce(0, Integer::sum)); &#125;&#125; Stream转换为数组和集合1234567891011121314151617181920212223242526272829303132public class StreamTest4 &#123; public static void main(String[] args) &#123; Stream&lt;String&gt; stream = Stream.of("hello", "world", "hello world");// String[] strArray = stream.toArray(i -&gt; new String[i]);// Arrays.asList(strArray).forEach(System.out::println);// String[] strArray = stream.toArray(String[]::new);// Arrays.asList(strArray).forEach(System.out::println);// List&lt;String&gt; list = stream.collect(Collectors.toList());// list.forEach(System.out::println); // 第一个参数是要返回的容器，第二个参数是对每一个结果进行处理，第三个参数是把所有处理过的结果组装进要返回的list// List&lt;String&gt; list = stream.collect(() -&gt; new ArrayList(), (theList, item) -&gt; theList.add(item), (theList1, theList2) -&gt; theList1.addAll(theList2));// List&lt;String&gt; list = stream.collect(ArrayList::new, ArrayList::add, ArrayList::addAll);// list.forEach(System.out::println); // 另一个重载的collect方法// List&lt;String&gt; list = stream.collect(Collectors.toCollection(ArrayList::new));// list.forEach(System.out::println); // 转换为Set// Set&lt;String&gt; set = stream.collect(Collectors.toCollection(TreeSet::new));// set.forEach(System.out::println); // 拼接字符串 String str = stream.collect(Collectors.joining()); System.out.println(str); &#125;&#125; map和flatMap1234567891011121314151617public class StreamTest5 &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = Arrays.asList("hello", "world", "helloworld", "test"); list.stream().map(String::toUpperCase).collect(Collectors.toList()).forEach(System.out::println); System.out.println("-------------"); List&lt;Integer&gt; list2 = Arrays.asList(1, 2, 3, 4, 5); list2.stream().map(item -&gt; item * item).collect(Collectors.toList()).forEach(System.out::println); System.out.println("-------------"); // flapMap是把流里的List也打开成一整个流 Stream&lt;List&lt;Integer&gt;&gt; stream = Stream.of(Arrays.asList(1), Arrays.asList(2, 3), Arrays.asList(4, 5, 6)); stream.flatMap(theList -&gt; theList.stream()).map(item -&gt; item).forEach(System.out::println); &#125;&#125; 其他方法1234567891011121314151617181920212223242526272829303132333435363738394041public class StreamTest6 &#123; public static void main(String[] args) &#123;// Stream&lt;String&gt; stream = Stream.generate(UUID.randomUUID()::toString);// stream.findFirst().ifPresent(System.out::println); // iterate会产生一个无限流，所以要配合limit使用 Stream&lt;Integer&gt; stream = Stream.iterate(1, item -&gt; item + 2).limit(6);// stream.forEach(System.out::println); //找出该流中大于2的元素，然后将每个月元素乘以2，然后过滤掉流中的前两个元素，然后再取流中的前两个元素，最后求出流中元素的总和。 // 1,3,5,7,9,11 结果是32// Integer integer = stream.filter(item -&gt; item &gt; 2).mapToInt(item -&gt; item * 2)// .skip(2).limit(2).sum();// System.out.println(integer);// IntSummaryStatistics intSummaryStatistics = stream.filter(item -&gt; item &gt; 2).mapToInt(item -&gt; item * 2)// .skip(2).limit(2).summaryStatistics();// System.out.println(intSummaryStatistics.getCount());// System.out.println(intSummaryStatistics.getMax());// System.out.println(intSummaryStatistics.getMin());// System.out.println(stream);// System.out.println(stream.filter(item -&gt; item &gt; 2)); // 这句代码会抛异常: stream has already been operated upon or closed// System.out.println(stream.distinct()); // 正确的调用方式如下 System.out.println(stream); Stream&lt;Integer&gt; stream2 = stream.filter(item -&gt; item &gt; 2); System.out.println(stream2); Stream&lt;Integer&gt; stream3 = stream2.distinct(); System.out.println(stream3); &#125;&#125; 中间操作和终止操作本质上的区别在对于一个 Stream 进行多次转换操作 (Intermediate 操作)，每次都对 Stream 的每个元素进行转换，而且是执行多次，这样时间复杂度就是 N（转换次数）个 for 循环里把所有操作都做掉的总和吗？其实不是这样的，转换操作都是 lazy 的，多个转换操作只会在 Terminal 操作的时候融合起来，一次循环完成。我们可以这样简单的理解，Stream 里有个操作函数的集合，每次转换操作就是把转换函数放入这个集合中，在 Terminal 操作的时候循环 Stream 对应的集合，然后对每个元素执行所有的函数。 中间操作都会返回一个Stream对象，比如说返回Stream。终止操作则不会返回Stream类型，可能不返回值，也可能返回其他类型的单个值。 1234567891011121314151617public class StreamTest7 &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = Arrays.asList("hello", "world", "hello world"); // map是一个中间操作，是惰性的，在没有遇到终止操作的时候，中间操作是不会执行的 Stream&lt;String&gt; test = list.stream().map(item -&gt; &#123; String result = item.substring(0, 1).toUpperCase() + item.substring(1); System.out.println("test"); return result; &#125;); // 这句代码才会输入map方法中的test test.forEach(System.out::println); &#125;&#125; 123456789public class StreamTest8 &#123; public static void main(String[] args) &#123; // 这段代码虽然输出了正确的结果，但是程序不会终止，因为distinct一直在为无限流不停的去重 IntStream.iterate(0, i -&gt; (i + 1 ) % 2).distinct().limit(6).forEach(System.out::println); // 下面才是正确的方式 IntStream.iterate(0, i -&gt; (i + 1 ) % 2).limit(6).distinct().forEach(System.out::println); &#125;&#125; 串行流和并行流的区别Stream 的并行操作依赖于 Java7 中引入的 Fork/Join 框架（JSR166y）来拆分任务和加速处理过程。 12345678910111213141516171819202122232425262728293031323334public class StreamTest9 &#123; public static void main(String[] args) &#123; // 准备500w个uuid来用不同的流进行排序 List&lt;String&gt; list = new ArrayList&lt;&gt;(5000000); for (int i = 0; i &lt; 5000000; i++) &#123; list.add(UUID.randomUUID().toString()); &#125; System.out.println("开始排序"); long startTime = System.nanoTime(); list.stream().sorted().count(); long endTime = System.nanoTime(); long millis = TimeUnit.NANOSECONDS.toMillis(endTime - startTime); System.out.println("排序耗时：" + millis); startTime = System.nanoTime(); list.parallelStream().sorted().count(); endTime = System.nanoTime(); millis = TimeUnit.NANOSECONDS.toMillis(endTime - startTime); System.out.println("排序耗时：" + millis); /** * 开始排序 * 排序耗时：6500 * 排序耗时：3394 */ &#125;&#125; 流的短路123456789101112131415public class StreamTest10 &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = Arrays.asList("hello", "world", "hello world"); // 打印长度为5的第一个单词// list.stream().mapToInt(String::length).filter(length -&gt; length == 5).findFirst().ifPresent(System.out::println); // 下面只会输入出hello和5，因为流针对每一个元素的统一应用所有操作，所以直接找到了第一个hello list.stream().mapToInt(item -&gt; &#123; int length = item.length(); System.out.println(item); return length; &#125;).filter(length -&gt; length == 5).findFirst().ifPresent(System.out::println); &#125;&#125; flatMap的应用场景1234567public class StreamTest11 &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = Arrays.asList("hello welcome", "world hello", "hello world hello", "hello welcome"); // 找出所有单词并且去重 list.stream().flatMap(item -&gt; Arrays.stream(item.split(" "))).distinct().forEach(System.out::println); &#125;&#125; 进一步应用 1234567891011public class StreamTest12 &#123; public static void main(String[] args) &#123; List&lt;String&gt; list1 = Arrays.asList("Hi", "Hello", "你好"); List&lt;String&gt; list2 = Arrays.asList("zhangsan", "lisi", "wangwu", "zhaoliu"); List&lt;String&gt; result = list1.stream().flatMap(item -&gt; list2.stream().map(item2 -&gt; item + " " + item2)).collect(Collectors.toList()); result.forEach(System.out::println); &#125;&#125; 分组：group by操作1234567891011121314151617181920212223242526272829303132333435363738public class StreamTest13 &#123; public static void main(String[] args) &#123; Student student1 = new Student("zhangsan", 100, 20); Student student2 = new Student("lisi", 90, 20); Student student3 = new Student("wangwu", 90, 30); Student student4 = new Student("zhangsan", 80, 40); List&lt;Student&gt; students = Arrays.asList(student1, student2, student3, student4); // select * from student group by name; /* * 传统方式： * 1. 循环列表 * 2. 取出学生名字 * 3. 检查map中是否存在该名字，不存在则直接添加到该map中；存在则将map中的List对象取出来，然后将该Student对象添加到List中 * 4. 返回map对象 */ // 流方式，一行代码 Map&lt;String, List&lt;Student&gt;&gt; map = students.stream().collect(Collectors.groupingBy(Student::getName)); System.out.println(map); // select count(*) from student group by name; Map&lt;String, Long&gt; map1 = students.stream().collect(Collectors.groupingBy(Student::getName, Collectors.counting())); System.out.println(map1); // 每个人的平均分 Map&lt;String, Double&gt; map2 = students.stream().collect(Collectors.groupingBy(Student::getName, Collectors.averagingDouble(Student::getScore))); System.out.println(map2); // 分区是分组的一种特例，就是用条件来分为两组 Map&lt;Boolean, List&lt;Student&gt;&gt; map3 = students.stream().collect(Collectors.partitioningBy(student -&gt; student.getScore() &gt;= 90)); System.out.println(map3); &#125;&#125;]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-方法引用]]></title>
    <url>%2F2017%2F02%2F05%2Fjdk8-3%2F</url>
    <content type="text"><![CDATA[方法引用方法引用：method reference方法引用实际上是个Lambda表达式的一种语法糖。 我们可以将方法引用看作是一个函数指针，function pointer。 方法引用共分为4类： 类名::静态方法名 1234567891011121314151617181920public class MethodReferenceTest &#123; public static void main(String[] args) &#123; Student student1 = new Student("zhangsan", 10); Student student2 = new Student("lisi", 90); Student student3 = new Student("wangwu", 50); Student student4 = new Student("zhaoliu", 40); List&lt;Student&gt; students = Arrays.asList(student1, student2, student3, student4); // 这是常规的lambda表达式写法 students.sort((o1, o2) -&gt; Student.compareStudentByScore(o1, o2)); students.forEach(student -&gt; System.out.println(student.getScore())); System.out.println("-----------"); // 下面展示方法引用的写法，这就是上面的代码的语法糖，更简洁 students.sort(Student::compareStudentByName); students.forEach(student -&gt; System.out.println(student.getName())); &#125;&#125; 引用名::实例方法名 12345678910 public class StudentComparator &#123; public int compareStudentByScore(Student student1, Student student2) &#123; return student1.getScore() - student2.getScore(); &#125; public int compareStudentByName(Student student1, Student student2) &#123; return student1.getName().compareToIgnoreCase(student2.getName()); &#125;&#125; 这里演示了如何使用 123StudentComparator studentComparator = new StudentComparator(); students.sort((o1, o2) -&gt; studentComparator.compareStudentByScore(o1, o2)); students.sort(studentComparator::compareStudentByScore); 类名::实例方法名 新增2个方法 1234567public int compareByScore(Student student) &#123; return this.getScore() - student.getScore(); &#125; public int comparByeName(Student student) &#123; return this.getName().compareToIgnoreCase(student.getName()); &#125; 然后演示 123456//使用lambda表达式和类型对象的实例方法 students.sort((o1, o2) -&gt; o1.compareByScore(o2)); // 使用方法引用// 引用的是类型对象的实例方法// 这种方式的调用，lambda表达式的第一个参数是作为调用方，然后其他的lambda表达式参数都作为实例方法的参数传入 students.sort(Student::compareByScore); 构造方法引用：类名::new 123Supplier&lt;Student&gt; supplier = () -&gt; new Student(); // 构造方法引用 Supplier&lt;Student&gt; supplier2 = Student::new;]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-Optional详解]]></title>
    <url>%2F2017%2F01%2F17%2Fjdk8-2%2F</url>
    <content type="text"><![CDATA[在Java中我们会经常遇到NullPointerException异常，代码里就少不了很多这样的代码 123if(null != obj) &#123; .......&#125; Java 8中的Optional是一个可以包含或不可以包含非空值的容器对象，在 Stream API中很多地方也都使用到了Optional。这是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 我们应该怎么使用Optional这个类呢。 1234567891011121314151617181920public class OptionalTest &#123; public static void main(String[] args) &#123; Optional&lt;String&gt; optional = Optional.of("hello"); // 这是传统方式的代码书写方式// if (optional.isPresent()) &#123;// System.out.println(optional.get());// &#125; // 我们应该用函数式风格来使用Optional optional.ifPresent(System.out::println); System.out.println("------"); System.out.println(optional.orElse("world")); System.out.println("------"); System.out.println(optional.orElseGet(() -&gt; "nihao")); &#125;&#125; 下面再展示一个具体的应用场景 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Employee &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125;public class Company &#123; private String name; private List&lt;Employee&gt; employeeList; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public List&lt;Employee&gt; getEmployeeList() &#123; return employeeList; &#125; public void setEmployeeList(List&lt;Employee&gt; employeeList) &#123; this.employeeList = employeeList; &#125;&#125;public class OptionalTest2 &#123; public static void main(String[] args) &#123; Employee employee = new Employee(); employee.setName("zhangsan"); Employee employee2 = new Employee(); employee2.setName("lisi"); Company company = new Company(); company.setName("company1"); List&lt;Employee&gt; employeeList = Arrays.asList(employee, employee2); company.setEmployeeList(employeeList); // 下面的代码使用函数式的风格开发，避免了null判断以及条件分支等等代码 Optional&lt;Company&gt; optional = Optional.ofNullable(company); System.out.println(optional.map(theCompany -&gt; theCompany.getEmployeeList()).orElse(Collections.emptyList())); &#125;&#125;]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security配置cors跨域访问]]></title>
    <url>%2F2016%2F12%2F29%2Fspring-security-cors%2F</url>
    <content type="text"><![CDATA[http://docs.spring.io/spring/docs/current/spring-framework-reference/html/cors.html 文档看似很清晰的描述了如何在Spring 4.2之后启用cors跨域访问，网上搜索介绍这样的帖子也不少。也提到了说什么如果用了Spring Security的话要采用filter的方式来配置。下面这段话就是官方文档 In order to support CORS with filter-based security frameworks like Spring Security, or with other libraries that do not support natively CORS, Spring Framework also provides a CorsFilter. Instead of using @CrossOrigin or WebMvcConfigurer#addCorsMappings(CorsRegistry), you need to register a custom filter defined like bellow:123456789101112131415161718192021import org.springframework.web.cors.CorsConfiguration;import org.springframework.web.cors.UrlBasedCorsConfigurationSource;import org.springframework.web.filter.CorsFilter;public class MyCorsFilter extends CorsFilter &#123; public MyCorsFilter() &#123; super(configurationSource()); &#125; private static UrlBasedCorsConfigurationSource configurationSource() &#123; CorsConfiguration config = new CorsConfiguration(); config.setAllowCredentials(true); config.addAllowedOrigin("http://domain1.com"); config.addAllowedHeader("*"); config.addAllowedMethod("*"); UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); source.registerCorsConfiguration("/**", config); return source; &#125;&#125; 在经过测试之后，实在是无论采用哪一种方式都不行，实在是太费解了，debug了半天跨域的时候GET方法根本连DispatchServlet都不进去，POST方法倒是可以跨域，发现POST请求是根据header的origin来判断是否跨域。 还是想着从Spring Security这边来入手，结果就发现HttpSecurity类提供了这么一个方法。 123456789101112/** * Adds a &#123;@link CorsFilter&#125; to be used. If a bean by the name of corsFilter is * provided, that &#123;@link CorsFilter&#125; is used. Else if corsConfigurationSource is * defined, then that &#123;@link CorsConfiguration&#125; is used. Otherwise, if Spring MVC is * on the classpath a &#123;@link HandlerMappingIntrospector&#125; is used. * * @return the &#123;@link CorsConfigurer&#125; for customizations * @throws Exception */public CorsConfigurer&lt;HttpSecurity&gt; cors() throws Exception &#123; return getOrApply(new CorsConfigurer&lt;HttpSecurity&gt;());&#125; 我抱着试一试的心态，加上了这句话代码。 123456789101112131415161718192021222324252627@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http.headers() .frameOptions() .sameOrigin() .and() // disable CSRF, http basic, form login .csrf().disable() // 跨域支持 .cors().and() .httpBasic().disable() // .formLogin().disable() // ReST is stateless, no sessions .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) // .and() // return 403 when not authenticated .exceptionHandling().authenticationEntryPoint(new NoAuthenticationEntryPoint()); // Let child classes set up authorization paths setupAuthorization(http); http.addFilterBefore(jsonWebTokenFilter, UsernamePasswordAuthenticationFilter.class);&#125; 就是.cors().and()这句了，然后还是采用addCorsMappings方法来配置。 12345678910111213@Configurationpublic class WebConfig extends WebMvcConfigurerAdapter &#123; @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping("/api/**") .allowedOrigins("http://domain2.com") .allowedMethods("PUT", "DELETE") .allowedHeaders("header1", "header2", "header3") .exposedHeaders("header1", "header2") .allowCredentials(false).maxAge(3600); &#125;&#125; 结果当然是成功了，Spring的文档也老是跟不上节奏，还是需要自己多探索和思考。希望能帮到遇到这个问题的朋友们。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8-Lambda表达式初步与函数式接口]]></title>
    <url>%2F2016%2F12%2F19%2Fjdk8-1%2F</url>
    <content type="text"><![CDATA[Lambda表达式初步与函数式接口“Lambda 表达式”(lambda expression)是一个匿名函数，Lambda表达式基于数学中的λ演算得名，直接对应于其中的lambda抽象(lambda abstraction)，是一个匿名函数，即没有函数名的函数。Lambda表达式可以表示闭包（注意和数学传统意义上的不同）。 为何需要Lambda表达式 在Java中，我们无法将函数作为参数传递给一个方法，也无法声明返回一个函数的方法。 在JavaScript中，函数参数一个函数，返回值是另一个函数的情况是非常常见的；JavaScript是一门非常典型的函数式语言。 Java匿名内部类示例： 123456new Thread(new Runnable() &#123; @Override public void run() &#123; // do something &#125;&#125;); 这样写是有点繁琐的，在Java8中可以直接下面这样写 123new Thread(() -&gt; &#123; // do something &#125;); 在Java8的循环中，我们也可以很方便的使用Lambda表达式。示例如下： 123456789List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5);// foreach语法for(Integer i : list) &#123; System.out.println(i); &#125;// JDK8新增的forEach方法list.forEach(i -&gt; System.out.println(i));// 这种只有一行代码，一个参数的调用，我们甚至还可以再简化一点list.forEach(System.out::println); 看forEach的方法源码 123456default void forEach(Consumer&lt;? super T&gt; action) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125;&#125; 接受了一个Consumer参数，这个接口是JDK8新增的一个函数式接口。 什么是函数式接口？ 一个接口，有且只有一个抽象方法，这个接口就称为函数式接口。 如果我们在某个接口上声明了@FunctionalInterface注解，那么编译器就会按照函数式接口的定义来要求该接口。 如果某个接口只有抽象方法，但我们并没有给该接口声明@FunctionalInterface注解，那么编译器依旧会将该接口看做是函数式接口。 12345678910@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123; void accept(T t); default Consumer&lt;T&gt; andThen(Consumer&lt;? super T&gt; after) &#123; Objects.requireNonNull(after); return (T t) -&gt; &#123; accept(t); after.accept(t); &#125;; &#125;&#125; 我们来试试自己写一个函数式接口。 1234567891011121314151617181920212223@FunctionalInterfaceinterface MyInterface &#123; void test(); // 这个不算抽象方法，因为MyInterface的实现类必然是Object的子类，他会直接继承Object类的实现，实现类依然只需要实现test()方法。 String toString();&#125;public class Test &#123; public void myTest(MyInterface myInterface) &#123; myInterface.test(); &#125; public static void main(String[] args) &#123; Test test = new Test(); test.myTest(() -&gt; &#123; // 这里就是MyInterface.test()方法的实现 System.out.println("mytest"); &#125;); &#125;&#125; Lambda表达式的作用Lambda表达式为Java添加了缺失的函数式编程特性，使我们能将函数当作一等公民看待。在将函数作为一等公民的语言中，Lambda表达式的类型是函数。但在Java中，Lambda表达式是对象，他们必须依附于一类特别的对象类型—-函数式接口（functional interface）。 例子下面是用lambda表达式和stream来对一个列表的字符串进行大写字母转换。 12List&lt;String&gt; list = Lists.newArrayList("hello", "world", "hello world");list.stream().map(String::toUpperCase).forEach(System.out::println); 上面看到有2个冒号的地方，这个叫做方法引用，方法引用有四种方式，这是其中一种，通过类的方式引用。 12Function&lt;String, String&gt; function = String::toUpperCase;System.out.println(function.apply("hello")); 那么对象会被当做lambda表达式的第一个参数传入，上面的代码就相当于”hello”.toUpperCase(); 下面演示一个Comparator的例子 1234List&lt;String&gt; names = Arrays.asList("zhangsan", "lisi", "wangwu", "zhaoliu");Collections.sort(names, (o1, o2) -&gt; o2.compareTo(o1));System.out.println(names); 这就是一个倒序排序，Collections.sort()的第二个参数就是一个Comparator对象，我们用lambda表示来写的，看一下Comparator是声明为函数式接口。所以可以用lambda来写。 12@FunctionalInterfacepublic interface Comparator&lt;T&gt; Java Lambda基本语法 Java中的Lambda表达式基本语法 (arg) -&gt; (body) 比如 (arg1, arg2) -&gt; {bodu} (type1 arg1, type2 arg2…) -&gt; {body} 示例 (int a, int b) -&gt; { return a + b; } () -&gt; System.out.println(“Hello World”); (String s) -&gt; {System.out.println(s);} () -&gt; 42 Java Lambda结构 一个Lambda表达式可以有零个或多个参数 参数的类型既可以明确声明，也可以根据上下文来推断。例如：(int a)与(a)效果相同。 所有参数需包含在圆括号内，参数之间用逗号相隔。例如：(a,b)或(int a,int b)或(String a,int b, float c) 空圆括号代表参数集为空。例如：() -&gt; 42 当只有一个参数，且其类型可推导时，圆括号()可省略。例如：a -&gt; return a * a; Lambda表达式的主体可包含零条或多条语句。 如果Lambda表达式的主体只有一条语句，花括号{}可省略。匿名函数的返回类型与该主体表达式一致。 如果Lambda表达式的主体包含一条以上语句，则表达式必须包含在花括号{}中(形成代码块)。匿名函数的返回类型与代码块的返回类型一致，若没有返回则为空。 主要接口详解Function接口12345678910111213141516171819202122public class FunctionTest &#123; public static void main(String[] args) &#123; FunctionTest test = new FunctionTest(); System.out.println(test.compute(1, value -&gt; 2 * value)); System.out.println(test.compute(2, value -&gt; 5 + value)); System.out.println(test.compute(3, value -&gt; value * value)); Function&lt;Integer, String&gt; function = String::valueOf; System.out.println(test.convert(5, function.compose((Integer i) -&gt; i + 1))); &#125; public int compute(int a, Function&lt;Integer, Integer&gt; function) &#123; int result = function.apply(a); return result; &#125; public String convert(int a, Function&lt;Integer, String&gt; function) &#123; return function.apply(a); &#125;&#125; compose和andThencompose()方法，它接受一个Function，也返回一个Function，结果就是执行参数里的apply，再执行本对象的apply。andThen()方法则相反，是先执行本对象的apply，再执行参数Function的apply。 12345678910111213141516171819202122232425262728293031323334353637383940public class FunctionTest2 &#123; public static void main(String[] args) &#123; FunctionTest2 test2 = new FunctionTest2(); // 输出12 System.out.println(test2.compute(2, value -&gt; value * 3, value -&gt; value * value)); // 输出36 System.out.println(test2.compute2(2, value -&gt; value * 3, value -&gt; value * value)); // 动态+-操作 System.out.println(test2.compute3(1, 2, (value1, value2) -&gt; value1 + value2)); System.out.println(test2.compute3(1, 2, (value1, value2) -&gt; value1 - value2)); // BIFunction实例 System.out.println(test2.compute4(2, 3, (value1, value2) -&gt; value1 + value2, value -&gt; value * value)); &#125; public int compute(int a, Function&lt;Integer, Integer&gt; function1, Function&lt;Integer, Integer&gt; function2) &#123; return function1.compose(function2).apply(a); &#125; public int compute2(int a, Function&lt;Integer, Integer&gt; function1, Function&lt;Integer, Integer&gt; function2) &#123; return function1.andThen(function2).apply(a); &#125; public int compute3(int a, int b, BiFunction&lt;Integer, Integer, Integer&gt; biFunction) &#123; return biFunction.apply(a, b); &#125; public int compute4(int a, int b, BiFunction&lt;Integer, Integer, Integer&gt; biFunction, Function&lt;Integer, Integer&gt; function) &#123; return biFunction.andThen(function).apply(a, b); &#125;&#125; BiFunction接受2个参数，返回一个值的函数式接口。 123456789101112131415161718192021222324252627282930313233343536373839public class PersonTest &#123; public static void main(String[] args) &#123; Person person1 = new Person("zhangsan", 20); Person person2 = new Person("lisi", 30); Person person3 = new Person("wangwu", 40); List&lt;Person&gt; people = Arrays.asList(person1, person2, person3); PersonTest test = new PersonTest();// List&lt;Person&gt; personResult = test.getPeopleByUsername("zhangsan", people);// personResult.forEach(person -&gt; System.out.println(person.getUsername()));// List&lt;Person&gt; personResult = test.getPeopleByAge(20, people);// personResult.forEach(person -&gt; System.out.println(person.getAge())); List&lt;Person&gt; personResult = test.getPeopleByAge2(20, people, (ageOfPerson, personList) -&gt; personList.stream().filter(person -&gt; person.getAge() &gt; ageOfPerson).collect(Collectors.toList())); personResult.forEach(person -&gt; System.out.println(person.getAge())); &#125; public List&lt;Person&gt; getPeopleByUsername(String username, List&lt;Person&gt; people) &#123; return people.stream().filter(person -&gt; person.getUsername().equals(username)).collect(Collectors.toList()); &#125; public List&lt;Person&gt; getPeopleByAge(int age, List&lt;Person&gt; people) &#123; BiFunction&lt;Integer, List&lt;Person&gt;, List&lt;Person&gt;&gt; biFunction = (ageOfPerson, personList) -&gt; personList.stream().filter(person -&gt; person.getAge() &gt; ageOfPerson).collect(Collectors.toList()); return biFunction.apply(age, people); &#125; public List&lt;Person&gt; getPeopleByAge2(int age, List&lt;Person&gt; people, BiFunction&lt;Integer, List&lt;Person&gt;, List&lt;Person&gt;&gt; biFunction) &#123; return biFunction.apply(age, people); &#125;&#125; Predicate判断用的函数式接口 123456public class PredicateTest &#123; public static void main(String[] args) &#123; Predicate&lt;String&gt; predicate = p -&gt; p.length() &gt; 5; System.out.println(predicate.test("hello")); &#125;&#125; 代码测试1234567891011121314151617181920212223242526272829public class PredicateTest2 &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); PredicateTest2 predicateTest2 = new PredicateTest2(); predicateTest2.conditionFilter(list, i -&gt; i % 2 == 0); predicateTest2.conditionFilter2(list, item -&gt; item &gt; 5, item -&gt; item % 2 == 0); &#125; // 以前我们在对数据进行筛选或者处理的时候，一般是单独定义一个方法来进行处理，现在我们只需要把筛选条件当作参数传入 public void conditionFilter(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate) &#123; for (Integer integer : list) &#123; if (predicate.test(integer)) &#123; System.out.println(integer); &#125; &#125; &#125; // Predicate的其他方法测试 public void conditionFilter2(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate, Predicate&lt;Integer&gt; predicate2) &#123; for (Integer integer : list) &#123; if (predicate.or(predicate2).test(integer)) &#123; System.out.println(integer); &#125; &#125; &#125;&#125; Supplier简单测试 1234567public class SupplierTest &#123; public static void main(String[] args) &#123; Supplier&lt;String&gt; supplier = () -&gt; "hello world"; System.out.println(supplier.get()); &#125;&#125; java.util.function包下面还有很多函数式接口，无非就是0参数，1个参数，2个参数的接口，用法都是一样的。12345678910111213141516171819202122public class BinaryOperatorTest &#123; public static void main(String[] args) &#123; BinaryOperatorTest binaryOperatorTest = new BinaryOperatorTest(); System.out.println(binaryOperatorTest.compute(1, 2, (a, b) -&gt; a + b)); System.out.println(binaryOperatorTest.compute(1, 2, (a, b) -&gt; a - b)); System.out.println("----------------"); System.out.println(binaryOperatorTest.getShort("hello123", "world", (a, b) -&gt; a.length() - b.length())); &#125; public int compute(int a, int b, BinaryOperator&lt;Integer&gt; binaryOperator) &#123; return binaryOperator.apply(a, b); &#125; public String getShort(String a, String b, Comparator&lt;String&gt; comparator) &#123; return BinaryOperator.minBy(comparator).apply(a, b); &#125;&#125;]]></content>
      <categories>
        <category>jdk8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程7-取消与关闭]]></title>
    <url>%2F2016%2F12%2F14%2Fconcurrency7%2F</url>
    <content type="text"><![CDATA[取消与关闭任务和线程的启动很容易。在大多数时候，我们都会让它们运行直到结束，或者让它们自行停止。然而，有时候我们希望提前结束任务或线程，或许是因为用户取消了操作，或者应用程序需要被快速关闭。 任务取消在Java中没有一种安全的抢占式方法来停止线程，因此也就没有安全的抢占式方法来停止任务。只有一些协作式的机制，使请求取消的任务和代码都遵循一种协商好的协议。 1234567private volatile boolean cancelled;public void run() &#123; while(!cancelled)&#123; // do something &#125;&#125; 中断一些特殊的阻塞库的方法支持中断。线程中断是一种协作机制，线程可以通过这种机制来通知另一个线程，告诉它在合适的或者可能的情况下轻质当前工作，并转而执行其他的工作。 在Java的API或语言规范中，并没有将中断与任何取消语义关联起来，但实际上，如果在取消之外的其他操作中使用中断，那么都是不合适的，并且很难职称起更大的应用。 每个线程都有一个boolean类型的中断状态。当中断线程时，这个线程的中断状态将设置为true。在Thread中包含了中断线程以及查询中断状态的方法。interrupt方法能中断目标线程，isInterrupted方法能返回目标线程的中断状态。静态方法interrupted将清除当前线程的中断状态，并返回它之前的值，这也是清除中断状态的唯一方法。 12345public class Thread()&#123; public void interrupt() &#123;...&#125; public boolean isInterrupted() &#123;...&#125; public static boolean interrupted() &#123;...&#125;&#125; 调用interrupt并不意味着立即停止目标线程正在进行的工作，而只是传递了请求中断的消息。 中断策略正如任务中应该包含取消策略一样，线程同样应该包含中断策略。中断策略规定线程如何解释某个中断请求-当发现中断请求时，应该做哪些工作。由于每个线程拥有各自的中断策略，因此除非你知道中断对该线程的含义，否则就不应该中断这个线程。 响应中断当调用可中断的阻塞函数时，例如Thread.sleep或BlockingQueue.put等，有两种实用策略可用于处理InterruptedException: 传递异常，从而使你的方法也成为可中断的阻塞方法 恢复中断方法，从而使调用栈中的上层代码能够对进行处理。 只有实现了线程中断策略的代码才可以屏蔽中断请求。在常规的任务和库代码中都不应该屏蔽中断请求。 处理不可中断的阻塞并非所有的可阻塞方法或者阻塞机制都能相应中断；如果一个线程由于执行同步的Socket I/O或者等待获得内置锁而阻塞，那么中断请求只能设置线程的中断状态，除此之外没有其他任何作用。对于那些由于执行补课中断操作而被阻塞的线程，可以使用类似于中断的手段来停止这些线程，但这要求我们必须知道线程阻塞的原因。 停止基于线程的服务应用程序通常会创建拥有多个线程的服务，例如线程池，并且这些服务的生命周期通常比创建它们的方法的生命周期更长。如果应用程序准备退出，那么这些服务所拥有的线程也需要结束。由于无法通过抢占式的方法来停止线程，因此它们需要自行结束。 除非拥有某个线程，否则不能对该线程进行操控。对于持有线程的服务，只要服务的存在时间大于创建线程的方法的存在时间，那么就应该提供生命周期方法。 例如日志服务，为了不给程序带来性能开销，记录日志的操作有一种方法是通过调用log方法将日志消息放入某个队列中，并由其他线程来处理。然后我们要合理的处理这个日志队列，避免因为JVM无法正常关闭时，停止日志线程导致消息丢失。 关闭ExecutorServiceExecutorService提供两种关闭方法：使用shutdown正常关闭，以及使用shutdownNow强行关闭。在进行强行关闭时，shutdownNow首先关闭当前正在执行的任务，然后返回所有尚未启动的任务清单。 这两种关闭方式的差别在于各自的安全性和响应性：强行关闭的速度更快，但风险也更大，因为任务很可能在执行到一半时被结束；而正常关闭虽然速度慢，但却更安全，因为ExecutorService会一直等到队列中的所有任务都执行完成后才关闭。在其他拥有线程的服务中也应该考虑提供累死的关闭方式以供选择。 shutdownNow的局限性当通过shutdownNow来强行关闭ExecutorService时，它会尝试取消正在执行的任务，并返回所有已提交但尚未开始的任务，从而将这些任务写入日志或者保存起来以便之后进行处理。 然而，我们无法通过常规方法来找出哪些任务已经开始但尚未结束。这意味着我们无法在关闭过程中知道正在执行的任务的状态，除非任务本身会执行某种检查。要知道哪些任务还没有完成，你不仅需要知道哪些任务还没有开始，而且还需要知道当Executor关闭时哪些任务正在执行。 未捕获的异常当线程内代码抛出RuntimeException时，Thread API提供了uncaughtExceptionHanlder，它能检测出某个线程由于未捕获的异常而终结的情况。 12345678910111213141516171819202122232425262728293031323334353637public class CaptureUncaughtException &#123; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(new HandlerThreadFactory()); executorService.execute(new ExceptionThread2()); &#125;&#125;class ExceptionThread2 implements Runnable &#123; @Override public void run() &#123; Thread t = Thread.currentThread(); System.out.println("run() by " + t); System.out.println("eh = " + t.getUncaughtExceptionHandler()); throw new RuntimeException(); &#125;&#125;class MyUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler &#123; @Override public void uncaughtException(Thread t, Throwable e) &#123; System.out.println("caught " + e); &#125;&#125;class HandlerThreadFactory implements ThreadFactory &#123; @Override public Thread newThread(Runnable r) &#123; System.out.println(this + " creating new Thread"); Thread t = new Thread(r); System.out.println("created " + t); t.setUncaughtExceptionHandler(new MyUncaughtExceptionHandler()); System.out.println("eh = " + t.getUncaughtExceptionHandler()); return t; &#125;&#125; JVM关闭JVM既可以正常关闭，也可以强行关闭。 关闭钩子在正常关闭中，JVM首先调用所有已注册的关闭钩子。关闭钩子是指通过Runtime.addShutdownHook注册的但尚未开始的线程。JVM并不能保证关闭钩子的调用顺序。在关闭应用程序线程时，如果有（守护或非守护）线程仍然在运行，那么这些线程接下来将与关闭进程并发执行。当所有的关闭钩子都执行结束时，如果runFinalizersOnExit为true，那么JVM将运行终结器，然后再停止。JVM并不会停止或中断任何在关闭时仍然运行的应用程序线程。当JVM最终结束时，这些线程将被强行结束。如果关闭钩子或终结器没有执行完成，那么正常关闭进行“挂起”并且JVM必须被强行关闭。当被强行关闭时，只是关闭JVM，而不会运行关闭钩子。 守护线程线程可分为两种：普通线程和守护线程。在JVM启动时创建的所有线程中，除了主线程以外，其他的线程都是守护线程（例如垃圾回收器以及其他执行辅助工作的线程）。当创建一个新线程时，新线程将继承创建它的线程的守护状态，因此在默认情况下，主线程创建的所有线程都是普通线程。 普通线程与守护线程之间的差异仅在于当线程退出时发生的操作。当一个线程退出时，JVM会检查其他正在运行的线程，如果这些线程都是守护线程，那么JVM会正常退出操作。当JVM停止时，所有仍然存在的守护线程都将被抛弃—-既不会执行finally代码块，也不会执行回卷栈，而JVM只是退出。我们应尽可能少地使用守护线程。 终结器finalize方法就是终结器，JVM并不保证何时运行甚至是否运行，应该避免使用终结器。 小结在任务、线程、服务以及应用程序等模块中的生命周期结束问题，可能会增加它们在设计和实现时的复杂性。Java并没有提供某种抢占式的机制来取消操作或者终结线程。相反，它提供了一种协作式的中断机制来实现取消操作，但这要依赖于如何构建取消操作的协议，以及能否始终遵循这些协议。通过FutureTask和Executor框架，可以帮助我们构建可取消的任务和服务。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程6-任务执行]]></title>
    <url>%2F2016%2F12%2F13%2Fconcurrency6%2F</url>
    <content type="text"><![CDATA[任务执行任务通常是一些抽象的且离散的工作单元。通过把应用程序的工作分解到多个任务中，可以简化程序的组织结构 在线程中执行任务在理想情况下，各个任务之间是相互独立的：任务并不依赖其他任务的状态、结果或边界效应。独立性有助于实现并发，例如向web服务器提交一个请求，不会影响正在处理的其他请求。 为任务创建线程如果为每一个任务都创建一个线程，那么资源开销是极大的，无限制的创建线程存在一些缺陷： 线程生命周期的开销非常高 资源消耗 稳定性 Executor框架 123public interface Executor &#123; void execute(Runnable command);&#125; Executor基于生产者-消费者模式，提交任务的操作相当于生产者（生成待完成的工作单元），执行任务的线程则相当于消费者（执行完这些工作单元）。 每当看到下面这种形式的代码时： new Thread(runnable).start(); 并且你希望获得一种更灵活的执行策略时，请考虑使用Executor来代替Thread。 线程池线程池从字面意思来看，是指管理一组同构工作线程的资源池。 在线程池中执行任务比「为每一个任务分配一个线程」优势更多。通过重用现有的线程而不是创建新线程，可以在处理多个请求时分摊在线程创建和销毁过程中产生的巨大开销。另外一个额外的好处死，当请求到达时，工作线程通常已经存在，因此不会由于等待创建线程而延迟任务的执行，从而提高了响应性。 Executors中的静态工厂方法提供了一些线程池： newFixedThreadPool newCachedThreadPool newSingleThreadExecutor newScheduledThreadPool Executor的生命周期ExecutorService提供了一些用于生命周期管理的方法。 1234567891011121314public interface ExecutorService extends Executor &#123; void shutdown(); List&lt;Runnable&gt; shutdownNow(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future&lt;?&gt; submit(Runnable task); &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; ExecutorService的生命周期有3种状态：运行、关闭和已终止。ExecutorService在初始创建时处于运行状态。shotdown方法将执行平缓的关闭过程：不再接受新的任务，同事等待已经提交的任务执行完成–包括那些还未开始执行的任务。shotdownNow方法将执行粗暴的关闭过程：它将尝试取消所有运行中的任务，并且不再启动队列中尚未开始执行的任务。 延迟任务与周期任务Timer类负责管理延迟任务以及周期任务。然而，Timer存在一些缺陷，因此应该考虑使用ScheduledThreadPoolExecutor来代替它。 携带结果的任务Callable与FutureCallable：它人为主入口点将返回一个值，并可能抛出一个异常。Future表示一个任务的生命周期，并提供相应的方法来判断是否已经完成或取消，以及获取任务的结果和取消任务等。 可以通过许多种方法创建一个Future来描述任务。ExecutorService中的所有submit方法都将反悔一个Future，从而将一个Runnable或Callable提交给Executor，并得到一个Future用来获得任务的执行结果或者取消任务。 CompletionService与BlockingQueueCompletionService将Executor和BlockingQueue的功能融合在一起。你可以将Callable任务提交给它来执行，然后使用类似于队列操作的take和poll等方法来获得已完成的结果，而这些结果会在完成时将被封装为Future。ExecutorCompletionService实现了CompletionService，并将计算部分委托给一个Executor。 小结通过围绕任务执行来设计应用程序，可以简化开发过程，并有助于实现并发。Executor框架将任务提交与执行策略解耦开来，同时还支持多重不同类型的执行策略。当需要创建线程来执行任务时，可以考虑使用Executor。要想在将应用程序分解为不同的任务时获得最大的好处，必须定义清晰的任务边界。某些应用程序中存在着比较明显的任务边界，而在其他一些程序中则需要进一步分析才能揭示出粒度更细的并行性。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成JWT到Spring Boot项目]]></title>
    <url>%2F2016%2F12%2F08%2Fspring-boot-jwt%2F</url>
    <content type="text"><![CDATA[这篇文章我们来讲一下如何集成JWT到Spring Boot项目中来完成接口的权限验证。 JWTJWT是一种用于双方之间传递安全信息的简洁的、URL安全的表述性声明规范。JWT作为一个开放的标准（ RFC 7519 ），定义了一种简洁的，自包含的方法用于通信双方之间以Json对象的形式安全的传递信息。因为数字签名的存在，这些信息是可信的，JWT可以使用HMAC算法或者是RSA的公私秘钥对进行签名。如何使用JWT？ 在身份鉴定的实现中，传统方法是在服务端存储一个session，给客户端返回一个cookie，而使用JWT之后，当用户使用它的认证信息登陆系统之后，会返回给用户一个JWT，用户只需要本地保存该token（通常使用local storage，也可以使用cookie）即可。因为用户的状态在服务端的内存中是不存储的，所以这是一种 无状态 的认证机制。服务端的保护路由将会检查请求头 Authorization 中的JWT信息，如果合法，则允许用户的行为。由于JWT是自包含的，因此减少了需要查询数据库的需要。 JWT的这些特性使得我们可以完全依赖其无状态的特性提供数据API服务，甚至是创建一个下载流服务。因为JWT并不使用Cookie的，所以你可以使用任何域名提供你的API服务而不需要担心跨域资源共享问题（CORS）。大概就是这样： Spring Boot集成我是勤劳的搬运工，这应该是翻译老外的东西，项目地址：https://github.com/thomas-kendall/trivia-microservices。 废话不多说了，我直接上代码,依然是搬运工。我是gradle构建的，就是引入一些依赖的jar包。顺便推荐一下阿里云的中央仓库 http://maven.aliyun.com/nexus/content/groups/public/ 12345678910111213141516dependencies &#123; compile('org.springframework.boot:spring-boot-starter-aop') compile('org.springframework.boot:spring-boot-starter-security') compile('org.mybatis.spring.boot:mybatis-spring-boot-starter:1.1.1') compile('org.springframework.boot:spring-boot-starter-web') compile('com.google.guava:guava:20.0') compile('com.alibaba:druid:0.2.9') compile('org.apache.commons:commons-lang3:3.5') compile('commons-collections:commons-collections:3.2.2') compile('commons-codec:commons-codec:1.10') compile('com.github.pagehelper:pagehelper:4.1.6') compile('io.jsonwebtoken:jjwt:0.6.0') runtime('mysql:mysql-connector-java') compileOnly('org.projectlombok:lombok') testCompile('org.springframework.boot:spring-boot-starter-test')&#125; 下面这个是类是产生token的主要类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * Created by YangFan on 2016/11/28 上午10:01. * &lt;p/&gt; */@Slf4jpublic class JsonWebTokenUtility &#123; private SignatureAlgorithm signatureAlgorithm; private Key secretKey; public JsonWebTokenUtility() &#123; // 这里不是真正安全的实践 // 为了简单，我们存储一个静态key在这里， signatureAlgorithm = SignatureAlgorithm.HS512; String encodedKey = "L7A/6zARSkK1j7Vd5SDD9pSSqZlqF7mAhiOgRbgv9Smce6tf4cJnvKOjtKPxNNnWQj+2lQEScm3XIUjhW+YVZg=="; secretKey = deserializeKey(encodedKey); &#125; public String createJsonWebToken(AuthTokenDetails authTokenDetails) &#123; String token = Jwts.builder().setSubject(authTokenDetails.getId().toString()) .claim("username", authTokenDetails.getUsername()) .claim("roleNames", authTokenDetails.getRoleNames()) .setExpiration(authTokenDetails.getExpirationDate()) .signWith(getSignatureAlgorithm(), getSecretKey()).compact(); return token; &#125; private Key deserializeKey(String encodedKey) &#123; byte[] decodedKey = Base64.getDecoder().decode(encodedKey); Key key = new SecretKeySpec(decodedKey, getSignatureAlgorithm().getJcaName()); return key; &#125; private Key getSecretKey() &#123; return secretKey; &#125; public SignatureAlgorithm getSignatureAlgorithm() &#123; return signatureAlgorithm; &#125; public AuthTokenDetails parseAndValidate(String token) &#123; AuthTokenDetails authTokenDetails = null; try &#123; Claims claims = Jwts.parser().setSigningKey(getSecretKey()).parseClaimsJws(token).getBody(); String userId = claims.getSubject(); String username = (String) claims.get("username"); List&lt;String&gt; roleNames = (List) claims.get("roleNames"); Date expirationDate = claims.getExpiration(); authTokenDetails = new AuthTokenDetails(); authTokenDetails.setId(Long.valueOf(userId)); authTokenDetails.setUsername(username); authTokenDetails.setRoleNames(roleNames); authTokenDetails.setExpirationDate(expirationDate); &#125; catch (JwtException ex) &#123; log.error(ex.getMessage(), ex); &#125; return authTokenDetails; &#125; private String serializeKey(Key key) &#123; String encodedKey = Base64.getEncoder().encodeToString(key.getEncoded()); return encodedKey; &#125;&#125; 现在我们需要一个定制授权过滤器，将能读取请求头部信息，在Spring中已经有一个这样的授权Filter称为：RequestHeaderAuthenticationFilter，我们只要扩展继承即可： 123456789101112131415161718@Componentpublic class JsonWebTokenAuthenticationFilter extends RequestHeaderAuthenticationFilter &#123; public JsonWebTokenAuthenticationFilter() &#123; // Don't throw exceptions if the header is missing this.setExceptionIfHeaderMissing(false); // This is the request header it will look for this.setPrincipalRequestHeader("Authorization"); &#125; @Override @Autowired public void setAuthenticationManager( AuthenticationManager authenticationManager) &#123; super.setAuthenticationManager(authenticationManager); &#125;&#125; 在这里，头部信息将被转换为Spring Authentication对象，名称为PreAuthenticatedAuthenticationToken我们需要一个授权提供者读取这个记号，然后验证它，然后转换为我们自己的定制授权对象，就是把header里的token转化成我们自己的授权对象。然后把解析之后的对象返回给Spring Security，这里就相当于完成了token-&gt;session的转换。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Componentpublic class JsonWebTokenAuthenticationProvider implements AuthenticationProvider &#123; private JsonWebTokenUtility tokenService = new JsonWebTokenUtility(); @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; Authentication authenticatedUser = null; // Only process the PreAuthenticatedAuthenticationToken if (authentication.getClass(). isAssignableFrom(PreAuthenticatedAuthenticationToken.class) &amp;&amp; authentication.getPrincipal() != null) &#123; String tokenHeader = (String) authentication.getPrincipal(); UserDetails userDetails = parseToken(tokenHeader); if (userDetails != null) &#123; authenticatedUser = new JsonWebTokenAuthentication(userDetails, tokenHeader); &#125; &#125; else &#123; // It is already a JsonWebTokenAuthentication authenticatedUser = authentication; &#125; return authenticatedUser; &#125; private UserDetails parseToken(String tokenHeader) &#123; UserDetails principal = null; AuthTokenDetails authTokenDetails = tokenService.parseAndValidate(tokenHeader); if (authTokenDetails != null) &#123; List&lt;GrantedAuthority&gt; authorities = authTokenDetails.getRoleNames().stream() .map(SimpleGrantedAuthority::new).collect(Collectors.toList()); // userId介入Spring Security principal = new User(authTokenDetails.getId().toString(), "", authorities); &#125; return principal; &#125; @Override public boolean supports(Class&lt;?&gt; authentication) &#123; return authentication.isAssignableFrom( PreAuthenticatedAuthenticationToken.class)|| authentication.isAssignableFrom( JsonWebTokenAuthentication.class); &#125;&#125; Spring Security上面完成了JWT和Spring Boot的集成。接下来我们再如何把自己的权限系统也接入Spring Security。刚才已经展示了通过JsonWebTokenAuthenticationProvider的处理，我们已经能通过header的token来识别用户，并拿到他的角色和userId等信息。 配置Spring Security有3个不可缺的类。首先配置拦截器，拦截所有的请求。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * Created by YangFan on 2016/11/28 上午11:32. * &lt;p/&gt; */@Componentpublic class DemoSecurityInterceptor extends AbstractSecurityInterceptor implements Filter &#123; @Autowired private FilterInvocationSecurityMetadataSource securityMetadataSource; @Autowired @Override public void setAccessDecisionManager(AccessDecisionManager accessDecisionManager) &#123; super.setAccessDecisionManager(accessDecisionManager); &#125; @Autowired @Override public void setAuthenticationManager(AuthenticationManager authenticationManager) &#123; super.setAuthenticationManager(authenticationManager); &#125; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; FilterInvocation fi = new FilterInvocation(request, response, chain); invoke(fi); &#125; public Class&lt;? extends Object&gt; getSecureObjectClass() &#123; return FilterInvocation.class; &#125; public void invoke(FilterInvocation fi) throws IOException, ServletException &#123; InterceptorStatusToken token = super.beforeInvocation(fi); try &#123; fi.getChain().doFilter(fi.getRequest(), fi.getResponse()); &#125; finally &#123; super.afterInvocation(token, null); &#125; &#125; @Override public SecurityMetadataSource obtainSecurityMetadataSource() &#123; return this.securityMetadataSource; &#125; public void destroy() &#123; &#125; public void init(FilterConfig filterconfig) throws ServletException &#123; &#125;&#125; 然后是把我们自己的权限数据加载到Spring Security中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/** * Created by YangFan on 2016/11/28 上午11:33. * &lt;p/&gt; * 最核心的地方，就是提供某个资源对应的权限定义，即getAttributes方法返回的结果。 * 此类在初始化时，应该取到所有资源及其对应角色的定义。 */@Componentpublic class DemoInvocationSecurityMetadataSourceService implements FilterInvocationSecurityMetadataSource &#123; private static Map&lt;String, Collection&lt;ConfigAttribute&gt;&gt; resourceMap = null; public DemoInvocationSecurityMetadataSourceService() &#123; &#125; private void loadResourceDefine() &#123; /* * 应当是资源为key， 权限为value。 资源通常为url， 权限就是那些以ROLE_为前缀的角色。 一个资源可以由多个权限来访问。 * sparta */ Role r = new Role(); r.setId(0L); r.setName("admin"); // 假数据 List&lt;Role&gt; roles = Collections.singletonList(r); // 替换为查询角色列表 resourceMap = new HashMap&lt;&gt;(); for (Role role : roles) &#123; ConfigAttribute ca = new SecurityConfig(role.getName()); Map&lt;String, Object&gt; params = new HashMap&lt;&gt;(); params.put("roleId", role.getId()); // 查询每个角色对于的权限,我这里假设直接查到了url List&lt;String&gt; resources = Collections.singletonList("/user/*"); for (String url : resources) &#123; /* * 判断资源文件和权限的对应关系，如果已经存在相关的资源url，则要通过该url为key提取出权限集合，将权限增加到权限集合中。 * sparta */ if (resourceMap.containsKey(url)) &#123; Collection&lt;ConfigAttribute&gt; value = resourceMap.get(url); value.add(ca); resourceMap.put(url, value); &#125; else &#123; Collection&lt;ConfigAttribute&gt; atts = new ArrayList&lt;&gt;(); atts.add(ca); resourceMap.put(url, atts); &#125; &#125; &#125; &#125; @Override public Collection&lt;ConfigAttribute&gt; getAllConfigAttributes() &#123; loadResourceDefine(); return null; &#125; // 根据URL，找到相关的权限配置。 @Override public Collection&lt;ConfigAttribute&gt; getAttributes(Object object) throws IllegalArgumentException &#123; FilterInvocation filterInvocation = (FilterInvocation) object; for (String url : resourceMap.keySet()) &#123; RequestMatcher requestMatcher = new AntPathRequestMatcher(url); HttpServletRequest httpRequest = filterInvocation.getHttpRequest(); if (requestMatcher.matches(httpRequest)) &#123; return resourceMap.get(url); &#125; &#125; return null; &#125; @Override public boolean supports(Class&lt;?&gt; arg0) &#123; return true; &#125;&#125; 现在我们拿到了用户的角色，也拿到了系统里有的角色和权限，就需要判断他是否有这个权限了，配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Created by YangFan on 2016/11/28 下午12:19. * &lt;p/&gt; * AccessdecisionManager在Spring security中是很重要的。 * &lt;p&gt; * 在验证部分简略提过了，所有的Authentication实现需要保存在一个GrantedAuthority对象数组中。 * 这就是赋予给主体的权限。 GrantedAuthority对象通过AuthenticationManager * 保存到 Authentication对象里，然后从AccessDecisionManager读出来，进行授权判断。 * &lt;p&gt; * Spring Security提供了一些拦截器，来控制对安全对象的访问权限，例如方法调用或web请求。 * 一个是否允许执行调用的预调用决定，是由AccessDecisionManager实现的。 * 这个 AccessDecisionManager 被AbstractSecurityInterceptor调用， * 它用来作最终访问控制的决定。 这个AccessDecisionManager接口包含三个方法： * &lt;p&gt; * void decide(Authentication authentication, Object secureObject, * List&lt;ConfigAttributeDefinition&gt; config) throws AccessDeniedException; * boolean supports(ConfigAttribute attribute); * boolean supports(Class clazz); * &lt;p&gt; * 从第一个方法可以看出来，AccessDecisionManager使用方法参数传递所有信息，这好像在认证评估时进行决定。 * 特别是，在真实的安全方法期望调用的时候，传递安全Object启用那些参数。 * 比如，让我们假设安全对象是一个MethodInvocation。 * 很容易为任何Customer参数查询MethodInvocation， * 然后在AccessDecisionManager里实现一些有序的安全逻辑，来确认主体是否允许在那个客户上操作。 * 如果访问被拒绝，实现将抛出一个AccessDeniedException异常。 * &lt;p&gt; * 这个 supports(ConfigAttribute) 方法在启动的时候被 * AbstractSecurityInterceptor调用，来决定AccessDecisionManager * 是否可以执行传递ConfigAttribute。 * supports(Class)方法被安全拦截器实现调用， * 包含安全拦截器将显示的AccessDecisionManager支持安全对象的类型。 */@Componentpublic class DemoAccessDecisionManager implements AccessDecisionManager &#123; public void decide(Authentication authentication, Object object, Collection&lt;ConfigAttribute&gt; configAttributes) throws AccessDeniedException, InsufficientAuthenticationException &#123; if (configAttributes == null) &#123; return; &#125; for (ConfigAttribute ca : configAttributes) &#123; String needRole = ca.getAttribute(); //ga 为用户所被赋予的权限。 needRole 为访问相应的资源应该具有的权限。 for (GrantedAuthority ga : authentication.getAuthorities()) &#123; if (needRole.trim().equals(ga.getAuthority().trim())) &#123; return; &#125; &#125; &#125; throw new AccessDeniedException("没有权限进行操作！"); &#125; public boolean supports(ConfigAttribute attribute) &#123; return true; &#125; public boolean supports(Class&lt;?&gt; clazz) &#123; return true; &#125;&#125; 我们试试登录的接口： 然后我们用这个token来调用另外一个接口。我们先试试不传Token会返回什么 判断没有登录，现在再来试试带上token的请求。已经成功的请求到了数据。 好了，核心配置就是这些，我把这些代码上传github上，有需要的可以下载下来看看。里面的角色和权限都是虚拟数据，应用还需要自行修改代码。https://github.com/sail-y/spring-boot-jwt]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程5-基础构建模块]]></title>
    <url>%2F2016%2F12%2F08%2Fconcurrency5%2F</url>
    <content type="text"><![CDATA[基础构建模块第4章介绍了构造线程安全类时采用的一些技术，例如将线程安全性委托给现有的线程安全类。委托是创建线程安全类的一个最有效的策略：只需让现有的线程安全类管理所有的状态即可。下面将介绍一些JDK提供的工具类。 同步容器类同步容器类包括Vector和Hashtable。这些类实现线程安全的方式是：将它们的状态封装起来，并对每个共有方法都进行同步，使得每次只有一个线程能访问容器的状态。 同步容器类的问题同步容器类都是线程安全的，但在某些情况下需要加锁来保护复合操作。例如2个线程都在进行「若没有，则添加」的运算，如果没有对这个复合操作加锁，就可能会出问题。 迭代器与ConcurrentModificationException无论是迭代还是foreach循环，当它们发现容器在迭代过程中被修改时，就会抛出ConcurrentModificationException异常。如果不希望在迭代期间对容器加锁，有一种替代方法就是「克隆」容器，并在副本中进行迭代。 隐藏迭代器虽然加锁可以防止迭代器抛出ConcurrentModificationException，但是必须在所有对共享容器进行迭代的地方都需要加锁。还有一个很隐蔽的迭代器，就是没有显式的迭代器，但是实际上也执行了迭代操作，那就是编译器会将字符串的连接操作转化为StringBuilder.append，而这个方法会调用容器的toString方法，标准容器的toString方法会迭代容器。12Set&lt;Integer&gt; set = new HashSet&lt;&gt;();System.out.println(set); 如果在输出期间对容器进行了修改，就会抛出异常。 并发容器JDK5提供了多种并发容器类来改进同步容器的性能。因为同步容器对所有容器状态的访问都串行化，降低了并发性，性能不太好。通过并发容器来代替同步容器，可以极大的提高伸缩性并降低防线。例如ConcurentHashMap和CopyOnWriteArrayList。BlockingQueue提供可阻塞的插入和获取操作。如果队列为空，那么获取元素的操作将一直阻塞，直到队列中出现一个可用的元素。如果队列已满，那么插入元素的操作将一直阻塞，直到队列中出现可用的空间。 ConcurrentHashMap同步容器类在执行每个操作期间都持有一个锁，HashMap的键值对是通过单向链表来实现的，当遍历很长的链表并且在某些或者全部元素上调用equals方法时，会耗费很长时间，而其他线程在这段时间内都不能访问该容器。ConcurrentHashMap与HashMap一样也是一个基于散列的Map，它使用了一种分段锁的机制来实现更大程度的共享，而不是将每个方法都进行同步。这样执行写入操作的线程可以并发地访问Map。它提供的迭代器也不会抛出ConcurrentModificationException，因此不需要在迭代的时候加锁。 ConcurrentHashMap将一些常见的复合操作实现为了原子操作，例如putIfAbsent,remove,replace等。 CopyOnWriteArrayListCopyOnWriteArrayList用于替代同步List，在某些情况下它提供了更好的并发性能，并且在迭代期间不需要对容器进行加锁或复制。CopyOnWriteArrayList底层用基础数组实现，不会被修改，可以随意并发的访问。不过显然每当修改容器的时候会复制底层数组，这会造成一定的开销。仅当迭代操作远远多余修改操作时，才应该使用这个容器。这个容器适用于许多事件通知系统：分发通知时迭代监听器，并调用。而注册或者注销监听器的操作则较少。 阻塞队列和生产者-消费者模式刚才提到BlockingQueue提供可阻塞的put和take操作。阻塞队列支持生产者-消费者这种设计模式。该模式将「找出需要完成的工作」与「执行工作」这两个过程分离开来，并把工作放入一个「待完成」的列表以便在随后处理。在基于阻塞队列构建的生产者-消费者设计中，当数据生成时，生产者把数据放入队列，而当消费者准备处理数据时，将从队列中获取数据。 12345678910111213141516171819202122232425262728293031323334353637public class BlockingQueueTest &#123; public static void main(String[] args) &#123; final BlockingQueue&lt;String&gt; bq = new ArrayBlockingQueue&lt;String&gt;(10); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; &#123; int i = 0; while (true) &#123; System.out.println("produce " + i++); try &#123; bq.put(i + ""); TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); executorService.execute(() -&gt; &#123; while (true) &#123; try &#123; String take = bq.take(); System.out.println("take " + take); TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); executorService.shutdown(); &#125;&#125; 但需要注意的是我们应该用有界队列，因此如果消费者处理速度较慢，队列可能会将耗尽内存。在构建高可靠的应用程序时，有界队列是一种强大的资源管理工具：它们能抑制并防止产生过多的工作项，使应用程序在负荷过载的情况秀爱变得更加健壮。 串行线程封闭对于可变对象，生产者-消费者这种设计与阻塞队列一起，促进了串行线程封闭，从而将对象所有权从生产者缴费给消费者。线程封闭对象只能由单个线程拥有，但可以通过安全地发布该对象来「转移」所有权。 双端队列JDK6还增加了两种容器类型，Deque和BlockingDeque。Deque是一个双端队列，实现了在队列头和队列尾的高效插入和移除。具体实现包括ArrayDeque和LinkedBlockingDeque。 阻塞方法与中断方法线程可能会阻塞或者暂停执行，等待I/O操作，等待锁等。简单举例就是Thread.sleep()。当某方法会抛出InterruptedException时，表示该方法是一个阻塞方法，如果这个方法被中断，那么它将努力提前结束阻塞状态。Thread提供了interrupt方法，用于中断线程或者查询线程是否已经被中断。每个线程都有一个布尔类型的属性，表示线程的中断状态，当中断线程时将设置这个值。我们看源码就知道，interrupt()只是将interrupt的标记设置一下而已，interrupt0()是一个native方法。具体什么时候中断，JDK并不保证。 1234567891011121314public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; &#125; &#125; interrupt0(); &#125; 看个例子： 1234567891011121314151617/** * Created by YangFan on 2016/10/26 下午4:17. * &lt;p/&gt; * 中断机制是一种协作机制，也就是说通过中断并不能直接终止另一个线程，而需要被中断的线程自己处理。 */public class InterruptTest &#123; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; &#123; while (!Thread.currentThread().isInterrupted()) &#123; System.out.println("running"); &#125; &#125;); executorService.shutdownNow(); &#125;&#125; 同步工具类下面介绍一些并发包的同步工具类，它们封装了一些状态，这些状态将决定执行同步工具类的线程是继续执行还是等待，此外还提供了一些方法对状态进行操作，以及另一些方法用于高效地等待同步工具类进入到预期状态，这些类有CountDownLatch、Semaphore和Barrier等。 CountDownLatchLatch可以延迟线程的进度直到其到达终止状态。它的作用相当于一扇门：在条件达到之前，这扇门是关闭着的，并没有任何线程能通过，直到条件到达结束状态时，这扇门打开并允许所有线程通过。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * Created by YangFan on 2016/10/26 下午5:10. * &lt;p/&gt; * CountDownLatch主要提供的机制是当多个（具体数量等于初始化CountDownLatch时count参数的值）线程都达到了预期状态或完成预期工作时触发事件， * 其他线程可以等待这个事件来触发自己的后续工作。值得注意的是，CountDownLatch是可以唤醒多个等待的线程的。 */public class CountDownLatchTest &#123; private static class WorkThread extends Thread &#123; private CountDownLatch countDownLatch; private int sleepSecond; public WorkThread(String name, CountDownLatch countDownLatch, int sleepSecond) &#123; super(name); this.countDownLatch = countDownLatch; this.sleepSecond = sleepSecond; &#125; @Override public void run() &#123; try &#123; System.out.println(this.getName() + " start: " + LocalDateTime.now()); TimeUnit.SECONDS.sleep(sleepSecond); countDownLatch.countDown(); System.out.println(this.getName() + " end: " + LocalDateTime.now()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private static class DoneThread extends Thread &#123; private CountDownLatch countDownLatch; public DoneThread(String name, CountDownLatch countDownLatch) &#123; super(name); this.countDownLatch = countDownLatch; &#125; @Override public void run() &#123; try &#123; System.out.println(this.getName() + " await start:" + LocalDateTime.now()); countDownLatch.await(); System.out.println(this.getName() + " await end:" + LocalDateTime.now()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; // CountDownLatch指定3次调用，无论前面有多少线程await，都需要等待CountDownLatch调用3次countDown()统一唤醒 public static void main(String[] args) &#123; CountDownLatch countDownLatch = new CountDownLatch(3); DoneThread d0 = new DoneThread("DoneThread1", countDownLatch); DoneThread d1 = new DoneThread("DoneThread2", countDownLatch); d0.start(); d1.start(); WorkThread w0 = new WorkThread("WorkThread0", countDownLatch, 2); WorkThread w1 = new WorkThread("WorkThread1", countDownLatch, 3); WorkThread w2 = new WorkThread("WorkThread2", countDownLatch, 4); w0.start(); w1.start(); w2.start(); &#125;&#125; FutureTaskFutureTask可以获得线程返回的结果，get方法取决于线程的状态，如果已经完成会直接返回，否则会一直阻塞直到任务执行完成。 12345678910111213141516171819202122232425/** * Created by YangFan on 2016/10/26 下午5:54. * &lt;p/&gt; */class CallableThread implements Callable &#123; @Override public Object call() throws Exception &#123; System.out.println("call()"); TimeUnit.MILLISECONDS.sleep(1500); return "123"; &#125;&#125;public class CallableTest &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; ExecutorService executorService = Executors.newCachedThreadPool(); Future future = executorService.submit(new CallableThread()); executorService.shutdown(); System.out.println(future.get());// while (!future.isDone()) &#123;// System.out.println(future.get());// &#125; &#125;&#125; SemaphoreSemaphore用来控制同时访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。Semaphore还可以用来实现某种资源池，或者对容器施加边界。Semaphore管理着一组虚拟的许可，许可的初始数量可通过构造函数来指定，在执行操作时可以先获得许可，并在使用后释放许可。如果没有许可，那么acquire()将阻塞直到有许可。 123456789101112131415161718192021222324252627/** * Created by YangFan on 2016/10/26 下午5:23. * &lt;p/&gt; * */public class SemaphoreTest &#123; public static void main(String[] args) &#123; final Semaphore semaphore = new Semaphore(5); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; semaphore.acquire(); System.out.println(Thread.currentThread().getName() + " acquire: " + LocalDateTime.now()); TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; semaphore.release(); System.out.println(Thread.currentThread().getName() + " release: " + LocalDateTime.now()); &#125; &#125;); &#125; &#125;&#125; CyclicBarrierCountDownLatch是一次性对象，一旦结束进入终止状态，就不能被重置。CyclicBarrier能阻塞一组线程直到某个事件发生。CyclicBarrier和CountDownLatch的关键区别在于，所有线程必须同时达到CyclicBarrier的条件，才能继续执行。CountDownLatch是等待某个条件或者事件，CyclicBarrier是等待其他线程。例如CountDownLatch是指6点一到大家就可以下班了，而CyclicBarrier是要等大家到齐了才能开会。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * Created by YangFan on 2016/10/26 下午5:43. * &lt;p/&gt; * CyclicBarrier从字面理解是指循环屏障，它可以协同多个线程，让多个线程在这个屏障前等待，直到所有线程都达到了这个屏障时，再一起继续执行后面的动作。 * */class CyclicBarrierThread implements Runnable &#123; private CyclicBarrier cyclicBarrier; private int sleepSecond; public CyclicBarrierThread(CyclicBarrier cyclicBarrier, int sleepSecond) &#123; this.cyclicBarrier = cyclicBarrier; this.sleepSecond = sleepSecond; &#125; @Override public void run() &#123; try &#123; System.out.println(Thread.currentThread().getName() + " running"); TimeUnit.SECONDS.sleep(sleepSecond); System.out.println(Thread.currentThread().getName() + " waiting " + LocalDateTime.now()); cyclicBarrier.await(); System.out.println(Thread.currentThread().getName() + " end wait " + LocalDateTime.now()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;&#125;/*CountDownLatch和CyclicBarrier都是用于多个线程间的协调的，它们二者的几个差别是：1、CountDownLatch是在多个线程都进行了latch.countDown()后才会触发事件，唤醒await()在latch上的线程，而执行countDown()的线程，执行完countDown()后会继续自己线程的工作；CyclicBarrier是一个栅栏，用于同步所有调用await()方法的线程，并且等所有线程都到了await()方法时，这些线程才一起返回继续各自的工作2、另外CountDownLatch和CyclicBarrier的一个差别是，CountDownLatch不能循环使用，计数器减为0就减为0了，不能被重置，CyclicBarrier可以循环使用3、CountDownLatch可以唤起多条线程的任务，CyclicBarrier只能唤起一条线程的任务注意，因为使用CyclicBarrier的线程都会阻塞在await方法上，所以在线程池中使用CyclicBarrier时要特别小心，如果线程池的线程过少，那么就会发生死锁了 */public class CyclicBarrierTest &#123; public static void main(String[] args) &#123; Runnable command = () -&gt; System.out.println("I'm coming"); CyclicBarrier cyclicBarrier = new CyclicBarrier(3, command); CyclicBarrierThread t1 = new CyclicBarrierThread(cyclicBarrier, 2); CyclicBarrierThread t0 = new CyclicBarrierThread(cyclicBarrier, 2); CyclicBarrierThread t2 = new CyclicBarrierThread(cyclicBarrier, 1); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(t1); executorService.execute(t0); executorService.execute(t2); executorService.shutdown(); &#125;&#125; 总结 可变状态是至关重要的 所有的并发问题都可以归结为如何协调对并发状态的访问。可变状态越少，就越容易保证线程安全性 尽量将域声明为final类型，除非需要它们是可变的。 不可变对象一定是线程安全的 不可变对象能极大地降低并发编程的复杂性。它们更简单而且安全，可以任意共享而无须使用加锁或保护性复制等机制。 封装有助于管理复杂性。 在编写线程安全的程序时，虽然可以将所有数据都保存在全局变量中，但为什么要这样做？将数据封装在对象中，更易于维持不变性条件：将同步机制封装在对象中，更易于遵循同步策略。 用锁来保护每个可变变量。 当保护同一个不变性中的所有变量时，要使用同一个锁。 在执行复合操作期间，要持有锁。 如果从多个线程中访问同一个可变变量时没有同步机制，那么程序就会出现问题。 不要故作聪明地推断出不需要使用同步。 在设计过程中考虑线程安全，或者在文档中明确地指出它不是线程安全的。 将同步策略文档化。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程4-对象的组合]]></title>
    <url>%2F2016%2F12%2F06%2Fconcurrency4%2F</url>
    <content type="text"><![CDATA[对象的组合本章将介绍一些组合模式，这些模式能够使一个类更容易成为线程安全的，并且在维护这些类时不会无意中破坏类的安全性保证。 设计线程安全的类在设计线程安全类的过程中，需要包含以下三个基本要素： 找出构成对象状态的所有变量 找出约束状态变量的不变性条件 建立对象状态的并发访问管理策略 收集同步需求要确保的类的线程安全性，就需要确保它的不变性条件不会在并发访问的情况下被破坏，这就需要对其状态进行推断。不变性条件是指变量的取值范围约束，后验条件是指状态改变的时候值是否合法。如果不了解对象的不变性条件与后验条件，那么就不能确保线程安全性。要满足在状态变量的有效值或状态转换上的各种约束条件，就需要借助于原子性与封装性。 依赖状态的操作先验条件是基于前一个状态的操作，例如不能从空队列中移除一个元素，这个被称为依赖状态的操作。在单线程程序中，如果某个操作无法满足先验条件，那么就只能失败。但在并发程序中先验条件可能会由于其他线程执行的操作而变成真。在并发程序中要一只等到先验条件为真，然后再执行操作。后面将会介绍JDK提供的BlockingQueue和Semaphore等同步工具类来实现依赖状态的行为。 状态的所有权许多情况下，所有权与封装性总是相互关联的：对象封装它拥有的状态，反之也成立，即对它封装的状态拥有所有权。 实例封闭如果某对象不是线程安全的，那么可以通过多种技术使其在多线程程序中安全地使用。你可以确保该对象只能由单个线程访问（线程封闭），或者通过一个锁来保护该对象的所有访问。将数据封装在对象内部，可以将数据的访问限制在对象的方法上，从而更容易确保线程在访问数据时总能持有正确的锁。封闭机制更易于构造线程安全的类，因为当封闭类的状态时，在分析类的线程安全性时就无须检查整个程序。 Java监视器模式从线程封闭原则及其逻辑推论可以得出Java监视器模式。遵循Java监视器模式的对象会把对象的所有可变状态都封装起来，并由对象自己的内置锁来保护。意思就是对象的属性全部通过同步的方法来访问或者修改。 12345678910public class PrivateLock &#123; private final Object myLock = new Object(); Widget widget; void someMethod() &#123; synchronized(myLock) &#123; // 访问或修改Widget的状态 &#125; &#125;&#125; 使用私有的锁对象而不是对象的内置锁，有许多优点。私有锁对象可以将锁封装起来，使客户代码无法得到锁，但客户代码可以通过共有方法来访问锁，以便参与到它的同步策略中。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程3-对象的共享]]></title>
    <url>%2F2016%2F12%2F05%2Fconcurrency3%2F</url>
    <content type="text"><![CDATA[对象的共享要编写正确的并发程序，管关键问题在于：在访问共享的可变状态时需要进行正确的管理。本章介绍如何共享和发布对象，从而使它们能够安全地由多个线程同时访问。 可见性「可见性」是指当一个线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。而普通变量做不到这一点，普通变量的值在线程间传递均需要通过主内存来完成，例如线程A修改一个普通变量的值，然后向主内存进行回写，另外一条线程B在线程A回写完成了之后再从主内存进行读取操作，新变量值才会对线程B可见。Java内存模型的有序性可以总结为一句话，如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指「线程内表现为串行的语义」，后半句是指「指令重排序」现象和「工作内存与主内存同步延迟」现象。Java语言提供了volatile和synchronized两个关键字来保证线程之间操作的有序性，volatile关键字本身就包含了禁止指令重排序的语义，而synchronized则是由”一个变量在同一时刻只允许一条线程对其进行lock操作”这条规则获得的，这条规则规定了持有同一个锁的两个同步块只能串行地进入。 123456789101112131415161718public class NoVisibility &#123; private static boolean ready; private static int number; private static class ReaderThread extends Thread &#123; public void run() &#123; while (!ready) Thread.yield(); System.out.println(number); &#125; &#125; public static void main(String[] args) &#123; new ReaderThread().start(); number = 42; ready = true; &#125;&#125; 上面这个例子可能是一个死循环，因为ReaderThread线程可能永远看不到ready的值变化（可见性问题）。还有另外一种情况就是输出了0，因为指令重排序优化的原因，ready = true可能会先于number=42执行。 指令重排序优化是指为了使得处理内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行乱序执行优化，处理器会再计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的，因此如果存在一个计算任务依赖另外一个计算任务的中间结果，那么其顺序性并不能靠代码的先后顺序来保证。 失效数据NoVisibility展示了在缺乏同步的程序中可能产生错误结果中的一种情况：失效数据。除非在每次访问变量的时候使用同步。 非原子的64位操作因为double和long是64位数据，内存模型允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行。如果有多个线程共享一个并未声明为volatile的long或double类型的变量，并且同时对它们进行读取和修改操作，那么某些线程可能会读取到一个既非原值，也不是其他线程修改的值代表了「半个变量」的数值。不过这很罕见，因为目前的商用虚拟机几乎都还是选择把64位数据的读写作为原子操作来对待，所以我们写代码一般也不需要对long和double变量专门声明为volatile。 加锁与可见性synchronized和final关键字能实现可见性，synchronized的可见性是由「对一个变量执行unlock操作之前，必须先把此变量同步回主内存中」这条规则获得的。另外，final关键字也可以实现可见性，因为被final修饰的字段在构造器中一旦初始化完成，并且构造器没有把this传递出去，那在其他线程中就能看见final字段的值。 加锁的含义不仅仅局限于互斥行为，还包括内存可见性。为了确保所有线程都能看到共享变量的最新值，所有执行读操作或者写操作的线程都必须在同一个锁上同步。 Volatile变量Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介来实现可见性的，无论是普通变量还是volatile变量都是如此，普通变量与volatile变量的区别是，volatile的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。因此，可以说volatile保证了多线程时操作变量的可见性，而普通变量则不能保证这一点。volatile变量通常用做某个操作完成、发生中断或者状态的标志。 注意：加锁机制即可以确保可见性又可以确保原子性，而volatile变量只能确保可见性。 当且仅当满足以下所有条件时，才应该使用volatile变量： 对变量的写入操作不依赖变量的当前值，或者你能确保只有单个线程更新变量的值。 该变量不会与其他状态变量一起纳入不变性条件中 这种访问变量时不需要加锁 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Created by YangFan on 2016/10/25 上午10:37. * &lt;p/&gt; */class VThread_0 implements Runnable &#123; @Override public void run() &#123; while (VolatileTest.isRunning) &#123; &#125; &#125;&#125;class VThread_1 implements Runnable &#123; @Override public void run() &#123; VolatileTest.isRunning = false; System.out.println("stop running"); &#125;&#125;public class VolatileTest &#123; public static boolean isRunning = true; /* 这个不是必现，得多试几次 stop running 后死循环 在第二个线程更改后，第一个线程并没有马上停止，原因从Java内存模型（JMM）说起。 根据JMM，Java中有一块主内存，不同的线程有自己的工作内存，同一个变量值在主内存中有一份，如果线程用到了这个变量的话，自己的工作内存中有一份一模一样的拷贝。 每次进入线程从主内存中拿到变量值，每次执行完线程将变量从工作内存同步回主内存中。 出现打印结果现象的原因就是主内存和工作内存中数据的不同步造成的。 */ // 线程安全围绕的是可见性和原子性这两个特性展开的，volatile解决的是变量在多个线程之间的可见性，但是无法保证原子性。 public static void main(String[] args) throws InterruptedException &#123; VThread_0 vThread_0 = new VThread_0(); VThread_1 vThread_1 = new VThread_1(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(vThread_0); executorService.execute(vThread_1); executorService.shutdown(); &#125;&#125; 发布与逸出「发布」的意思是使对象能够在当前作用于之外的代码中使用，当某个不应该发布的对象被发布时，这种情况就被称为「逸出」。 123456789private Set&lt;Secret&gt; knownSecrets;public void initialize() &#123; knownSecrets = new HashSet&lt;&gt;();&#125;public Set&lt;Secret&gt; getKnownSecrets() &#123; return knownSecrets;&#125; 上面的代码发布了HashSet对象，但是却导致knownSecrets里的Secret逸出了，因为任何调用者都能修改knownSecrets里的值。 线程封闭当访问共享的可变数据时，通常需要使用同步，一种避免使用同步的方式就是不共享数据，如果仅在单线程内访问数据时，就不需要同步，这种技术被称为线程封闭。Java提供了ThreadLocal类来帮助维持线程封闭性。 不变性不可变对象一定是线程安全的，当满足以下条件时，对象才是不可变的： 对象创建以后其状态就不能修改。 对象的所有域都是final类型。 对象是正确创建的（在对象的创建期间，this对象没有逸出） 安全发布的常用模式要安全的地发布一个对象，对象的引用以及对象的状态必须同时对其他线程可见。一个正确构造的对象可以通过以下方式来安全地发布： 在静态初始化函数中初始化一个对象引用。 将对象的引用保存到volatile类型的域或者AtomicReferance对象中。 将对象的引用保存到某个正确构造对象的final类型域中。 将对象的引用保存到一个由锁保护的域中。 如果对象从技术上来看是可变的，但其状态在发布后不会在再改变，那么把这种对象称为「事实不可变对象」，在没有额外的同步情况下，任何线程都可以安全地使用被安全发布的事实不可变对象。 对象的发布需求取决于它的可变性： 不可变对象可以通过任意机制来发布。 事实不可变对象必须通过安全方式来发布。 可变对象必须通过安全方式来发布，并且必须是线程安全的或者由某个锁保护起来。 在并发程序中使用和共享对象时，可以使用一些实用的策略，包括： 线程封闭：线程封闭的对象只能由一个线程拥有，对象被封闭在该线程中，并且只能由这个线程修改。 只读共享：在没有额外同步的情况下，共享的只读对象可以由多个线程并发访问，但任何线程都不能修改它。共享的只读对象包括不可变对象和事实不可变对象。 线程安全共享：线程安全的对象在其内部实现同步，因此多个线程可以通过对象的公有接口来进行访问而不需要进一步的同步。 保护对象：被保护的对象只能通过持有特定的锁来访问。保护对象包括封装在其他线程安全对象中的对象，以及已发布的并且由某个特定锁保护的对象。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程2-线程安全性]]></title>
    <url>%2F2016%2F12%2F02%2Fconcurrency2%2F</url>
    <content type="text"><![CDATA[线程安全性在构建文件的并发程序时，必须正确地使用线程和锁，但这些终归只是一些机制。要编写线程安全的代码，其核心在于要对状态访问操作进行管理，特别是对共享的和可变的状态的访问。一个对象是否是线程安全的，取决于它是否被多个线程同时访问。当多个线程访问某个状态变量并且其中有一个线程执行写入操作时，必须采用同步机制来协同这些线程对变量的访问。Java中的主要同步机制是关键字synchronized，它提供了一种独占的加锁方式，但「同步」这个术语还包括volatile类型的变量，显式锁以及原子变量。如果当多个线程访问同一个可变的状态变量时没有使用合适的同步，那么程序就会出现错误。有三种方式可以修复这个问题： 不在线程之间共享该状态变量 将状态变量改为不可变的变量 在访问状态变量时使用同步 什么是线程安全性当多个线程访问某个类时，不管运行时环境采用何种调度方式或者这些线程将如何交替执行，并且在主调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么就称这个类是线程安全的。在线程安全类中封装了必要的同步机制，因此客户端无须进一步采取同步措施。无状态对象一定是线程安全的。（没有共享数据） 原子性像count++这种是属于非原子操作，它包含了三个独立的操作：读取count的值，将值+1，然后将计算结果写入count。所以不同线程在自增的时候，这个值在第一步拿到的可能是过期的数据。在并发编程中，这种由于不恰当的执行时序出现不正确的结果是一种非常重要的情况，它有一个正式的名字：竞态条件。 竞态条件当某个计算的正确性取决于多个线程的交替执行时序时，那么就会发生竞态条件。最常见的竞态条件就是「先检查后执行」，通过一个可能失效的观测结果来决定下一步的动作。 复合操作要避免竞态条件问题，就必须在某个线程修改该变量时，通过某种方式防止其他线程使用这个变量，从而确保其他线程只能在修改操作完成之前或之后读取和修改状态，而不是在修改状态的过程中。 我们将先检查后执行和读取-修改-写入等操作统称为「复合操作」。我们要以原子方式执行确保线程安全性，Java通过加锁机制来确保原子性。原子操作是指，对于访问同一个状态的所有操作（包括该操作本身）来说，这个操作是一个以原子方式执行的操作（不可分割的操作）。 加锁机制要保持状态的一致性，就需要在某个原子性操作中更新所有相关状态变量。 内置锁Java提供了一种内置的锁机制来支持原子性：同步代码快。以关键字synchronized来修饰的方法就是一种横跨整个方法体的同步代码块，其中该同步代码块的所就是方法调用的所在对象。静态的synchronized方法以Class对象作为锁。 123synchronized (lock) &#123; //访问或修改由锁保护的共享状态&#125; 每个Java对象都可以用做一个实现同步的锁，这些锁被称为内置锁或监视器锁。线程在进入同步代码之前会自动获得锁，并且在退出同步代码块时自动释放锁，而无论是通过正常的控制路径退出，还是通过代码块中抛出异常退出。获得内置锁的唯一途径就是进入由这个锁保护的同步代码块或方法。Java的内置锁是互斥锁，意思是最多只有一个线程能持有这种锁，当线程A尝试获取一个由线程B持有的锁时，线程A必须等待或阻塞，直到线程B释放这个锁。如果B永远不释放这个锁，那么A也将永远地等下去。由于每次只能又一个线程执行内置锁保护的代码块，因此，由这个锁保护的同步代码块会以原子方式执行，多个线程在执行该代码块时也不会相互干扰。并发环境中的原子性与事务应用程序的原子性有着相同的含义–一组语句作为一个不可分割的单元被执行。任何一个执行同步代码块的线程，都不可能看到有其他线程正在执行由同一个锁保护的同步代码块。 重入当某线程请求一个由其他线程持有的锁时，发出请求的线程就会阻塞。然而，由于内置锁是可重入的，因此如果某个线程试图获得一个已经由它自己持有的锁，那么这个请求就会成功。synchronized同步块对同一条线程来说是可重入的，不会出现把自己锁死的问题。 synchronized关键字在经过编译之后，会在同步块的前后分别形成monitorenter和monitorexit这两个字节码指令。根据虚拟机规范的要求，在执行monitorenter指令时，首先要尝试获取对象的锁，如果这个对象没有被锁定，或者当前线程已经拥有了那个对象的锁，把锁的计数器加1，相应的，在执行monitorexit指令时会将锁计数器减1，当计数器为0时，锁就被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。 用锁来保护状态由于锁能使其保护的代码路径以串行形式来访问，因此可以通过锁来构造一些协议以实现对共享状态的独占访问。对于可能被多个线程同时访问的可变状态变量，在访问它时都需要持有同一个锁，在这种情况下，我们称状态变量是由这个锁保护的。并非所有数据都需要锁的保护，只有被多个线程同时访问的可变数据才需要通过锁来保护。虽然synchronized方法可以确保单个操作的原子性，但如果把多个操作合并为一个复合操作，还是需要额外的加锁机制。 活跃性与性能一般来讲，对整个方法进行同步，每次只有一个线程可以执行，可能会导致性能糟糕。因此我们通常在只需要同步的地方用同步代码块，只对代码块中的共享状态变量进行加锁保护。我们要找到简单性（对整个方法进行同步）与并发性（对尽可能短的代码进行同步）之间的平衡。 通常，在简单性与性能之间存在着相互制约因素。当实现某个同步策略时，已定不要盲目地为了性能而牺牲简单性（这可能会破坏安全性）。 当执行时间较长的计算或者可能无法快速完成的操作时（例如：网络I/O或控制台I/O），一定不要持有锁。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程1-介绍]]></title>
    <url>%2F2016%2F12%2F01%2Fconcurrency1%2F</url>
    <content type="text"><![CDATA[在看完了《深入理解Java虚拟机》之后，继续看《Java并发编程实战一书》。相信在了解虚拟机之后，再来看并发相关知识，能理解得更透彻，书中也讲到，对Java内存模型理解得越深入，就对并发编程掌握得越好。顺道说一下，关于JDK里线程和并发相关类的使用，我主要是通过《Think in Java》学习的，这里就不再介绍基本使用方法了。 简介线程也被称为轻量级进程（这一部分在《深入理解Java虚拟机》中提到过，点击查看）。在大多数现代操作系统中， 都是以线程为基本的调度单位，而不是进程。 线程的优势要想充分发挥多处理器系统的强大计算能力，线程可以有效的降低程序的开发和维护成本，同时提升复杂应用程序的性能。 发挥多处理器的强大能力现在，多处理系统日益普及，个人PC基本上也都是多个处理器了。由于基本的调度单位是线程，因此如果程序中只有一个线程，那么最多只能在一个处理器上运行。在双处理器系统上，单线程的程序只能使用一半的CPU资源，而在拥有100个处理器的系统上将有99%的资源无法使用。多线程程序可以同时在多个处理器上执行，如果设计正确，多线程程序可以通过提高处理器资源的利用率来提升系统吞吐率。 建模的简单性通过使用线程，可以将复杂并且异步的工作流进一步分解为一组简单并且同步的工作流，每个工作流在一个单独的线程中运行，并在特定的同步位置进行交互。例如Servlet，框架负责解决请求管理、线程创建、负载平衡等细节，在正确的时刻将请求分发给正确的应用组件。我们开发的时候的就像在开发单线程程序一样，可以简化组件的开发。 异步事件的简化处理服务器应用程序在接受来自多个远程客户端的请求时，如果为每个连接都分配其各自的线程并且使用同步I/O，那么就会降低这类程序的开发难度。如果某个应用程序请求数据花费时间较长或者阻塞了，在单线程应用程序在阻塞期间所有的请求都会停顿，为了避免这个问题，单线程服务器应用程序必须使用非阻塞I/O，这种I/O的复杂性太远远高于同步I/O，并且很容易出错。然而，如果每个请求都拥有自己的处理线程，那么在处理某个请求时发生的阻塞将不会影响其他请求的处理。 响应更灵敏的用户界面将GUI应用的各种事件放入单独的线程中运行，时间线程能及时地处理界面事件，从而使用户界面具有更高的灵敏度。 线程带来的风险Java对线程的支持其实是一把双刃剑。虽然Java提供了相应的语言和库，以及一种明确的跨平台内存模型，这些工具简化了并发应用程序的开发，但同时也提高了对开发人员的技术要求。 安全性问题线程的安全性是非常复杂的，在没有充足同步的情况下，多个线程的操作执行顺序是不可预测的。由于多个线程要共享相同的内存地址空间，并且是并发运行，因此它们可能会访问或修改其他线程正在使用的变量，要使多线程程序的行为可以预测，必须对共享变量的访问操作进行协同，在Java中提供了各种同步机制来协同这种访问。 活跃性问题当某个操作无法继续执行下去时，就会发生活跃性问题。在串行程序中，活跃性问题的形式之一就是无意中造成的无限循环，从而使循环之后的代码无法得到执行。线程也会带来一些其他活跃性问题，例如死锁，饥饿，以及活锁。 性能问题活跃性意味着某件正确的事情始终会发生，但却不够好。线程带来性能问题就是线程调度带来的开销，还有线程使用共享数据必须使用同步机制，同步机制往往也会抑制编译器做某些优化等问题。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM12-线程安全与锁优化]]></title>
    <url>%2F2016%2F11%2F25%2FJVM12%2F</url>
    <content type="text"><![CDATA[线程安全当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象就是线程安全的。 Java语言中的线程安全讨论线程安全，就限定于多个线程之间存在共享数据访问这个前提，因为如果一段代码根本不会与其他线程共享数据，那么从线程安全的角度来看，程序是串行执行还是多线程执行对它来说是完全没有区别的。我们将Java语言中各种操作共享的数据分为以下5类：不可变、绝对线程安全、相对线程安全、线程兼容和线程独立。 不可变 在Java语言中，不可变的对象一定是线程安全的。基本数据类型采用final关键字修饰，如果是对象则需要保证自己的行为不会影响状态，例如String的replace()等方法都是产生新的对象。 绝对线程安全 绝对的线程安全就是前面的提到的定义，这个定义很严格，一个类要达到「不管运行时环境如何，调用者都不需要任何额外的同步措施」通常需要付出很大的，甚至有时候是不切实际的代价。在Java API中标注自己是线程安全的类，大多数都不是绝对的线程安全。 相对线程安全 相对的线程安全就是我们通常意义上所讲的线程安全，它需要保证对这个对象单独的操作是线程安全的，我们在调用的时候不需要做额外保障措施。在Java语言中，大部分的线程安全类都属于这种类型，例如Vector、HashTable等。 线程兼容 线程兼容是指对象本身并不是线程安全的，但是可以通过调用端正确地使用同步手段来保证对象在并发环境中可以安全的使用，我们平常说一个类不是线程安全的，绝大多数时候指的是这一种情况。 线程对立 线程对立是指无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码。一个线程对立的例子是Thread类的suspend()和resume()方法，如果并发进行的话，无论调用时是否进行了同步，目标线程都是存在死锁风险的，如果suspend()中断的线程就是即将要执行resume()的那个线程，那肯定就要产生死锁了。 线程安全的实现方法 互斥同步 互斥同步（Mutual Exclusion &amp; Synchronization）是常见的一种并发正确性保证手段。同步是指在多个线程并发访问共享数据时，保证共享数据在同一时刻只能被一个（或者是一些，使用信号量的时候）线程使用。而互斥是实现同步的一种手段，临界区（Critial Section）、互斥量（Mutex）和信号量（Semaphore）都是主要的互斥实现方式。因此，在这四个字里面，互斥是因，同步是果；互斥是方法，同步是目的。 在Java中，最基本的互斥同步手段就是synchronized关键字，synchronized关键字在经过编译之后，会在同步块的前后分别形成monitorenter和monitorexit这两个字节码指令。 根据虚拟机规范的要求，在执行monitorenter指令时，首先要尝试获取对象的锁，如果这个对象没有被锁定，或者当前线程已经拥有了那个对象的锁，把锁的计数器加1，相应的，在执行monitorexit指令时会将锁计数器减1，当计数器为0时，锁就被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。 关于monitorenter和monitorexit，有两点是要特别注意的： synchronized同步块对同一条线程来说是可重入的，不会出现把自己锁死的问题 同步块在已进入的线程执行完之前，会阻塞后面其它线程的进入 因为Java的线程是映射到操作系统的原生线程之上的，如果要阻塞或者唤醒一个线程，都需要操作系统来帮忙完成，这就需要从用户态转换到核心态中，因此状态转换需要耗费很多的处理器时间，对于代码简单的同步块，状态转换消耗的时间有可能比用户代码执行的时间还长，所以synchronized是Java语言中一个重量级（Heavyweight）锁，有经验的程序员都会在确实必要的情况下才使用这种操作。除了synchronized，还有java.util.concurrent包中的ReentrantLock来实现同步。 非阻塞同步 互斥同步最主要的问题就是进行线程阻塞和唤醒所带来的性能问题，因此这种同步也称为阻塞同步。互斥同步属于一种悲观的并发策略。随着硬件指令集（CAS指令）的发展，我们还可以采用基于冲突检测的乐观并发策略：先操作，没有其他线程竞争，就成功了；如果有其他线程争用，产生了冲突，就再采取补偿措施。 AtomicInteger的incrementAndGet方法就是无限循环自增直到成功。 锁优化自旋锁与自适应自旋互斥同步，对性能影响最大的是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核状态完成，这些操作给系统的并发性能带来了很大的压力。同时，虚拟机开发团队也注意到很多应用上，共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得。如果物理机上有一个以上的处理器，能让两个或两个以上的线程同时并行执行，我们就可以让后面请求锁的那个线程”稍等一下”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只需要让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁。 JDK1.4.2就已经引入了自旋锁，只不过默认是关闭的，在JDK1.6中就已经改为默认开启了。自旋不能代替阻塞，且先不说处理器数量的要求，自旋等待本身虽然避免了线程切换的开销，但是它是要占据处理器时间的，因此如果锁被占用的时间很短，自旋等待的效果就非常好；反之，如果锁被占用的时间很长，那么自旋的线程只会白白消耗处理器资源，而不会做任何有用的工作，反而会带来性能上的浪费。因此自旋等待必须有一定的限度，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程了，自旋次数的默认值是10，可以使用参数-XX:PreBlockSpin来更改。 在JDK1.6之后引入了自适应的自旋锁。自适应意味着自旋的时间不再固定了，而是由前一次在同一个锁上自旋的时间以及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而它将允许自旋等待持续相对更长的时间，比如100个循环。另外如果对于某一个锁，自旋很少成功获得过，那么在以后要获得这个锁时将可能忽略掉自旋过程，以避免浪费处理器资源。有了自适应自旋，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测就会越来越准确。 锁消除锁消除是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的支持，如果判断在一段代码中，堆上所有数据都不会逃逸出去从而被其他线程访问到，那就可以把它们当做栈上数据对待，认为它们是线程私有的，同步加锁自然无需进行。 锁粗化原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小—-只在共享数据的实际作用域中才进行同步，这样是为了使得需要同步的操作数尽可能变小，如果存在锁竞争，那等待锁的线程也能尽快拿到锁。 大部分情况下，上面的原则都是正确的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。 轻量级锁偏向锁]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM11-Java内存模型与线程]]></title>
    <url>%2F2016%2F11%2F25%2FJVM11%2F</url>
    <content type="text"><![CDATA[「内存模型」可以理解为在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象。 Java内存模型Java虚拟机规范中试图定义一种Java内存模型来屏蔽掉各种硬件和操作系统内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。在此之前，主流程序语言（如C/C++等）直接使用物理硬件和操作系统的内存模型，因此，会由于不同平台上内存模型的差异，有可能导致程序在一套平台上并发完全正常，而在另外一台平台上并发访问却经常出错，因此在某些场景就必须针对不同的平台来编写程序。 主内存与工作内存Java内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节。此处的变量与Java编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但不包括局部变量与方法参数，因为后者是线程私有的，不会被共享，自然不存在竞争问题。 Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的私有工作内存，线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。（这很好的诠释了volatile关键字的作用和原理） 内存间交互操作Java内存模型中定义了以下8种操作来完成，虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的 lock（锁定）：作用于主内存的变量，它把一个变量标识为一条线程独占的状态 unlock（解锁）：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，它把工作内存中的一个变量的值传送到主内存中，以便随后的write操作使用。 write（写入）：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。 如果要把变量从主内存复制到工作内存，那就要顺序执行read和load操作，如果要把变量从工作内存同步回主内存，就要顺序地执行store和write操作。Java内存模型规定了在执行上述8种基本操作时必须满足如下规则： 不允许read和load、store和write操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者从工作内存发起了回写但主内存不接受的情况出现。 不允许一个线程丢弃它最近的assign操作，即变量在工作内存中改变了之后必须把该变化同步回主内存。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存中。 一个新的变量只能在主内存中“诞生”，不允许在工作内存中字节使用一个未被初始化（load或assign的变量），换句话说就是对一个变量实施use、store操作之前，必须先执行过了assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。 如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值。 如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定住的变量。 对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）。 这8种内存访问操作以及上述规则的限定，再加上下面讲的对volatile的一些特殊规定，就已经完全确定了Java程序中哪些内存访问操作在并发下是安全的。 对于volatile型变量的特殊规则关键字volatile可以说是Java虚拟机提供的最轻量级的同步机制，我们需要正确的理解并使用它。 当一个变量定义为volatile之后，它将具备两种特性： 1.保证此变量对所有线程的可见性，这里的「可见性」是指当一条线程修改了这个变量的值，，新值对于其他线程来说是可以立即得知的。而普通变量做不到这一点，普通变量的值在线程间传递均需要通过主内存来完成，例如线程A修改一个普通变量的值，然后向主内存进行回写，另外一条线程B在线程A回写完成了之后再从主内存进行读取操作，新变量值才会对线程B可见。Java里面的运算并非原子操作，导致volatile变量在并发下一样是不安全的，看一段例子： 123456789101112131415161718192021222324252627282930313233343536373839/** * Created by YangFan on 2016/11/25 上午11:05. * &lt;p/&gt; * volatile变量自增预算测试 */public class VolatileTest &#123; public static volatile int race = 0; public static void increase() &#123; race++; &#125; private static final int THREADS_COUNT = 20; public static void main(String[] args) &#123; Thread[] threads = new Thread[THREADS_COUNT]; for (int i = 0; i &lt; THREADS_COUNT; i++) &#123; threads[i] = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int j = 0; j &lt; 10000; j++) &#123; increase(); &#125; &#125; &#125;); threads[i].start(); &#125; // 等待所有累加线程都结束 while (Thread.activeCount() &gt; 1) &#123; Thread.yield(); &#125; System.out.println(race); &#125;&#125; 这个结果每次可能都不一样，因为“race++”在虚拟机内部被分解成了很多指令，不同线程在自增的时候，这个值拿到的可能是过期的数据。 由于volatile变量值能保证可见性，在不符合以下两条规则的运算场景中，我们仍然要通过加锁来保证原子性。 运算结果并不依赖变量的当前值，或者能确保只有单一的线程修改变量的值 变量不需要与其他的状态变量共同参与不变约束 下面的场景就很适用： 1234567891011volatile boolean shutdownRequested;public void shutdown() &#123; shutdownRequested = true;&#125;public void doWork() &#123; while(!shutdownRequested) &#123; // do stuff &#125;&#125; 2.使用volatile变量的第二个语义是禁止指令重排序优化，普通变量仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。我们通过一个例子来看看为何指令重排序会干扰程序的并发执行。 指令重排序优化是指为了使得处理内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行乱序执行优化，处理器会再计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的，因此如果存在一个计算任务依赖另外一个计算任务的中间结果，那么其顺序性并不能靠代码的先后顺序来保证。 12345678910111213141516171819Map condigOptions;char[] configText;// 此变量必须定义为volatilevolatile boolean initialized = false;// 假设以下代码在线程A中执行// 模拟读取信息配置，当读取完成后将initialized设置为true以通知其他线程配置可用configOptions = new HashMap();configText = readConfigFile(fileName);processConfigOptions(configText, configOptions);initialized = true;// 假设以下代码在线程B中执行// 等待initialized为true，代表线程A已经把配置信息初始化完成while (!initialized) &#123; sleep();&#125;// 使用线程A中初始化好的配置信息doSomethingWihtConfig(); 上面的代码如果有定义initialized变量时没有使用volatile修饰，就可能会由于指令重排序的优化，导致位于线程A中最后一句的代码“initialized = true;”被提前执行(重排序优化是机器级的优化操作，提前执行是说这句话对应的汇编代码被提前)，这样在线程B中的代码可能就会出错。 对于long和double型变量的特殊规则因为double和long是64位数据，内存模型允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32为的操作来进行，即允许虚拟机实现选择可以不保证64位数据类型的load、store、read和write这4个操作的原子性。 如果有多个线程共享一个并未声明为volatile的long或double类型的变量，并且同时对它们进行读取和修改操作，那么某些线程可能会读取到一个既非原值，也不是其他线程修改的值代表了「半个变量」的数值。不过这很罕见，因为目前的商用虚拟机几乎都还是选择把64位数据的读写作为原子操作来对待，所以我们写代码一般也不需要对long和double变量专门声明为volatile。 原子性、可见性与有序性Java内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性这3个特征来建立的。 原子性 由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write，我们大致可以认为基本数据类型的访问读写是具备原子性的（例外就是long和double的非原子性协定，不过也无须太在意这几乎不会发生的例外情况。） 如果还需要更大范围的原子性保证，Java内存模型还提供了lock和unlock操作，也就是synchronized关键字。 可见性 可见性是指当一个线程修改了共享变量的值，其他线程能够立即得知这个修改。Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介来实现可见性的，无论是普通变量还是volatile变量都是如此，普通变量与volatile变量的区别是，volatile的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。因此，可以说volatile保证了多线程时操作变量的可见性，而普通变量则不能保证这一点。 synchronized和final关键字也能实现可见性，synchronized的可见性是由「对一个变量执行unlock操作之前，必须先把此变量同步回主内存中」这条规则获得的。另外，final关键字也可以实现可见性，因为被final修饰的字段在构造器中一旦初始化完成，并且构造器没有把this传递出去，那在其他线程中就能看见final字段的值。 有序性 Java内存模型的有序性可以总结为一句话，如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指「线程内表现为串行的语义」，后半句是指「指令重排序」现象和「工作内存与主内存同步延迟」现象。 Java语言提供了volatile和synchronized两个关键字来保证线程之间操作的有序性，volatile关键字本身就包含了禁止指令重排序的语义，而synchronized则是由”一个变量在同一时刻只允许一条线程对其进行lock操作”这条规则获得的，这条规则规定了持有同一个锁的两个同步块只能串行地进入。 先行发生原则如果Java内存模型中所有的有序性都仅仅靠volatile和synchronized来完成，那么好像有一些操作将会变得很繁琐，但是我们在编写Java并发代码的时候并没有感觉到这一点，这是因为Java语言中有一个「先行发生」的原则。这个原则非常重要，它是判断数据是否存在竞争、线程是否安全的主要依据，依靠这个原则，我们可以通过几条规则一揽子地解决并发环境下两个操作之间是否可能存在冲突的所有问题。 先行发生是Java内存模型中定义的两项操作之间的偏序关系，如果说操作A先行发生于B，其实就是说发生在操作B之前，操作A产生的影响能被B观察到，「影响」包括修改了内存中共享变量的值、发送了消息、调用了方法等。 Java内存模型有有些天然的先行发生关系，这些先行发生关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来的话，它们就没有顺序行保障，虚拟机可以对它们随意地进行重排序。 程序次序规则：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说，应该是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。 管程锁定规则：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，这里「后面」是指时间上的先后顺序。 volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里「后面」同样是指时间上的先后顺序。 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作。 线程终止规则：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值等手短检测到线程已经终止执行。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测到是否有中断发生。 对象终结规则：一个对象的初始化完成（构造函数执行结束）先行发生于它的finalize()方法的开始。 传递性：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。 Java语言无须任何同步手段保障就能成立的先行发生规则就只有上面这些。下面演示如何用这些规则来判定操作间是否具备顺序性 1234567891011private int value = 0;public void setValue(int value)&#123; this.value = value;&#125;public int getValue()&#123; return value;&#125; 这是很普通的getter/setter方法，假设存在线程A和B，线程A先(时间上的先后)调用了“setValue(1)”，然后线程B调用了同一个对象的“getValue()”，那么线程B的返回值是什么？我们根据规则来分析一下：由于两个方法分别由线程A和线程B调用，不在一个线程中，所以程序次序规则不适用；由于没有同步块，就没有lock和unlock操作，所以管程锁定规则不适用；由于value变量没有被volatile关键字修饰，所以volatile变量规则不适用；后面的线程启动、终止、中断规则和对象终结规则也和这里完全没有关系。因为没有一个使用的先行发生规则，所以最后一条传递性也无从谈起。因此可以判定尽管线程A在操作时间上先于线程B，但是无法确定线程B中的“getValue()”方法的返回结果，换句话说，这里面的操作不是线程安全的 那如何修复这个问题？至少有两种比较简单的方案： setter/getter都定义成synchronized的，这样可以套用管程锁定规则 value定义为volatile变量，由于setter方法对value的修改不依赖于value的原值，满足volatile关键字的使用场景，这样可以套用volatile变量规则 我们也得出一个结论：时间先后顺序与先行发生原则之间基本没有太大的关系，所以我们衡量并发安全问题的时候不要受到时间顺序的干扰，一切必须以先行发生原则为准。 Java与线程线程的实现Java的线程API基本都是Native方法，意味着这个方法没有使用或无法使用平台无关的手段来实现。实现线程有3种方式：使用内核线程实现、使用用户线程实现和使用用户线程加轻量级进程混合实现。 使用内核线程实现 内核线程就是直接由操作系统支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器对线程进行调度，并负责将线程的任务映射到各个处理器上。不过程序一般不会直接去使用内核线程，而是使用内核线程的一种高级接口–轻量级进程。这种方式系统调用代价较高，并且因为消耗内核资源，所以轻量级进程数量有限。轻量级进程与内核线程之间是1：1的关系。 使用用户线程实现 从广义上来讲，一个线程只要不是内核线程，就可以认为是用户线程。而狭义上的用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知线程存在的实现。用户线程的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。这种方式的操作可以是非常快速且低消耗的，劣势在于没有系统内核支持，实现起来非常的复杂。Java和Ruby都曾经使用过用户线程，最终又都放弃使用它。进程与用户线程之间是1：N的关系。 使用用户线程加轻量级进程混合实现 线程除了依赖内核线程实现和完全由用户程序自己实现之外，还有一种将内核线程与用户线程一起使用的实现方式。在这种混合实现下，既存在用户线程，也存在轻量级进程。因此用户线程的创建、切换、析构等操作依然廉价，并且可以支持大规模的用户线程并发。而轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级进程来完成，大大降低了整个进城被完全阻塞的风险。用户线程与轻量级进程之间是N：M的关系。 Java线程的实现 1.2之前是用户线程实现的，1.2开始替换为基于操作系统原生线程模型来实现。因此在目前的JDK版本中，操作系统支持怎样的线程模型，很大程度上决定了Java虚拟机的线程是怎样映射的。对Sun JDK来说，它的Windows版和linux版都是使用一对一的线程模型实现的，一条Java线程就映射到一条轻量级进程之中，因为windows和Linux系统提供的线程模型就是一对一的。 Java线程调度线程调度是指系统为线程分配处理器使用权的过程，主要调度方式有两种，分别是协同式线程调度和抢占式线程调度。 协同式调度，线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知切换到另外一个线程上。好处是实现简单，干完自己的事情后进行线程切换，没有什么同步问题，坏处是一旦程序出问题，将会阻塞下去。 抢占式调度的多线程系统，每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。这种实现线程调度的方式下，线程的执行时间是系统可控的，不会出现什么阻塞问题。 线程状态线程有5种状态 新建：创建后尚未启动的线程处于这种状态 运行：Runable包括了操作系统线程状态中的Running和Ready，也就是处于此状态的线程有可能正在执行，也有可能正在等待着CPU为他分配执行时间。 无限期等待：处于这种状态的线程不宜被分配CPU执行时间，她们要等待被其他线程显式的唤醒。以下方法会让线程陷入无限期的等待状态： 没有设置Timeout参数的Object.wait()方法 没有设置Timeout参数的Thread.join()方法 LockSupport.park()方法 限期等待：处于这种状态的线程也不会被分配CPU执行时间，不过无须等待被其他线程显式地唤醒，在已定时间之后它们会由系统自动唤醒。以下方法会让线程进入限期等待状态： Thread.sleep()方法。 设置了Timeout参数的Object.wait()方法。 设置了Timeout参数的Thread.join()方法。 LockSupport.parkNanos()方法。 LockSupport.parkUntil()方法 阻塞：线程被阻塞了，「阻塞状态」与「等待状态」的区别是：「阻塞状态」在等待着获取一个到一个排他锁，这个时间将在另外一个线程放弃这个锁的时候发生；而「等待状态」则是等待一段时间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。 结束：已终止线程的线程状态，线程已经结束执行。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM10-运行期优化]]></title>
    <url>%2F2016%2F11%2F17%2FJVM10%2F</url>
    <content type="text"><![CDATA[Java程序最初是通过解释器进行解释执行的，当虚拟机发现某个方法或代码块的运行特别频繁时，就会把这些代码认定为『热点代码』。为了提高热点代码的执行效率，在运行时，虚拟机将会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化，完成这个任务的编译器称为即时编译器。 HotSpot虚拟机内的即时编译器解释器与编译器HotSpot虚拟机是采用解释器与编译器并存的架构。解释器和编译器各有优势：当程序需要迅速启动和执行的时候，解释器可以首先发挥作用，省去编译的时间，立即执行。在程序运行后，随着时间的推移，编译器逐渐发挥作用，把越来越多的代码编译成本地代码之后，可以获取更高的执行效率。 编译对象与触发条件在运行过程中被即时编译器编译的『热点代码』有两类： 被多次调用的方法 被多次执行的循环体 前者很好理解，一个方法被调用得多了，方法体内代码执行的次数自然就多，他成为”热点代码”也是理所当然。而后者则是为了解决一个方法只被调用过一次或者少量的几次，但是方法体内部存在循环次数较多的循环体问题，这样循环体的代码也被重复执行多次，因此这些代码也应该认为是”热点代码”。 判断一段代码是不是热点代码，是不是需要触发即时编译，这样的行为称为热点探测，目前主要有两种方式： 基于采样的热点探测 基于计数器的热点探测 HotSpot是使用的第二种，基于计数器的热点探测方法，因此它为每个方法准备了两类计数器：方法调用计数器和回边计数器。在确定虚拟机运行参数的前提下，这两个计数器都有一个确定的阀值，超过这个阀值，就会触发JIT编译。 方法调用计数器这个计数器就是统计方法被调用的次数，默认阀值在Client模式下是1500次，在Server模式下是10000次，这个阀值可以通过虚拟机参数-XX:CompileThreshold来设置。当一个方法被调用时，会检查方法是否存在被JIT编译过的版本，如果存在，则优先使用编译后的本地代码来执行。如果不存在已被编译过的版本，则将此方法的调用计数器值加1，然后判断方法调用计数器和回边计数器值之和是否超过方法调用计数器的阈值。如果已经超过阈值，那么将会向即时编译器提交一个该方法的代码编译请求。 如果不做任何设置，执行引擎并不会同步等待编译请求完成，而是直接进入解释器按照解释方法执行字节码，直到提交的请求被编译器编译完成。当编译工作完成之后，这个方法的调用入口地址就会被系统自动改写成新的，下一次调用该方法时就会使用已编译的版本。 如果不做任何设置，方法调用计数器统计的并不是方法被调用的绝对次数，而是一个相对的执行频率，即一段时间之内方法被调用的次数。当超过一定的时间限度，如果方法的调用次数仍然不足以让它提交给即时编译器编译，那这个方法的调用计数器就会少一半，这个过程称为方法的调用计数器热度的衰减，而这段时间就称为此方法统计的半衰周期。进行热度衰减的动作是在虚拟机进行垃圾回收时顺便进行的，可以使用虚拟机参数-XX:-UseCounterDecay来关闭热度衰减，让方法计数器统计方法调用的绝对次数，这样，只要系统运行时间足够长，绝大部分方法都会被编译成本地代码。另外，可以使用-XX:CounterHalfLifeTime参数设置半衰周期的时间，单位是秒。 回边计数器它的作用是统计一个方法中循环体代码执行的次数，在字节码中遇到控制流向后跳转的指令称为”回边”。显然，建立回边计数器统计的目的就是为了触发OSR编译。 关于回边计数器的阈值，虽然HotSpot也提供了一个类似于方法调用计数器阈值-XX:CompileThreshold的参数-XX:BackEdgeThreshold供用户设置，但是当前虚拟机实际上并未使用此参数，因此我们需要设置另外一个参数-XX:OnStackReplacePercentage来间接调整回边计数器的阈值，其计算公式如下： Client模式 方法调用计数器阈值 × OSR比率 / 1000，其中OSR比率默认值933，如果都取默认值，Client模式下回边计数器的阈值应该是13995 Server模式 方法调用计数器阈值 × (OSR比率 - 解释器监控比率) / 100，其中OSR比率默认140，解释器监控比率默认33，如果都取默认值，Server模式下回边计数器阈值应该是10700 当解释器遇到一条回边指令时，会先查找将要执行的代码片段中是否有已经编译好的版本，如果有，它将会优先执行已编译好的代码，否则就把回边计时器的值加1，然后判断方法调用计数器与回边计数器值之和是否已经超过回边计数器的阈值。当超过阈值之后，将会提交一个OSR编译请求，并且把回边计数器的值降低一些，以便继续在解释器中执行循环，等待编译器输出编译结果。 与方法计数器不同，回边计数器没有热度衰减的过程，因此这个计数器统计的就是该方法循环执行的绝对次数。当计数器溢出的时候，它还会把方法计数器的值也调整到溢出状态，这样下次再进入该方法的时候就会执行标准编译过程。 编译过程在默认设置下，无论是方法调用产生的即时编译请求，还是OSR编译请求，虚拟机在代码编译器还未完成的时候，都仍然按照解释方式继续执行，而编译动作则在后台的编译线程中进行。用户可以通过-XX:-BackgroundCompilation来禁止后台编译，在禁止后台编译后，一旦达到JIT的编译条件，执行线程向虚拟机提交编译请求后将会一直等待，直到编译过程完成后再开始执行编译器输出的本地代码。 对于Client Compiler（C1编译器）来说，它是一个简单快速的三段式编译，主要关注点在于局部性的优化，而放弃了许多耗时间长的全局优化手段。 对于Sever Compiler（C2编译器）来说，它则是专门面向服务端的典型应用并为服务端的性能配置特别调整过的编译器，也是一个充分优化过的高级编译器，几乎能达到GNU C++编译器使用-O2参数时的优化强度，它会执行所有经典的优化动作，如无用代码消除、循环展开、常量传播、基本块重排序等，还会实施一些与Java语言特性密切相关的优化技术，如范围检查消除、空值检查消除等，另外，还有可能根据解释器或Client Compiler提供的性能监控信息，进行一些不稳定的激进优化，如守护内联、分支频率预测等，下一部分将讲解上述的一部分优化手段。 Server Compiler从即时编译的标准来看，无疑是比较缓慢的，但它的编译速度依然远远超过传统的静态优化编译器，而且它相对于Client Compiler编译输出的代码质量有所提高，可以减少本地代码的执行时间，从而抵消了额外的编译时间开销，所以也有很多非服务端的应用选择使用Server模式的虚拟机运行。 优化技术概览在Sun官方的Wiki上，HotSpot虚拟机设计团队列出了一个相对比较全面、在即时编译器中采用的优化技术列表，其中有不少经典编译器的优化手段，也有许多针对Java语言（准确地说是运行在Java虚拟机上得所有语言）本身进行的优化技术，下面主要看几项最有代表性的优化技术： 语言无关的经典优化技术之一：公共子表达式消除 语言无关的经典优化技术之一：数组范围检查消除 最重要的优化技术之一：方法内联 最前沿的优化技术之一：逃逸分析 公共子表达式消除公共子表达式消除的含义是：如果一个表达式E已经计算过了，并且从先前的计算到现在E中所有变量的值都没有发生变化，那么E的这次出现就成了公共子表达式。对于这种表达式，没有必要花时间再对它进行计算，只需要直接用前面的计算结果代替E就可以了。如果这种优化仅限于程序的基本块内，便称为局部公共子表达式消除，如果这种优化的范围涵盖了多个基本块，那就称为全局公共子表达式消除。下面举例说明： int d = (c * b) * 12 + a + (a + b * c); 如果这段代码交给javac编译器则不会进行任何优化，但进入虚拟机即时编译器后，它将会进行如下优化：编译器检测到“c*b”与“b*c”是一样的表达式，而且在计算期间b与c的值是不会变的。因此这条表达式就可能被视为： int d = E * 12 + a + (a + E) 还有可能进行代数简化： int d = E * 13 + a * 2 表达式进行交换之后，再计算起来就可以节省一些时间了。 数组边界检查消除我们知道Java语言是一门动态安全的语言，对数组的读写访问也不像C、C++那样在本质上是裸指针操作。如果有一个数组foo[]，在Java语言中访问数组元素foo[i]的时候系统将会自动进行上下界的范围检查，即检查i必须满足i&gt;=0&amp;&amp;i&lt;foo.length这个条件，否则将抛出一个运行时异常：java.lang.ArrayIndexOutOfBoundsException。这对软件开发者来说是一件很好的事情，即使程序员没有专门编写防御代码，也可以避免大部分的溢出攻击。但是对于虚拟机的执行子系统来说，每次数组元素的读写都带有一次隐含的判定条件，对于拥有大量数组访问的程序代码，这无疑也是一种性能负担。 无论如何，为了安全，数组边界检查肯定是必须做的，但数组边界检查是不是在运行时每次都做则不一定。例如：数组下标是一个常量，如foo[3]，只要在编译期根据数据流分析来确定foo.length的值，并判断下表“3”没有越界，执行的时候就无须判断了。更加常见的情况是数组访问发生在循环之中，如果编译器只要通过数据流分析就可以判定循环变量的取值范围永远在区间[0,foo.length])内，那在整个循环中就可以把数组的上下界检查消除，这可以节省很多次的条件判断操作。 方法内联它是编译器最重要的优化手段之一，除了消除方法调用的成本之外，它更重要的意义是为其他优化手段建立良好的基础。 12345678910public static void foo(Object obj)&#123; if(obj!=null) &#123; System.out.println("do something"); &#125;&#125;public static void testInline(String[] args) &#123; Object = null; foo(obj);&#125; 采用方法内联后大致成这样 123456public static void testInline(String[] args) &#123; Object = null; if(obj!=null) &#123; System.out.println("do something"); &#125;&#125; 因为有了方法内联的基础，这段代码还能被继续优化，删除不可能被执行的dead code。只有非虚方法才能直接内联，虚方法需要运行时确定调用目标，所以虚拟机还有一套“类型继承关系分析”的技术来确定目前已加载的类中，某个接口是否有多余一种的实现，某个类是否存在子类、子类是否为抽象类等信息。 逃逸分析逃逸分析是目前Java虚拟机中比较前沿的优化技术，他与类型继承关系分析一样，并不是直接优化代码的手段，而是为其他优化手段提供依据的分析技术。逃逸分析的基本行为就是分析对象动态作用域。 当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，称为方法逃逸。甚至还有可能被外部线程访问到，譬如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸。 如果能证明一个对象不会逃逸到方法或线程之外，也就是别的方法或线程无法通过任何途径访问到这个对象，则可能为这个变量进行一些高校的优化。如下： 栈上分配：Java虚拟机中，对象在堆上分配，Java堆中的对象对于各个线程都是共享可见的。虚拟机的垃圾收集系统可以回收堆中不再使用的对象，但回收动作无论是筛选可回收对象还是回收和整理内存都要耗费时间。如果确定一个对象不会逃逸出方法之外，那么让这个对象在栈上分配将会是一个不错的主意，对象所占用的内存空间就可以随着栈帧出栈而销毁，这样垃圾收集系统的压力将会小很多。 同步消除：线程同步本身是一个相对耗时的过程，如果逃逸分析能够确定一个变量不会逃逸出线程，无法被其他线程访问，那这个变量的读写肯定就不会有竞争，对这个变量实施的同步措施也就可以消除掉。 标量替换：标量是指一个数据已经无法再分解成更小的数据来表示了，Java虚拟机中的原始数据类型(int、long等)都不能进一步分解，它们就可以称为标量。相对的，如果一个数据可以继续分解那它就称作聚合量，Java中的对象就是最典型的聚合量。如果把一个Java对象拆散，根据程序访问的情况，将其使用到的成员变量恢复原始类型来访问就叫做标量替换。如果逃逸分析证明一个对象不会被外部访问，并且这个对象可以被拆散的话，那程序真正执行的时候将可能不创建这个对象，而改为直接创建它的若干个被这个方法使用到的成员变量来代替。将对象拆分后除了可以让对象的成员变量在栈上分配和读写之外，还可以为后续进一步的优化手段创建条件。 关于逃逸分析的论文在1999年就已经发表，但直到Sun SDK1.6才实现了逃逸分析，而且直到现在这项优化尚未足够成熟，仍有很大的改进余地。不成熟的原因主要是不能保证逃逸分析的性能收益必定高于它的消耗。虽然在实际测试结果中，实施逃逸分析后的程序往往能运行出不错的成绩，但是在实际的应用程序，尤其是大型程序中反而发现实施逃逸分析可能出现效果不稳定的情况，或因分析过程耗时但却无法有效判别出非逃逸对象而导致性能有所下降。 如果有需要，并且确认对程序运行有益，可以使用参数-XX:+DoEscapeAnalysis来手动开启逃逸分析，开启之后可以通过参数-XX:+PrintEscapeAnalysis来查看分析结果。有了逃逸分析支持之后，就可以使用参数-XX:+EliminateAllocations来开启标量替换，使用参数-XX:+EliminatLocks来开启同步消除，使用参数-XX:+PrintEliminateAllocations查看标量的替换情况。 尽管目前逃逸分析技术仍不是十分成熟，但是在今后的虚拟机中，逃逸分析技术肯定会支撑起一系列实用有效的优化技术。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM9-虚拟机字节码执行引擎]]></title>
    <url>%2F2016%2F11%2F15%2FJVM9%2F</url>
    <content type="text"><![CDATA[执行引擎是Java虚拟机最核心的组成部分之一，本章将主要从概念模型的角度来讲解虚拟机的方法调用和字节码执行。 运行时栈帧结构栈帧（Stack Frame）是用于支持虚拟机进行方法代用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素。栈帧存储了局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用开始至执行完成的过程，都对应着一个栈帧在虚拟机里面从入栈到出栈的过程。对于执行引擎来说，在活动线程中，只有位于栈顶的栈帧才是有效的，称为当前栈帧，与这个栈帧相关联的方法称为当前方法，执行引擎运行的所有字节码指令都只针对当前栈帧进行操作，在概念模型上，典型的栈帧结构图如下： 局部变量表局部变量表是一组变量值存储空间，用于存放参数和方法内部定义的局部变量。局部变量表的容量以变量槽（Slot）为最小单位。虚拟机规范中没有明确指明一个Slot应占用的内存空间大小，只是向导性的说到每个Slot都应该能存放一个boolean、byte、char、short、int、float、reference和returnAddress。reference表示对一个对象实例的引用，returnAddress目前很少见了。一个Slot可以存放一个32位以内的数据，那么64位的long和double会被分配两个连续的Slot空间。实例方法第0位索引的Slot默认是用于传递方法所属对象实例的引用（this），然后从1开始是方法参数，参数表分配完后再是方法体内部的变量。前面提到过，类变量在准备阶段会赋予系统初始值，初始化阶段赋予程序员定义的初始值，所以就算没有设值也会有一个默认值，但局部变量则不一样，没有设值变进行使用的话，编译无法通过。 1234public static void main(String[] args) &#123; int a; System.out.println(a);&#125; 操作数栈操作数栈也常称为操作栈，它是一个后入先出栈。操作数栈的每一个元素可以是任意的Java数据类型，包括long和double。32位的数据类型所占的栈容量为1，64为数据类型所占的栈容量为2。当一个方法刚刚开始执行的时候，这个方法的操作数栈是空的，在方法的执行过程中，会有各种字节码指令往操作数栈中写入和提取内容，也就是出栈/入栈操作。举个例子，整数加法的字节码指令iadd在运行的时候操作数栈中最接近栈顶的两个元素已经存入了两个int类型的数值，当执行这个指令时，会将这两个int值出栈并相加，然后将加的结果入栈。 动态连接每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接。我们知道Class文件的常量池中存有大量的符号引用，字节码中的方法调用指令就以常量池中指向方法的符号引用作为参数。这些符号引用会在类加载阶段或者第一次使用的时候就转化为直接引用，这种转化称为静态解析。另外一部分将在每一次运行期间转化为直接引用，这部分称为动态连接。 方法返回地址当一个方法开始执行后，只有两种方式退出方法，要么遇到方法返回的字节码指令，要么是在方法执行过程中遇到了异常。无论哪种退出方式，在方法退出后，都需要返回到方法被调用的位置，程序才能继续执行，方法返回时可能需要在栈帧中保存一些信息，用来帮助恢复它的上层方法的执行状态。 方法调用方法调用并不等同于方法执行，方法调用阶段唯一的任务就是确定被调用方法的版本（即调用哪一个方法），暂时还不涉及方法内部的具体运行过程。 解析所有方法调用中的目标方法在Class文件里面都是一个常量池中的符号引用，在类加载解析阶段，会将其中一部分符号引用转化为直接引用，这个前提是调用目标在程序代码写好、编译器进行编译时必须确定下来。这类方法的调用称为解析（Resolution）。 在Java语言符合“编译期可知，运行期不可变”的方法主要包括静态方法和私有方法。 调用方法的虚拟机字节码指令： invokestatic：调用静态方法 invokespecial：调用实例构造器方法、私有方法和父类方法 invokevirtual：调用所有的虚方法 invokeinterface：调用接口方法，会在运行时再确定一个实现此接口的对象 invokedynamic：先在运行时动态解析出调用点限定符所引用的方法，然后再执行该方法，在此之前的4条调用指令，分派逻辑是固化在Java虚拟机内部的，而invokedynamic指令的分派逻辑是由用户所设定的引导方法决定的。 能被invokestatic和invokespecial指令调用的方法，能在解析阶段把符号引用转化为直接引用，这些方法称为非虚方法，其他方法称为虚方法（final除外）。被final修饰的虽然是用invokevirtual调用的，但是它是一个非虚方法。 分派分派调用可能是静态的也可能是动态的，又可分为单分派和多分派。 静态分派先上一段代码 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Created by YangFan on 2016/11/16 上午11:14. * &lt;p/&gt; * 静态分派演示 */public class StaticDispatch &#123; static abstract class Human &#123; &#125; static class Man extends Human &#123; &#125; static class Woman extends Human &#123; &#125; public void sayHello(Human guy) &#123; System.out.println("hello, guy"); &#125; public void sayHello(Woman guy) &#123; System.out.println("hello, lady"); &#125; public void sayHello(Man guy) &#123; System.out.println("hello, gentleman"); &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); StaticDispatch sd = new StaticDispatch(); sd.sayHello(man); sd.sayHello(woman); &#125;&#125; 运行结果： 12hello, guyhello, guy 很简单，下面从虚拟机的角度来讲解一下。 Human man = new Man(); 上面的“Human”称为变量的静态类型（Static Type），后面的“Man”称为变量的实际类型（Actual Type）。静态类型在编译期是可知的，实际类型变化的结果在运行期才可确定。虚拟机（编译器）在重载时是通过参数的静态类型而不是实际类型作为判定依据的，所以选了sayHello(Human)作为调用目标，并把这个方法的符号引用写到main()方法里的两条invokevirtual指令的参数中。 所有依赖静态类型来定位方法执行版本的分派动作称为静态分派。 动态分派动态分派和多态的重写有着密切的关联。 12345678910111213141516171819202122232425262728293031323334/** * Created by YangFan on 2016/11/16 下午3:25. * &lt;p/&gt; * 方法动态分派演示 */public class DynamicDispatch &#123; static abstract class Human &#123; protected abstract void sayHello(); &#125; static class Man extends Human &#123; @Override protected void sayHello() &#123; System.out.println("man say hello"); &#125; &#125; static class Woman extends Human &#123; @Override protected void sayHello() &#123; System.out.println("woman say hello"); &#125; &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); man.sayHello(); woman.sayHello(); man = new Woman(); man.sayHello(); &#125;&#125; 运行结果： 123man say hellowoman say hellowoman say hello 我们用javap -c 命令看看输出结果 123456789101112131415161718192021public static void main(java.lang.String[]); Code: 0: new #2 // class polymorphic/DynamicDispatch$Man 3: dup 4: invokespecial #3 // Method polymorphic/DynamicDispatch$Man.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: new #4 // class polymorphic/DynamicDispatch$Woman 11: dup 12: invokespecial #5 // Method polymorphic/DynamicDispatch$Woman.&quot;&lt;init&gt;&quot;:()V 15: astore_2 16: aload_1 17: invokevirtual #6 // Method polymorphic/DynamicDispatch$Human.sayHello:()V 20: aload_2 21: invokevirtual #6 // Method polymorphic/DynamicDispatch$Human.sayHello:()V 24: new #4 // class polymorphic/DynamicDispatch$Woman 27: dup 28: invokespecial #5 // Method polymorphic/DynamicDispatch$Woman.&quot;&lt;init&gt;&quot;:()V 31: astore_1 32: aload_1 33: invokevirtual #6 // Method polymorphic/DynamicDispatch$Human.sayHello:()V 36: return 0~15行的字节码是准备动作，作用是建立man和woman的内存空间、调用Man和Woman类型的实例构造器，将这两个实例的引用存放在第1、2个局部变量Slot之中，这个动作对应了这两句代码。 12Human man = new Man();Human woman = new Woman(); 然后16~21行是关键。16: aload_1和20: aload_2两句分别将两个对象压入栈顶，17和21的invokevirtual就是调用方法指令，后面的是参数（方法的符号引用）。那么多态如何确定执行的目标方法，下面说一下invokevirtual指令的运行时解析过程： 找到操作数栈的第一个元素所指向的对象的实际类型，记作C。 如果在类型C中找到与常量中的描述符和简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找过程结束；如果不通过，则返回java.lang.IllegalAccessError异常。 否则，按照继承关系从下往上一次对C的各个父类进行第2步的搜索和验证过程。 如果始终没有找到合适的方法，则抛出java.lang.AbstracMethodError异常。 由于invokevirtual指令执行的第一步就是在运行期确定接收者的实际类型，所以两次调用中的invokevirtual指令把常量池中的类方法符号引用解析到了不同的直接引用上，这个过程就是Java语言中方法重写的本质。我们把这种在运行期根据实际类型确定方法执行版本的过程称为动态分派。 单分派与多分派方法的接收者与方法的参数统称为方法的宗量。根据分派基于多少种宗量，可以将分派划分为单分派和多分派两种。单分派是根据一个宗量对目标方法进行选择，多分派则是根据多于一个宗量对目标方法进行选择。看代码1234567891011121314151617181920212223242526272829303132333435363738394041/** * Created by YangFan on 2016/11/16 下午5:07. * &lt;p/&gt; * 单分派、多分派演示 */public class Dispatch &#123; static class QQ &#123;&#125; static class _360 &#123;&#125; public static class Father &#123; public void hardChoice(QQ arg) &#123; System.out.println("father choose qq"); &#125; public void hardChoice(_360 arg) &#123; System.out.println("father choose 360"); &#125; &#125; public static class Son extends Father &#123; public void hardChoice(QQ arg) &#123; System.out.println("son choose qq"); &#125; public void hardChoice(_360 arg) &#123; System.out.println("son choose 360"); &#125; &#125; public static void main(String[] args) &#123; Father father = new Father(); Father son = new Son(); father.hardChoice(new _360()); son.hardChoice(new QQ()); &#125;&#125; 运行结果： 12father choose 360son choose qq 编译阶段，也就是静态分派的过程，先确定静态类型是Father还是Son，再确定参数是QQ还是360，因为是根据两个宗量进行选择，所以Java语言的静态分派是多分派类型。 运行阶段，也就是动态分派的过程，在执行“son.hardChoice(new QQ());”对应的invokevirtual指令时，由于编译期已决定目标方法的签名必须为hardChoice(QQ),所以只需要确定方法接收者的实际类型是Father还是Son。因为只有一个宗量作为选择依据，所以Java语言的动态分派属于单分派类型。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM8-类加载机制]]></title>
    <url>%2F2016%2F11%2F07%2FJVM8%2F</url>
    <content type="text"><![CDATA[类加载机制虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 类加载的时机类被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载、验证、准备、解析、初始化、使用、卸载7个阶段。其中验证、准备、解析3个部分统称为连接，这个阶段的发生顺序如下图所示： 图中加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（也称为动态绑定或晚期绑定）。 虚拟机规定了5种主动引用必然会触发初始化（加载、验证、准备在此之前开始） 遇到new、getstatic、putstatic或invokestatic这4条字节码指令时。这4条指令的Java代码场景是：使用new关键字实例化对象，读取或设置一个类的静态字段（被final修饰，已在编译期放入常量池的除外），调用一个类的静态方法。 使用java.lang.reflect反射调用的时候，如果类还没有初始化，则需要先初始化 父类未初始化，则先初始化一个类的父类 虚拟机启动的时候会先初始化执行的主类（包含main的类） 当使用JDK1.7的动态语言支持时，如果一个java.lang.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化。 除了这些主动引用，其他引用类都不会触发初始化，称为被动引用。例如通过子类引用父类的静态字段，不会触发子类初始化。 类加载的过程下面分别介绍一下加载、验证、准备、解析和初始化这5个阶段所执行的具体动作。 加载“加载”是“类加载”过程的一个阶段，在加载阶段，虚拟机需要完成3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口 验证验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 文件格式验证 第一阶段要验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理，验证通过后，才会存入方法区。例如 是否以魔数0xCAFEBABE开头 主、次版本号是否在当前虚拟机处理范围之内 常量池的常量中是否有不被支持的常量类型 元数据验证 第二阶段是对字节码描述的信息进行语义分析，以保证其描述的信息符合Java语言规范，例如 是否有父类 是否继承了不允许继承的类(被final修饰的类) 如果这个类不是抽象类，是否实现了父类或接口中要求实现的方法 字节码验证 第三阶段目的是通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。例如：在操作栈上放置了一个int类型的数据，使用时却按long类型来加载入本地变量表中。 符号引用验证 最后一个阶段的校验发生再虚拟机将符号引用转换为直接引用的时候，这个转化动作将在连接的第三个阶段-解析阶段发生。符号引用验证可以看做是对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验，例如符号引用中通过字符串描述的全限定名是否能找到对应的类，在指定类中是否存在符合方法的字段描述以及简单名称所描述的方法和字段等等。 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中分配。这个时候进行内存分配仅包括类变量（static），而不包括实例变量。注意初始值是指分配零值 public static int value = 123; 变量value在准备阶段过后的初始值是0而不是123，因为这时候尚未开始执行任何Java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器()方法之中，所以把value赋值为123的动作将在初始化阶段才会执行。加了final的常量除外，这个放在方法区常量池中的数据将会在准备阶段被赋值 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。符号引用就是那些我们用javap命令看到的Methodref，Fieldref一类的。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 初始化类初始化阶段是类加载过程的最后一步，前面的类加载过程中，除了在加载阶段用户应用程序可以通过自定义类加载器之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码。在准备阶段，变量已经赋过一次系统要求的初始值，而在初始化阶段，则根据程序员通过程序制定的主观计划去初始化类变量和其他资源。初始化阶段就是执行类构造器&lt;clinit&gt;()方法的过程。&lt;clinit&gt;()方法就是由编译器收集类中所有的类变量的赋值动作和静态语句（static{}块）。 类加载器虚拟机设计团队把类加载阶段中的“通过一个类的全限定名来获取描述此类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需要的类。实现这个动作的代码模块称为“类加载器”。类加载器虽然只用于实现类的加载动作，但它在Java程序中起到的作用却远远不限于类加载阶段。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其再Java虚拟机中的唯一性。意思就是比较两个类相等的前提是这两个类由同一个类加载器加载。 123456789101112131415161718192021222324252627282930313233343536/** * Created by YangFan on 2016/11/15 下午3:37. * &lt;p/&gt; * 相等是指类的.class对象的equals()方法、isAssignableFrom()方法、isInstance()方法的返回结果，也包括使用instanceof关键字做对象所属关系判定等情况。 */public class ClassLoaderTest &#123; public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; ClassLoader myLoader = new ClassLoader() &#123; @Override public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; try &#123; String fileName = name.substring(name.lastIndexOf(".") + 1) + ".class"; InputStream is = getClass().getResourceAsStream(fileName); if (is == null) &#123; return super.loadClass(name); &#125; byte[] b = new byte[is.available()]; is.read(b); return defineClass(name, b, 0, b.length); &#125; catch (IOException e) &#123; throw new ClassNotFoundException(); &#125; &#125; &#125;; Object obj = myLoader.loadClass("clazzloader.ClassLoaderTest").newInstance(); System.out.println(obj.getClass()); System.out.println(obj instanceof ClassLoader); &#125;&#125; 运行结果： 12class clazzloader.ClassLoaderTestfalse 双亲委派模型（重要）从Java虚拟机的角度来将，只存在两种不同的类加载器：一种是启动类加载器（Bootstrap ClassLoader），这个类加载器使用C++语言实现，是虚拟机自身的一部分；另一种就是其他的类加载器，这些类加载器由Java语言实现，独立于虚拟机外部，并且全都继承自抽象类java.lang.ClassLoader。从开发人员角度来看，还可以划分得更细致一些： 启动类加载器前面介绍过，它负责加载的是JAVA_HOME/lib下的类库，系统类加载器无法被Java程序直接应用。 扩展类加载器这个类加载器由sun.misc.Launcher$ExtClassLoader实现，它负责用于加载JAVA_HOME/lib/ext目录中的，或者被java.ext.dirs系统变量指定所指定的路径中所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器这个类加载器由sun.misc.Launcher$AppClassLoader实现。这个类加载器是ClassLoader.getSystemClassLoader()方法的返回值，所以一般也称它为系统类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 这个图展示的类加载器之间的这种层次关系，称为双亲委派模型。双亲委派模型要求除了顶层的启动类加载器外，其余的类加载都应当有自己的父类加载器。这里的类加载器之间的父子关系一般不会以继承的关系来实现，而是都使用组合关系来复用父加载器代码。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。这样做能保证一个类始终是被同一个类加载器加载。 1234567891011121314151617/** * Created by YangFan on 2016/11/15 下午4:17. * &lt;p/&gt; * 我们可以打印一下各种加载器看看是否复合图上描述 */public class Loader &#123; public static void main(String[] args) &#123; // 应用程序类加载器 System.out.println(ClassLoader.getSystemClassLoader()); // 扩展类加载器 System.out.println(ClassLoader.getSystemClassLoader().getParent()); // 启动类加载器 System.out.println(ClassLoader.getSystemClassLoader().getParent().getParent()); // 应用程序类加载器加载的路径 System.out.println(System.getProperty("java.class.path")); &#125;&#125; 运行结果： 1234sun.misc.Launcher$AppClassLoader@330bedb4sun.misc.Launcher$ExtClassLoader@5cad8086null/Users/xiaomai/code/IdeaProjects/jvm/out/production/jvm:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar 这里说到一个实践，就是有时候我们想覆盖第三方jar包中的某个类，除了替换jar包中的class文件的方式，还可以直接在项目中编写一个一样包名的类。上面打印在前面的/Users/xiaomai/code/IdeaProjects/jvm/out/production/jvm（相当于web项目里WEB-INF下的class文件夹）目录下的class会优先于第三方jar包中的class加载。但是却没有办法写一个同样包名的类来覆盖lib和ext下面的库的类。上面的null，表示ClassLoader就是Bootstrap ClassLoader。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM7-类文件结构]]></title>
    <url>%2F2016%2F11%2F04%2FJVM7%2F</url>
    <content type="text"><![CDATA[Class类文件结构本章说一下Java编译后的class文件结构。 魔数与Class文件的版本我这里用sublime打开一个class文件，看到前面4个字节是十六进制0xCAFEBABE,这个是Class文件的魔数. 很多文件存储标准中都使用魔数进行身份识别，因为扩展名可以更改，魔数就是确定这个文件是否为一个能被虚拟机接受的Class文件。 然后看0000 0034，转换成十进制是52，这个表示Java编译的版本号，相信大家在工作中也遇见过Unsupported major.minor version 52.0之类的错误，指的就是这个版本号，52对应的是JDK8。 常量池再后面的就是常量池，常量池可以理解为Class文件之中的资源仓库，我们前面提到过，Java运行时内存区域里有一块方法区，方法区里面有一个运行时常量池，Class文件的这部分数据，会在运行时被加载到方法区的运行时常量池中。常量池中主要存放两大类常量：字面量和符号引用。『字面量』比较接近于Java语言层面的常量概念，如文本字符串、声明为final的常量值等。而『符号引用』则属于编译原理方面的概念，包括了下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 先放一段代码 1234567public class TestClass &#123; private int m; public int inc() &#123; return m + 1; &#125;&#125; 我们用javap命令来看一下编译后的class文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960~ javap -verbose TestClassCompiled from &quot;TestClass.java&quot;public class clazz.TestClass minor version: 0 major version: 52 flags: ACC\_PUBLIC, ACC\_SUPERConstant pool: #1 = Methodref #4.#18 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #3.#19 // clazz/TestClass.m:I #3 = Class #20 // clazz/TestClass #4 = Class #21 // java/lang/Object #5 = Utf8 m #6 = Utf8 I #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 LocalVariableTable #12 = Utf8 this #13 = Utf8 Lclazz/TestClass; #14 = Utf8 inc #15 = Utf8 ()I #16 = Utf8 SourceFile #17 = Utf8 TestClass.java #18 = NameAndType #7:#8 // &quot;&lt;init&gt;&quot;:()V #19 = NameAndType #5:#6 // m:I #20 = Utf8 clazz/TestClass #21 = Utf8 java/lang/Object&#123; public clazz.TestClass(); descriptor: ()V flags: ACC\_PUBLIC Code: stack=1, locals=1, args\_size=1 0: aload\_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 11: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lclazz/TestClass; public int inc(); descriptor: ()I flags: ACC\_PUBLIC Code: stack=2, locals=1, args\_size=1 0: aload\_0 1: getfield #2 // Field m:I 4: iconst\_1 5: iadd 6: ireturn LineNumberTable: line 15: 0 LocalVariableTable: Start Length Slot Name Signature 0 7 0 this Lclazz/TestClass;&#125;SourceFile: &quot;TestClass.java&quot; 看看常量池里的内容： Utf8就是UTF-8编码的字符串，Class、Methodref和Fieldref则是符号引用。符号引用后面的编号最终也指向了字符串表示他们的值。 访问标志常量池结束后，紧接着2个字节代表访问标志(access_flag)。包括这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final。上面的flags中的值是flags: ACC_PUBLIC, ACC_SUPER表示这个类是public的。 描述符这里说一下方法和字段的描述符。基本类型是取首字母的大写基本上。例如byte就是B,有3个特殊的，long是J，boolean是Z，V是void。L表示对象（例如Ljava/lang/String）。对数组而言，每一维度将使用一个前置的“[”字符来描述。 如定义一个为“java.lang.String[][]”类型的二维数组，将被记录为“[[Ljava/lang/String”。 描述方法的时候，是先参数列表，后返回值。参数列表在小括号“()”内。例如()V表示0个参数，返回值为void，int test(int[] i, char c)的描述符为([IC)I。 字节码指令集aload_0、iconst_1之类的都是字节码指令，下面将字节码操作按用途分为9类，按照分类介绍一下。 加载和存储指令加载和存储指令用于将数据在栈帧中的局部变量表和操作数栈之间来回传输： 将一个局部变量加载到操作栈：iload、iload_、lload、lload_、fload、fload_、dload、dload_、aload、aload_ 将一个数字从操作数栈存储到局部变量表：istore、istore_、lstore、lstore_、fstore、fstore_、dstore、dstore_、astore、astore_ 将一个常量加载到操作栈：bipush、sipush、ldc、ldc_w、ldc2_w、aconst_null、iconst_、lconst_、fconst_、dconst_ 扩充局部变量表的访问索引的指令：wide。 存储数据的操作数栈和局部变量表主要就是由加载和存储指令进行操作，除此之外，还有少量指令，如访问对象的字段或数组元素的指令也会向操作数栈传输数据。上面有尖括号的表示一组指令（例如iload_，就代表了iload_0、iload_1、iload_2、iload_3），iload_0也等价于iload 0。 运算指令运算或算术指令用于堆两个操作数栈上的值进行某种特定运算，并把结果重新存入到操作栈顶。大体上算术指令可以分为两种：对整型数据进行运算的指令与堆浮点型数据进行运算的指令，无论是哪种算术指令，都使用Java虚拟机的数据类型，由于没有直接支持byte、short、char和boolean类型的算术指令，对于这类数据的运算，应使用操作int类型的指令代替。 加法指令：iadd、ladd、fadd、dadd 减法指令：isub、lsub、fsub、dsub 乘法指令：imul、lmul、fmul、dmul 除法指令：idiv、ldiv、fdiv、ddiv 求余指令：irem、lrem、frem、drem 取反指令：inge、lneg、fneg、dneg 位移指令：ishl、ishr、iushr、lshl、lshr、lushr 按位或指令：ior、lor 按位与指令：iand、land 按位异或指令：ixor、lxor 局部变量自增指令：iinc 比较指令：dcmpg、dcmpl、fcmpg、fcmpl、lcmp 类型转换指令类型转换指令可以将两种不同的数值类型进行相互转换，这些转换操作一般用于实现用户代码中的显式类型转换操作，或者用于处理字节指令集中数据类型相关指令无法与数据类型一一对应的问题。以下是宽化类型转换，Java虚拟机直接支持，无需指令： int类型到long、float或者double类型 long类型到float、double类型 float类型到double类型 窄化类型指令包括：i2b、i2c、i2s、l2i、f2i、f2l、d2i、d2l、和d2f。窄化类型转换可能导致不同的正负号、不同的数量级以及精度丢失的情况。 对象创建与访问指令 创建类实例的指令：new 创建数组的指令：newarray、anewarray、multianewarray 访问类字段和实例字段的指令：getfield、putfield、getstatic、putstatic 把一个数组元素加载到操作数栈的指令：baload、caload、saload、iaload、laload、faload、daload、aaload 将一个操作数栈的值存储到数组元素中的指令：bastore、castore、sastore、iastore、fastore、dastore、aastore 取数组长度的指令：arraylength 检查类实例类型的指令：instanceof、checkcast 操作数栈管理指令 将操作数的组合暂定一个或两个元素出栈：pop、pop2 复制栈顶一个或两个数值并将复制值或双份的复制值重新压入栈顶：dup、dup2、dup_x1、dup_x2、dup_x2、dup2_x2 将栈最顶端的两个数值互换：swap 控制转移指令 条件分支：ifeq、iflt、ifle、ifne、ifgt、ifge、ifnull、ifnonnull、if_icmpeq、if_icmpne、if_icmplt、if_icmpgt、if_icmple、if_icmpge、if_acmpeq、和if_acmpne 复合条件分支：tableswitch、lookupswitch 无条件分支：goto、goto_w、jsr、jsr_w、ret 方法调用和返回指令 invokevirtual调用对象的实例方法 invokeinterface调用接口方法 invokespecial调用一些需要特殊处理的实例方法，包括实例初始化方法、私有方法和父类方法 invokestatic调用类方法 invokedynamic指令用于在运行时动态解析出调用点限定符所引用的方法，并执行该方法 异常处理指令throw语句由athrow指令实现，而catch语句不是由字节码来实现的，采用异常表来实现。 同步指令同步一段指令集序列在Java语言中是由synchronized语句块来表示的，在Java虚拟机的指令集中由monitorenter和monitorexit两条指令来支持。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM6-虚拟机性能监控与故障处理工具]]></title>
    <url>%2F2016%2F11%2F02%2FJVM6%2F</url>
    <content type="text"><![CDATA[给一个系统定位问题的时候，知识、经验是关键基础，数据是依据，工具是运用知识处理数据的手段。这里说的数据包括：运行日志、异常堆栈、GC日志、线程快照(threaddump/javacore文件)、堆转储快照(heapdump/hprof文件)等。经常使用适当的虚拟机监控和分析的工具可以加快我们分析数据、定位问题的速度。 JDK的命令行工具JDK的安装目录bin下提供了很多工具，这些工具其实是jdk/lib/tools.jar的包装而已。 jps：虚拟机进城状况工具jps(JVM Process Status Tool):可以列出正在运行的虚拟机进程，并显示虚拟机执行主类以及这些进程的本地虚拟机唯一ID(Local Virtual Machine IIdentifier, LVMID)，这个LVMID跟系统里的PID是一致的。jps命令格式： jsp [ options ] [ hostid ] jps执行样例： 1234~ jps -l772 15944 sun.tools.jps.Jps15547 org.jetbrains.jps.cmdline.Launcher 选项 作用 -q 只输出LVMID，省略主类的名称 -m 输出虚拟机进程启动时传递给主类main()函数的参数 -l 输出主类的全名，如果进城执行的是Jar包，输出Jar包路径 -v 输出虚拟机进城启动时JVM参数 jstat：虚拟机统计信息监视工具jstat(JVM Statistics Monitoring Tool)是用于监视虚拟机各种运行状态的命令行工具。它可以显示本地或者远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 jstat命令格式为： jstat [ option vmid [ interval [ s | ms ] [ count ] ] ] 对本机来说vmid就是LVMID。interval和count表示间隔和次数，省略表示只查询1次。 选项 作用 -class 监视类装载、卸载数量、总空间以及类装载所耗费的时间 -gc 监视Java堆状况，包括Eden区、两个Survivor区、、老年代、永久带等的容量、已用空间、GC时间合计等信息 -gccapacity 监视内容基本与-gc相同，但输出主要关注Java堆各个区域使用到的最大、最小空间 -gccause 与-gcutil功能一样，但是会额外输出导致上一次GC产生的原因 -gcnew 监视新生代GC状况 -gcnewcapacity 监视内容基本与-gcnew相同，但输出主要关注使用到的最大、最小空间 -gcold 监视老年代GC状况 -gcoldcapacity 监视内容基本与-gcold相同，但输出主要关注使用到的最大、最小空间 -gcpermcapacity 输出永久代使用到的最大、最小空间 -compiler 输出JIT编译器编译过的方法、耗时等信息 -printcompilation 输出已经被JIT编译的方法 jstat执行样例： 12345~ jstat -gcutil 15547 1000 3 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 74.83 0.00 2.40 0.09 98.65 95.48 2 0.013 0 0.000 0.013 74.83 0.00 2.40 0.09 98.65 95.48 2 0.013 0 0.000 0.013 74.83 0.00 2.40 0.09 98.65 95.48 2 0.013 0 0.000 0.013 这里就是每隔1000毫秒，一共执行3次，查询LVMID为15547的gcutil信息。显示空间占用总空间的百分比，S0和S1就是2个Survivor区，E是Eden，O是Old老年代，M表示MetaSpace(JDK8中的元数据区)。YGC(Young GC)和FGC(Full GC)显示的是GC的次数。FGCT和GCT是时间， jinfo：Java配置信息工具jinfo(Configuration Info for Java)的作用是实时地查看和调整虚拟机各项参数。使用jps命令的-v可以查看虚拟机启动时显式指定的参数列表，但如果想知道未被显式指定的参数的系统默认值，可以使用jinfo的-flag选项进行查询，jinfo还可以使用-sysprops选项把虚拟机进程的System.getProperties()的内容打印出来，它还有在运行期修改虚拟机参数的能力。jinfo命令格式： jinfo [ option ] pid 执行样例： 12~ jinfo -flag CMSInitiatingOccupancyFraction 15547 -XX:CMSInitiatingOccupancyFraction=-1 注意jinfo对windows只提供了-flag选项 jmap：Java内存映射工具jmap(Memory Map for Java)命令用于声称堆转储快照（一般称为heapdump或dump文件）。不用命令要想获取Java堆转储快照，可以使用“-XX:+HeapDumpOnOutOfMemoryError”参数，可以让虚拟机在OOM异常出现之后自动生成dump文件，Linux命令下可以通过kill -3发送进程退出信号也能拿到dump文件。 jmap的作用并不仅仅是为了获取dump文件，它还可以查询finalize执行队列、Java堆和永久代的详细信息，如空间使用率、当前使用的是哪种收集器等。和jinfo一样，jmap有不少功能在Windows平台下也是受限制的，除了生成dump文件的-dump选项和用于查看每个类的实例、空间占用统计的-histo选项在所有操作系统都提供之外，其余选项都只能在Linux和Solaris系统下使用。 jmap命令格式： jmap [ option ] vmid 选项 作用 -dump 生成Java堆转储快照。格式为-dump:[live, ]format=b,file=，其中live自参数说明是否只dump出存活的对象 -finalizerinfo 显示在F-Queue中等待Finalizer线程执行finalize方法的对象。只在Linux和Solaris系统下有效 -heap 显示Java堆详细信息，如使用哪种收集器、参数配置、分代状况等。只在Linux和Solaris系统下有效 -histo 显示堆中对象统计信息，包括类、实例数量、合计容量 -permstat 以ClassLoader为统计口径显示永久代内存状态。只在Linux和Solaris系统下有效 -F 当虚拟机进行对-dump选项没有响应时，可使用这个选项强制生成dump快照。只在Linux和Solaris系统下有效 jmap生成dump文件： 123~ jmap -dump:format=b,file=idea.bin 15547Dumping heap to /Users/xiaomai/idea.bin ...Heap dump file created jhat：虚拟机堆转储快照分析工具jhat(JVM Heap Analysis Tool)是与jmap搭配使用的。实际工作中很少用到，比较简陋。分析一下刚才生成的dump文件： 12345678910~ jhat idea.binReading from idea.bin...Dump file created Wed Nov 02 16:55:47 CST 2016Snapshot read, resolving...Resolving 63292 objects...Chasing references, expect 12 dots............Eliminating duplicate references............Snapshot resolved.Started HTTP server on port 7000Server is ready. jstack：Java堆栈跟踪工具jstack(Sstack Trace for Java)命令用于生成虚拟机当前时刻的线程快照(一般称为threaddump或者javacore文件)。『线程快照』就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程死锁、死循环、请求外部资源导致的长时间等待。 jstack [ option ] vmid 选项 作用 -F 当正常输出的请求不被响应时，强制输出线程堆栈 -l 除堆栈外，显示关于锁的附加信息 -m 如果调用到本地方法的时候，可以显示C/C++的堆栈 下面展示部分输出： 12345678910111213141516171819~ jstack -l 155472016-11-02 17:36:52Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.31-b07 mixed mode):"Attach Listener" #13 daemon prio=9 os_prio=31 tid=0x00007f9492316000 nid=0x3f0b waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE Locked ownable synchronizers: - None"NettythreadDeathWatcher-2-1" #12 daemon prio=1 os_prio=31 tid=0x00007f9491346800 nid=0x5103 waiting on condition [0x00007000061e2000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at io.netty.util.ThreadDeathWatcher$Watcher.run(ThreadDeathWatcher.java:147) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145) at java.lang.Thread.run(Thread.java:745) Locked ownable synchronizers: - None JDK可视化工具jdk/bin下还有两个可视化工具。 JConsole：Java监视与管理平台VisualVM：多合一故障处理工具是到目前为止随JDK发布的功能最为强大的运行监视和故障处理工具，除了最基本的运行监视、 故障处理外，还有性能分析的功能，且十分强大。Visual VM还有一个很大的优点，它对应用程序的实际性能影响很小，使得它可以直接应用在生产环境中。VisualVM需要安装一些插件，才能强大的使用，否则就跟没有安装软件的操作系统一样。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM5-内存分配与回收策略]]></title>
    <url>%2F2016%2F11%2F01%2FJVM5%2F</url>
    <content type="text"><![CDATA[内存分配之前讲了垃圾回收器体系以及运作原理，现在来看看对象内存分配那点事儿。对象的内存分配，往大方向讲就是在堆上分配，对象主要分配在新生代的Eden区上，也可能直接分配在老年代中，并不固定，取决于使用的哪一种垃圾收集器以及虚拟机参数设置。 对象优先在Eden分配大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够的空间进行分配时，虚拟机会发一起一次Minor GC。不同的垃圾收集器组合对于对象的分配是有影响的，我们这里都是测试在Serial+SerialOld的收集器组合下测试的代码。下面的代码，-Xms20M -Xmx20M -Xmn10M三个参数限制了Java堆大小为20M，不可扩展，分给新生代10M，剩下10M分给老年代，-XX:SurvivorRatio=8定义了Eden区与一个Survivor区的空间比例是8:1,-XX:+UseSerialGC参数指定Serial垃圾收集器 12345678910111213141516/** * Created by YangFan on 2016/11/1 下午3:34. * &lt;p/&gt; * VM 参数: -verbose:gc -XX:+PrintGCDetails -Xms20M -Xmx20M -Xmn10M -XX:SurvivorRatio=8 -XX:+UseSerialGC */public class EdenGC &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) &#123; byte[] allocation1 = new byte[2 * _1MB]; byte[] allocation2 = new byte[2 * _1MB]; byte[] allocation3 = new byte[2 * _1MB]; // 发生一次MinorGC byte[] allocation4 = new byte[4 * _1MB]; &#125;&#125; GC输出： 12345678910[GC (Allocation Failure) [DefNew: 7643K-&gt;517K(9216K), 0.0078067 secs] 7643K-&gt;6661K(19456K), 0.0078482 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Heap def new generation total 9216K, used 4750K [0x00000007bec00000, 0x00000007bf600000, 0x00000007bf600000) eden space 8192K, 51% used [0x00000007bec00000, 0x00000007bf0223b8, 0x00000007bf400000) from space 1024K, 50% used [0x00000007bf500000, 0x00000007bf581668, 0x00000007bf600000) to space 1024K, 0% used [0x00000007bf400000, 0x00000007bf400000, 0x00000007bf500000) tenured generation total 10240K, used 6144K [0x00000007bf600000, 0x00000007c0000000, 0x00000007c0000000) the space 10240K, 60% used [0x00000007bf600000, 0x00000007bfc00030, 0x00000007bfc00200, 0x00000007c0000000) Metaspace used 3062K, capacity 4494K, committed 4864K, reserved 1056768K class space used 333K, capacity 386K, committed 512K, reserved 1048576K 我们可以看到eden space是8M，前面3个对象都分配到了eden区，在分配allocation4的时候，eden区已经不够了，于是发生了一次Minor GC，但是3个对象都是存活的，并且无法放进Survivor(from space)区，所以通过分配担保机制转移到了老年代去。然后4M的allocation4分配进了Eden区。 大对象直接进入老年代虚拟机提供了一个-XX:PretenureSizeThreshold参数，大于这个设置值的对象直接在老年代分配。这样做的目的是避免在Eden区以及两个Survivor区之间发生大量的复制（新生代采用复制算法）。 -XX:PretenureSizeThreshold只在Serial和ParNew两款收集器有效。 12345678910111213141516/** * Created by YangFan on 2016/11/1 下午3:34. * &lt;p/&gt; * VM 参数: -verbose:gc -XX:+PrintGCDetails -XX:PretenureSizeThreshold=3M -Xms20M -Xmx20M -Xmn10M -XX:SurvivorRatio=8 -XX:+UseSerialGC */public class EdenGC &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) &#123; byte[] allocation1 = new byte[2 * _1MB]; byte[] allocation2 = new byte[2 * _1MB]; byte[] allocation3 = new byte[2 * _1MB]; // 发生一次MinorGC byte[] allocation4 = new byte[4 * _1MB]; &#125;&#125; 运行结果： 12345678910[GC (Allocation Failure) [DefNew: 6357K-&gt;554K(9216K), 0.0055719 secs] 6357K-&gt;4650K(19456K), 0.0056074 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Heap def new generation total 9216K, used 6991K [0x00000007bec00000, 0x00000007bf600000, 0x00000007bf600000) eden space 8192K, 78% used [0x00000007bec00000, 0x00000007bf2490f0, 0x00000007bf400000) from space 1024K, 54% used [0x00000007bf500000, 0x00000007bf58ab68, 0x00000007bf600000) to space 1024K, 0% used [0x00000007bf400000, 0x00000007bf400000, 0x00000007bf500000) tenured generation total 10240K, used 4096K [0x00000007bf600000, 0x00000007c0000000, 0x00000007c0000000) the space 10240K, 40% used [0x00000007bf600000, 0x00000007bfa00020, 0x00000007bfa00200, 0x00000007c0000000) Metaspace used 3201K, capacity 4496K, committed 4864K, reserved 1056768K class space used 350K, capacity 388K, committed 512K, reserved 1048576K 看到对象超过了3M，直接进入了tenured generation(老年代)。 长期存活的对象将进入老年代对象在Eden区每gc留下来一次(大小可复制到Survivor区中)，年龄+1，默认是15岁后移到老年代。这个阀值可以通过-XX:MaxTenuringThreshold设置。 123456789101112131415161718192021/** * Created by YangFan on 2016/11/2 下午13:58. * &lt;p/&gt; * 1岁后直接进入老年代 * * VM参数：-verbose:gc -XX:+PrintGCDetails -Xms20M -Xmx20M -Xmn10M -XX:SurvivorRatio=8 -XX:+UseSerialGC -XX:MaxTenuringThreshold=1 */public class TenuringThresholdTest &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) &#123; byte[] allocation1 = new byte[_1MB / 4]; byte[] allocation2 = new byte[4 * _1MB]; // Eden区放不下了，发起第一次GC，allocation1年龄+1，allocation2因为无法放入Survivor区通过分配担保机制提前进入老年代，allocation3进入新生代Eden区 byte[] allocation3 = new byte[4 * _1MB]; allocation3 = null; // 发起第二次GC，allocation3被回收，allocation1年龄过大进入老年代，allocation4进入Eden区 byte[] allocation4 = new byte[4 * _1MB]; &#125;&#125; 运行结果： 1234567891011[GC (Allocation Failure) [DefNew: 5843K-&gt;783K(9216K), 0.0062294 secs] 5843K-&gt;4879K(19456K), 0.0062786 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] [GC (Allocation Failure) [DefNew: 4961K-&gt;0K(9216K), 0.0018562 secs] 9057K-&gt;4867K(19456K), 0.0018840 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap def new generation total 9216K, used 4178K [0x00000007bec00000, 0x00000007bf600000, 0x00000007bf600000) eden space 8192K, 51% used [0x00000007bec00000, 0x00000007bf014930, 0x00000007bf400000) from space 1024K, 0% used [0x00000007bf400000, 0x00000007bf400228, 0x00000007bf500000) to space 1024K, 0% used [0x00000007bf500000, 0x00000007bf500000, 0x00000007bf600000) tenured generation total 10240K, used 4866K [0x00000007bf600000, 0x00000007c0000000, 0x00000007c0000000) the space 10240K, 47% used [0x00000007bf600000, 0x00000007bfac0ae8, 0x00000007bfac0c00, 0x00000007c0000000) Metaspace used 3103K, capacity 4494K, committed 4864K, reserved 1056768K class space used 338K, capacity 386K, committed 512K, reserved 1048576K 为了适应不同程序的内存状况，Survivor空间中相同年龄的所有对象大小总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到“-XX:MaxTenuringThreshold”设置要求的年龄。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM4-垃圾收集器]]></title>
    <url>%2F2016%2F10%2F31%2FJVM4%2F</url>
    <content type="text"><![CDATA[哪些内存需要回收？程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭，方法或者线程结束的时候内存自然就跟着回收了，所以不需要考虑过多回收的问题。而Java堆和方法区就不一样了，这部分内存的分配和回收都是动态的。 Java堆内存回收因为堆就是放对象的地方，要回收内存，首先要知道哪些对象是不可能再被任何途径使用的 引用计数法这个算法的实现是：给对象中添加一个引用计数器，每当有一个地方引用它时，计数器+1，当引用失效时，计数器-1。Object-C就是使用的这种方式，Java没有选用引用计数算法来管理内存，因为它很难解决对象之间相互循环引用的问题。例子如下 123456789101112131415161718192021222324252627/** * Created by YangFan on 2016/10/31 下午3:48. * &lt;p/&gt; * 虚拟机参数：-verbose:gc */public class ReferenceCountingGC &#123; public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 这个成员属性的唯一意义就是占点内存，以便能在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void main(String[] args) &#123; ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; System.gc(); &#125;&#125; 运行结果 12[GC (System.gc()) 7440K-&gt;632K(125952K), 0.0012069 secs][Full GC (System.gc()) 632K-&gt;520K(125952K), 0.0058047 secs] 看到632K-&gt;520K，意味着两个对象相互引用也被回收了，侧面说明虚拟机不是通过引用计数法来判断对象是否存活的。 可达性分析法这个算法的基本思路是通过一系列的称为GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连(从GC Roots到这个对象不可达)时，则证明此对象是不可用的。下图中object5、object6、object7虽然相互关联，但是到GC Roots是不可达的，所以他们会被回收。 在Java语言中，可用作为GC Roots的对象包括下面几种： 虚拟机栈(栈帧中的本地变量)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI（即一般说的本地方法）引用的对象 引用JDK1.2之前，Java中引用的定义很传统：如果引用类型的数据中存储的数值代表的是另一块内存的起始地址，就称这块内存代表着一个引用。这种定义很纯粹，但是太过于狭隘，一个对象只有被引用或者没被引用两种状态。我们希望描述这样一类对象：当内存空间还足够时，则能保留在内存中；如果内存空间在进行垃圾收集后还是非常紧张，则可以抛弃这些对象。很多系统的缓存功能都符合这样的应用场景。在JDK1.2之后，Java对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用4种，这4种引用强度依次减弱。 强引用在代码中普遍存在，类似Object obj = new Object()这类的引用，只要引用还在，垃圾收集器就不会回收 软引用是用来描述一些还有用但并非必需的对象。在系统将要发生内存溢出异常之前，将会把这些列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK1.2之后，提供了SoftReference来实现软引用。 弱引用也是用来描述非必需对象，被弱引用关联的对象只能生存到下一次GC之前。无论当前内存是够足够，都会回收掉被弱引用关联的对象。在JDK1.2之后，提供了WeakReference类来实现弱引用。 虚引用的存在不会对一个对象的生存时间构成影响，它的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK1.2之后，提供了PhantomReference类来实现。 对象自我拯救用可达性分析算法，对象也需要标记2次后才会被回收，第一次是发现没有与GC Roots相连的引用链接会标记一次，然后看他覆盖finalize()方法或者finalize()被调用过没有，如果finalize()不需要执行，就直接被回收了，如果需要执行，稍后GC会进行第二轮标记，对象有可能被移出回收队列(例如在finalize()中重新给自己赋值)。上代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Created by YangFan on 2016/10/31 下午4:54. * &lt;p/&gt; * 此代码演示两点： * 1. 对象可以在GC时自我拯救 * 2. 这种自救的机会只有一次，因为一个对象的finalize()方法最多只会被系统自动调用一次。 */public class FinalizeEscapeGC &#123; public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive() &#123; System.out.println("yes, i am still alive :)"); &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println("finalize method executed!"); FinalizeEscapeGC.SAVE_HOOK = this; &#125; public static void main(String[] args) throws InterruptedException &#123; SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次成功拯救自己 SAVE_HOOK = null; System.gc(); // 因为finalize方法的优先级很低，所以暂停了0.5秒等待它执行 TimeUnit.MILLISECONDS.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125;else &#123; System.out.println("no, i am dead :( "); &#125; // 下面代码一样，但是这次失败了，因为finalize只执行一次 SAVE_HOOK = null; System.gc(); // 因为finalize方法的优先级很低，所以暂停了0.5秒等待它执行 TimeUnit.MILLISECONDS.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125;else &#123; System.out.println("no, i am dead :( "); &#125; &#125;&#125; 运行结果 123finalize method executed!yes, i am still alive :)no, i am dead :( 方法区回收Java虚拟机规范中说过可以不要求虚拟机在方法区实现垃圾收集，而且在方法区中进行垃圾收集性价比一般比较低。HotSpot VM永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。判断一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是“无用的类”： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 在大量使用反射、动态代理、CGLib等ByteCode框架、动态生成JSP以及OSGi这类频繁自定义ClassLoader的场景都需要虚拟机具备类卸载功能，以保证方法区不会溢出。 垃圾回收算法下面介绍几种垃圾回收算法的思想及发展过程。 标记-清除算法最基础的收集算法是标记-清除(Mark-Sweep)算法，，如同它的名字一样，算法分为标记和清除两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。（标记过程已经介绍过了）。这种算法主要有两个不足： 一个是效率问题，标记和清除两个过程的效率都不高 另一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行中需要分配较大对象时对象的创建，无法找到足够的连续内存而不得不提前出发另一次垃圾收集动作。标记-清除算法的执行过程如图：。 复制算法复制算法是为了解决效率问题而出现的，它将可用的内存分为两块，每次只用其中一块，当这一块内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已经使用过的内存空间一次性清理掉。这样每次只需要对整个半区进行内存回收，内存分配时也不需要考虑内存碎片等复杂情况，只需要移动指针，按照顺序分配即可。复制算法的执行过程如图： 只是这个算法代价太高，内存缩小为原来的一半，现在商用虚拟机都采用这种算法来回收“新生代”，IBM研究表明新生代98%的对象“朝生夕死”，所以不需要按1:1来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代可用内存空间为整个新生代容量的90%，只有10%的内存会被“浪费”。我们没有办法保证每次回收都只有不多余10%的对象存活，所以如果Survivor空间不够用的时候，这些对象将直接通过分配担保机制进入老年代。 标记-整理算法复制算法在对象存活率较高时就要进行较多的复制操作，效率会变低，如果对象存活率太高，还需要额外的空间进行分配担保，所以老年代一般不能直接用这种算法。标记-整理算法是先标记对象，让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。如图： 分代收集算法概括一下Java内存的布局：当前的商业虚拟机垃圾收集都采用“分代收集”算法，把Java堆分为新生代和老年代。在新生代中，垃圾收集时都有大批对象死去，只有少量存活，只需复制少量存活的对象成本低。老年代对象存活率高、没有额外的空间进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。 垃圾收集器垃圾收集器是内存回收的具体实现，JDK1.7之后的HotSpot虚拟机包含的收集器如下图所示： 上图展示了7种不作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。 Serial收集器Serial收集器是一个单线程的收集器，在进行垃圾收集的时候，会暂停其他所有的工作线程，直到它收集结束。新生代采用复制算法，老年代采取标记-整理算法。虽然它会暂停用户的工作线程似乎显得很不能接受，不过它仍然是Client模式下虚拟机的默认新生代收集器，因为它简单而高效，收集几十兆内存停顿时间可以控制在几十毫秒，这是可以接受的。 ParNew收集器ParNew收集器是Serial收集器的多线程版本，它是运行在Server模式下虚拟机中首选的新生代收集器。有个重要的原因是，除了Serial收集器，目前只有它能与CMS收集器配合工作。(CMS是HotSpot在JDK1.5推出的第一款真正意义上的老年代并发收集器，第一次实现了垃圾收集线程基本上与用户线程同时工作–意思就是几乎不会暂停用户的工作线程)。ParNew收集器默认开启的收集线程数与CPU数量相同，与Serial相比，CPU数量越多，它的效果才越好。在CPU数量非常多的情况下，可以使用-XX:ParallelGCThreads参数来限制垃圾收集的线程数。 Parallel Scavenge收集器Parallel Scavenge收集器是一个使用复制算法的新生代收集器。它的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量。吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)，假如虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。高吞吐量可以高效率的运用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数。Parallel Scavenge收集器还有个参数-XX:+UseAdaptiveSizePolicy值得关注，这个参数打开后虚拟机会根据当前系统的运行情况动态调整新生代大小、Eden与Survivor区的比例、晋升老年代对象年龄，以提供最合适的停顿时间或者最大的吞吐量，这种调节方式成为GC自适应调节策略。如果我们不太了解手工优化，交给虚拟机区去优化是个不错的选择。 Serial Old收集器Serial Old是Serial收集器的老年代版本，它是单线程收集器，使用标记-整理算法。这个收集器主要也是在Client模式下的虚拟机使用。 Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和标记-整理算法。由于Parallel Scavenge无法与CMS配合工作，Serial Old在服务端应用上又不给力，JDK1.6后Parallel Old的发布才有了Parallel Scavenge+Parallel Old的应用组合，适合注重吞吐量以及CPU资源敏感的场合。 CMS收集器CMS收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器是基于标记-清除算法实现的。CMS非常优秀但是有3个缺点： 占用CPU资源 并发收集会产生浮动垃圾(收集的同时产生的新垃圾) 内存空间碎片问题 G1收集器G1是一款面向服务端应用的垃圾收集器，是当今收集器技术发展的最前沿成果之一，随JDK1.7 HotSpot发布。G1的目标是替换掉CMS收集器，特点如下： 并行(多线程)+并发(与用户线程同时工作) 分代收集 空间整合(不会产生空间碎片) 可预测的停顿 在G1收集器之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1收集器不再是这样，使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region的集合。 G1收集器跟踪各个Region里面的垃圾堆积的价值大小，在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也是Garbage-First名称的由来）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM3-内存溢出异常实战]]></title>
    <url>%2F2016%2F10%2F31%2FJVM3%2F</url>
    <content type="text"><![CDATA[在Java虚拟机规范的描述中，除了程序计数器，其他几个运行时区域都有发生OutOfMemoryError异常的可能。本文有两个目的： 通过代码验证Java虚拟机规范中描述的各个运行时区域存储的内容。 希望我们在工作中遇到问题的时候能迅速判断是哪个区域的内存溢出，知道什么样的代码会导致这些区域溢出，以及出现这些异常后该如何处理。 这个图展示了如何在Idea中设置VM参数。 Java堆异常Java堆用于储存对象实例，只要不断地创建对象且对象不被回收，那么在对象数量到达最大堆的容量限制后就会产生OOM。 1234567891011121314151617181920/** * Created by YangFan on 2016/10/31 下午1:34. * &lt;p/&gt; * 设置堆大小为20m，不可扩展(堆的最小值-Xms参数和最大值-Xmx参数设置为一样可避免堆自动扩展) * VM参数：-Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError */public class HeapOOM &#123; static class OOMObject &#123; &#125; public static void main(String[] args) &#123; List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while (true) &#123; list.add(new OOMObject()); &#125; &#125;&#125; 结果如下 1234java.lang.OutOfMemoryError: Java heap spaceDumping heap to java_pid56046.hprof ...Heap dump file created [27956110 bytes in 0.186 secs]Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space 这个问题很常见，根据错误提示可以定位到代码，分清楚是内存泄露还是内存溢出。如果是内存泄露，找出GC无法回收的对象代码位置。如果不存在泄露，就是说内存中的对象确实都还必须存活着，应当检查一下虚拟机的堆参数(-Xms和-Xmx)，代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗。 虚拟机栈和本地方法栈溢出由于HotSpot虚拟机中并不区分虚拟机栈和本地方法栈，因此对于HotSpot来说-Xoss(设置本地方法栈大小)是无效的，栈容量只由-Xss参数设置。关于虚拟机栈和本地方法栈，在虚拟机规范中描述了两种异常： 如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError异常。 如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常。 123456789101112131415161718192021222324252627/** * Created by YangFan on 2016/10/31 下午2:06. * &lt;p/&gt; * 不断地递归调用导致栈深度增加 * VM参数：-Xss128k * */public class JavaVMStackSOF &#123; private int stackLength = 1; public void stackLength() &#123; stackLength++; stackLength(); &#125; public static void main(String[] args) &#123; JavaVMStackSOF javaVMStackSOF = new JavaVMStackSOF(); try &#123; javaVMStackSOF.stackLength(); &#125; catch (Throwable e) &#123; System.out.println("stack length:" + javaVMStackSOF.stackLength); throw e; &#125; &#125;&#125; 运行结果 123stack length:29460Exception in thread &quot;main&quot; java.lang.StackOverflowError at oom.JavaVMStackSOF.stackLength(JavaVMStackSOF.java:19) 在单线程下，无论是栈帧太大，还是虚拟机栈容量太小，当内存无法分配的时候，虚拟机抛出的都是StackOverFlow异常。可以通过不断创建线程的方式产生内存溢出异常，不过这个异常与栈容量大小没有什么关系，因为不断创建线程，每个线程分配的容量越大，那么总共可产生线程数量就越小，就越容易出现OOM。这个只能通过减少最大堆内存(留给栈分配的内存变大)和减少栈容量来换取更多的线程。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Created by YangFan on 2016/10/31 下午2:18. * &lt;p/&gt; * 不断创建线程导致内存溢出 * VM参数：-Xss2M */public class JavaVMStackOOM &#123; private int count = 0; public void stackLeakByThread() &#123; while (true) &#123; Thread thread = new Thread() &#123; @Override public void run() &#123; try &#123; count++; TimeUnit.SECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; thread.start(); &#125; &#125; // 不要在Windows下运行这段代码，可能会假死 public static void main(String[] args) &#123; JavaVMStackOOM javaVMStackOOM = new JavaVMStackOOM(); try &#123; javaVMStackOOM.stackLeakByThread(); &#125; catch (Throwable e) &#123; System.out.println("thread count: " + javaVMStackOOM.count); throw e; &#125; &#125;&#125; 运行结果 123thread count: 2028Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: unable to create new native thread at java.lang.Thread.start0(Native Method) 方法区和运行时常量池溢出前面提到过，运行时常量池也是方法区的一部分，并且在JDK8 HotSpot中去掉了永久代。String.intern()是一个Native方法，它的作用是：如果常量池中有一个String对象的字符串就返回池中的这个字符串的String对象；否则，将此String对象包含的字符串添加到常量池中去，并且返回此String对象的引用。 12345678910111213141516171819202122/** * Created by YangFan on 2016/10/31 下午3:01. * &lt;p/&gt; * * VM参数-XX:PermSize=10M -XX:MaxPermSize=10M * * 对于JDK 1.6 HotSpot而言，方法区=永久代，这里看到OutOfMemoryError的区域是“PermGen space”，即永久代，那其实也就是方法区溢出了 * * JDK7这个例子会一直循环，因为JDK 7里String.intern生成的String不再是在perm gen分配,而是在Java Heap中分配 * JDK8移除了永久代（Permanent Generation ），替换成了元空间（Metaspace）内存分配模型 * 设置虚拟机参数-XX:MaxMetaspaceSize=1m，可出现OutOfMemoryError: Metaspace 溢出 */public class RuntimeConstantPoolOOM &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); int i = 0; while (true) list.add(String.valueOf(i++).intern()); &#125;&#125; 本机直接内存溢出这个地方的溢出，特征是发现OOM后Dump文件很小，而程序中间接或直接使用了NIO，那就考虑检查一下是不是这个原因。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM2-Java内存区域]]></title>
    <url>%2F2016%2F10%2F28%2FJVM2%2F</url>
    <content type="text"><![CDATA[Java内存区域下面从概念上介绍Java虚拟机内存的各个区域，讲解这些区域的作用、服务对象以及其中可能产生的问题，这是翻越虚拟机内存管理这堵围墙的第一步。 运行时数据区域Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域，包含以下几个运行时数据区域。 注意看图上分为线程共享数据区域和线程私有数据区域。 线程私有数据区程序计数器程序计数器(Program Counter Register)是比较小的一块内存空间,在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 一个处理器一时间只会执行一条线程的指令，因此线程切换后为了能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为线程私有的内存。 Java虚拟机栈与程序计数器一样，Java虚拟机栈(Java Virtual Machine Stacks)也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧(Stack Frame)用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈入栈到出栈的过程。在这个区域中，如果我们写一个回调的死循环可能会抛出StackOverFlow异常，或者是在区域大小动态扩展的时候申请不到足够的内存，也会抛出OutOfMemoryError异常。 本地方法栈与虚拟机栈类似，不过是为Native方法服务的。虚拟机规范中没有强制的规定，HotSpot VM直接把本地方法栈和虚拟机栈合二为一了。 线程共享数据区Java堆对于大多数应用来说，Java堆(Java Heap)是Java虚拟机所管理的内存中最大的一块，此内存区域的唯一目的就是存放对象实例。由于现代GC基本都采用分代收集算法，所以Java堆还可以细分为：新生代和老年代；再细致一点还有Eden空间、From Survivor空间、To Survivor空间等。这个区域如果满了，会抛出OutOfMemoryError异常。 方法区方法区(Method Area)用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。在Hotspot VM中，这个区域被称为“永久代”(Permanent Generation)，其他虚拟机则不存在永久代。并且使用永久代来实现方法区，容易遇到内存溢出问题(-XX:MaxPermSize)，所以JDK8的HotSpot VM去掉“永久代”，以“元数据区”（Metaspace）替代之。在JDK7的HotSpot中，原本放在永久代的字符串常量池也被移除。这个区域如果满了，会抛出OutOfMemoryError异常。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分，图上面没有。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译器生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。那么运行时常量池相对Class文件常量池另外一个重要特征是动态性，并非Class文件中常量池的内容才能进入方法区运行时常量池，例如String的intern()方法就能将新的常量放入池中。常量池如果满了，会抛出OutOfMemoryError异常。 直接内存直接内存(Direct Memory)并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常出现。JDK1.4加入的NIO，引入了基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这个堆外内存虽不会受堆大小的限制，但是受本机总内存（RAM+SWAP）大小以及处理器寻址空间的限制，所以可能会出现OutOfMemoryError异常。 对象探秘对象创建在语言层面上，创建对象只是一个new关键字而已，而在虚拟机中创建一个对象的过程呢？ 当虚拟机遇到一条new指令，先检查指令参数能否在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析和初始化过，如果没有，就先执行相应的类加载过程。 在类加载检查通过后，虚拟机为新生类分配内存(对象所需内存大小在类加载完成后已经确定)，为对象分配空间就是把一块确定大小的内存从Java堆中划分出来。 如果Java堆内存是规整的，使用指针碰撞方式。意思是所有用过的内存在一边，空闲的内存在另外一边，中间放着一个指针作为分界点的指示器，分配内存就仅仅是把指针向空闲那边挪动一段与对象大小相等的距离罢了。如果垃圾收集器选择的是Serial、ParNew这种基于压缩算法的，虚拟机采用这种分配方式。 如果Java堆内存不是规整的，已使用的内存和未使用的内存相互交错，那么虚拟机将采用的是空闲列表法来为对象分配内存。意思是虚拟机维护了一个列表，记录上哪些内存块是可用的，再分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的内容。如果垃圾收集器选择的是CMS这种基于标记-清除算法的，虚拟机采用这种分配方式。 除了可用空间外，还有个问题是在虚拟机中创建并发创建对象也不是线程安全的，有两个方案解决这个问题： 对分配内存空间的动作进行同步处理 使用本地线程分配缓冲(Thread Local Allocation Buffer, TLAB)，即每个线程在Java堆中预先分配一小块内存。哪个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。虚拟机是否使用TLAB,可以通过-XX:+/-UseTLAB参数来设定。 内存分配完成，虚拟机需要将分配到的内存空间都初始化为零值。这一步保证了对象的实例字段不被赋值就可以使用对应字段的零值。 虚拟机进行必要设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希吗、对象的GC分代年龄信息。这些信息放在对象的对象头中。 从虚拟机角度来看一个新的对象已经产生了，但从Java程序的视角来看，对象创建才刚开始—-&lt;init&gt;方法没有执行，所有的字段都还为零。接下来执行&lt;init&gt;方法，按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 对象的内存布局在Hotspot虚拟机中，对象在内存中存储的布局可用分为3块区域：对象头，实例数据，和对齐填充。 对象头包含2部分数据，第一部分用于存储对象自身的运行时数据(如哈希吗、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等)。第二部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，不过这跟对象访问定位的实现方式有关系，下面介绍。 实例数据就是对象真正存储的有效信息，包括从父类继承下来的。 对齐填充起占位符的作用，因为HotSpot VM的自动内存管理系统要求对象的起始地址必须的8字节的整倍数。 对象的访问定位建立对象是为了使用对象，我们的Java程序需要通过栈上的reference数据来操作堆上的具体对象，目前主流的访问方式有使用句柄和直接指针两种。 如果使用句柄访问，Java堆中会划分一块内存来做句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。如图所示 如果使用直接指针访问，那么Java堆的对象布局中就包含了类型指针，而reference中存储的直接就是对象地址。(这种方式类型指针就在对象数据中)。HotSpot就是使用的这种方式，如图所示]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM1-Java介绍]]></title>
    <url>%2F2016%2F10%2F28%2FJVM1%2F</url>
    <content type="text"><![CDATA[今天开写JVM系列的学习笔记，书籍为周志明的《深入理解Java虚拟机》 走进Java Java技术体系Sun官方所定义的技术体系包括以下几个组成部分 Java程序设计语言 各种硬件平台上的Java虚拟机 Class文件格式 Java API类库 来自商业机构和开源社区的第三方Java类库 我们把Java程序设计语言、Java虚拟机、Java API类库这三部分统称为JDK，JDK是用于支持Java程序开发的最小环境。Java API类库中的Java SE API子集和Java虚拟机这两部分统称为JRE，JRE是支持Java程序运行的标准环境。下图展示了Java技术体系所包含的内容，以及JDK和JRE所覆盖的范围。 Java发展史1999年HotSpot虚拟机作为JDK1.2附加程序发布，成为JDK1.3及之后版本的Sun JDK默认虚拟机。2002年5月8日，JDK1.4发布，新特性包括，正则表达式、异常链、NIO、日志类、XML解析器和XSLT转换器等。2004年2月13日，JDK1.5发布，JDK1.5在Java语法易用性上做出了非常大的改进。例如，自动装箱、泛型、动态注解、枚举、可变长参数、循环遍历(foreach循环)等语法特性，在虚拟机和API层面上，这个版本改进了Java的内存模型(Java Memory Model, JMM)，提供了java.util.concurent并发包等。2006年12月11日，JDk1.6发布，JDK1.6的改进包括：提供动态语言支持，提供编译API和微型HTTP服务器API等。这个版本对Java虚拟机内部做了大量改进，包括锁与同步、垃圾收集、类加载等方面的算法。2011年7月，JDK7发布，由于Sun公司被Oracle收购等各种原因，原计划在JDK7发布的Lambda，Jigsaw和Coin延迟，JDK7的主要改进包括：提供新的G1收集器，加强对非Jaa语言的调用支持，升级类加载架构等。2014年3月19日，JDK8发布，JDK8改进比较多，最大的改进是Lambda表达式（以及因之带来的函数式接口，很多原有类都做了变更，但能够与以往版本兼容，堪称奇功！），还有Stream API流式处理，joda-time等等一些新特性。但有一些本来计划发布的大变更，比如模块化等推迟到了JDK9中。 Java虚拟机在JDK1.3之后，HotSpot VM成了Sun JDK和OpenJDK中所带的默认虚拟机。HotSpot VM的热点代码探测能力可以通过执行计数器找出最具有编译价值的代码，然后通知JIT编译器以方法单位进行编译。如果一个方法被频繁调用，或方法中有效循环次数很多，将会分别触发标准编译和OSR(栈上替换)编译动作。通过编译器与解释器恰当地协同工作，可以在最优化的程序响应时间与最佳执行性能中取得平衡，而且无须等待本地代码输出才能执行程序，即时编译的时间压力也相对减小，这样有助于引入更多的代码优化技术没输出质量更高的本地代码。我们可以在命令行里执行java -version看看本机上的虚拟机。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java获取URL上的参数]]></title>
    <url>%2F2016%2F07%2F21%2FJava%E8%8E%B7%E5%8F%96URL%E4%B8%8A%E7%9A%84%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[最近遇到一个需求需要在获取URL字符串上的kv键值对，我们都知道Java Web在请求是直接用request来获取值的。如果是字符串呢，就需要正则表达式来自己截取了。自己写代码是比较麻烦的，下面推荐用Guava工具包，2行代码就可以解决这个需求了。 12345private String getPara(String url, String name) &#123; String params = url.substring(url.indexOf(&quot;?&quot;) + 1, url.length()); Map&lt;String, String&gt; split = Splitter.on(&quot;&amp;&quot;).withKeyValueSeparator(&quot;=&quot;).split(params); return split.get(name);&#125; 先截取到?后面的字符串，然后再用Splitter.on(&quot;&amp;&quot;).withKeyValueSeparator(&quot;=&quot;).split(params);就轻松的解决了~]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Guava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring cloud @EnableOAuth2Client]]></title>
    <url>%2F2016%2F07%2F15%2FSpring-cloudt-EnableOAuth2Client%2F</url>
    <content type="text"><![CDATA[Spring Cloud oauth2 开启客户端功能，并启用LoadBalanced 如果不添加以下配置，只添加@EnableOAuth2Client注解，spring cloud默认是在web环境下使用的AuthorizationCodeResourceDetails。 具体代码在OAuth2RestOperationsConfiguration类中。 123456789101112131415161718192021222324252627282930313233343536373839@Configuration@ConditionalOnBean(OAuth2ClientConfiguration.class)@ConditionalOnWebApplicationprotected static class SessionScopedConfiguration &#123; @Bean @ConfigurationProperties(&quot;security.oauth2.client&quot;) @Primary public AuthorizationCodeResourceDetails oauth2RemoteResource() &#123; AuthorizationCodeResourceDetails details = new AuthorizationCodeResourceDetails(); return details; &#125; @Bean public FilterRegistrationBean oauth2ClientFilterRegistration( OAuth2ClientContextFilter filter, SecurityProperties security) &#123; FilterRegistrationBean registration = new FilterRegistrationBean(); registration.setFilter(filter); registration.setOrder(security.getFilterOrder() - 10); return registration; &#125; @Configuration protected static class ClientContextConfiguration &#123; @Resource @Qualifier(&quot;accessTokenRequest&quot;) protected AccessTokenRequest accessTokenRequest; @Bean @Scope(value = &quot;session&quot;, proxyMode = ScopedProxyMode.INTERFACES) public DefaultOAuth2ClientContext oauth2ClientContext() &#123; return new DefaultOAuth2ClientContext(this.accessTokenRequest); &#125; &#125;&#125; 这个东西我也没找到在哪里可以配置，就自己在Application手动加入以下配置来使用吧。 123456789101112@Bean@Primary@LoadBalancedpublic OAuth2RestTemplate xmRestTemplate(ClientCredentialsResourceDetails xmOauth2RemoteResource) &#123; return new OAuth2RestTemplate(xmOauth2RemoteResource);&#125;@Bean@ConfigurationProperties(&quot;security.oauth2.client&quot;)public ClientCredentialsResourceDetails xmOauth2RemoteResource() &#123; return new ClientCredentialsResourceDetails();&#125;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud OAuth2RestTemplate loadBalanced]]></title>
    <url>%2F2016%2F04%2F21%2Fspring-cloud-OAuth2RestTemplate-loadBalanced%2F</url>
    <content type="text"><![CDATA[在项目中访问另一个微服务的时候我们可以这样用RestTemplate来调用其他服务: 123@Autowiredprivate RestTemplate restTemplate;RestResponse response = restTemplate.postForObject(&quot;http://user-service/user/getUser&quot;, para, User.class); 在spring cloud环境下，这个注入的restTemplate是具备了客户端负载均衡功能的，也会用到eureka服务发现功能，user-service就是服务的名称。我的项目启用了oauth2认证。spring cloud也提供了一个OAuth2RestTemplate来很方便的调用其他服务。但是在我测试的时候一直报错UnknownHost，我猜测他肯定是没有用到loadBalanced和eureka的服务发现功能。我翻遍了官方文档也没有找到相关的说明。 Google搜了大半天后，看了作者在git也讨论过这个类loadBalanced功能之类的，还翻到一个没有什么用 @LoadBalanced注解，翻了半天源码后终于在OAuth2LoadBalancerClientAutoConfiguration这样一个类中发现了一点蛛丝马迹。 1234567891011121314151617181920212223242526@Configuration@ConditionalOnClass(&#123; LoadBalancerInterceptor.class, OAuth2RestTemplate.class &#125;)@ConditionalOnBean(LoadBalancerInterceptor.class)@AutoConfigureAfter(OAuth2AutoConfiguration.class)public class OAuth2LoadBalancerClientAutoConfiguration &#123; @Configuration @ConditionalOnProperty(value = &quot;security.oauth2.resource.loadBalanced&quot;, matchIfMissing = false) protected static class UserInfoLoadBalancerConfig &#123; @Bean public UserInfoRestTemplateCustomizer loadBalancedUserInfoRestTemplateCustomizer( final LoadBalancerInterceptor loadBalancerInterceptor) &#123; return new UserInfoRestTemplateCustomizer() &#123; @Override public void customize(OAuth2RestTemplate restTemplate) &#123; List&lt;ClientHttpRequestInterceptor&gt; interceptors = new ArrayList&lt;&gt;( restTemplate.getInterceptors()); interceptors.add(loadBalancerInterceptor); restTemplate.setInterceptors(interceptors); &#125; &#125;; &#125; &#125;&#125; 可以看到，只要配置了security.oauth2.resource.loadBalanced为true，我们的OAuth2RestTemplate就具有LoadBalancer功能了。我们先在application.yml中加上这样的配置。 1234security: oauth2: resource: loadBalanced: true 然后注入这个类： 12@Autowiredprivate OAuth2RestTemplate restTemplate; 结果还是不行，一样的错误，难道这个类没有用吗，于是我在我的代码和customize方法初始化执行的时候打了2个断点，发现注入对象的根本就不是这个地方初始化使用的那个对象。又倒腾了好一会才找到，必须得注入一个bean名字为userInfoRestTemplate的对象。 123@Autowired@Qualifier(&quot;userInfoRestTemplate&quot;)private OAuth2RestTemplate restTemplate; 终于可以正常使用了，不知道为什么这个配置并没有在文档中提到，估计以后会补上这个文档的。现在spring cloud的文档有些地方跟最新的代码也表现得不太一致，特别是spring security这一块，做的时候一定要多多注意。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-cloud OAuth2.0配置]]></title>
    <url>%2F2016%2F03%2F31%2FSpring-cloud-OAuth2-0%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在spring cloud项目环境中配置oauth2.0认证。 http://callistaenterprise.se/blogg/teknik/2015/04/27/building-microservices-part-3-secure-APIs-with-OAuth/可以先看看这篇文章。 我花了不少时间才把这个调通，spring cloud的版本和文档也存在不一致的地方。以下所有的操作都基于Brixton.RC1搭建，须保持所有相关项目都引用此parent。否则会出现各种莫名其妙的错误。 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-parent&lt;/artifactId&gt; &lt;version&gt;Brixton.RC1&lt;/version&gt; &lt;relativePath/&gt;&lt;/parent&gt; https://spring.io/blog/2015/11/30/migrating-oauth2-apps-from-spring-boot-1-2-to-1-3 这篇文章显示了不同版本之间的区别。目前官网最新提供的Angel SR6和Brixton RC1，它们引用的Spring Boot版本不一样。这2个版本在Spring Security这一块改动比较大。Spring Boot1.3 移除了官方文档中提到的@EnableOAuth2Resource注解。http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_token_relay反正感觉官网提供这个文档写得不太对。 下面展示我最终正常运行的一个配置。Zuul Proxy和AuthServer，我把它们放在了同一个应用里。在pom中加入oauth2的依赖。 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt;&lt;/dependency&gt; 然后是Application 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081@SpringBootApplication@EnableZuulProxy//必须添加@EnableResourceServer，Zuul才会进行Token Relay。//(查看各种源码后才发现。文档描述的@EnableOAuth2Sso根本没有什么卵用。只有//@EnableResourceServer才会加载OAuth2AuthenticationProcessingFilter)@EnableResourceServer@EnableAuthorizationServerpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; //为什么不用自动配置。因为/oauth/check_token默认是denyAll. //必须手动设置oauthServer.checkTokenAccess("isAuthenticated()"); //才访问能验证Access Token。 @Configuration protected static class OAuthSecurityConfig extends AuthorizationServerConfigurerAdapter &#123; @Autowired private AuthenticationManager authenticationManager; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints.authenticationManager(authenticationManager); &#125; @Override public void configure(AuthorizationServerSecurityConfigurer oauthServer) throws Exception &#123; oauthServer.checkTokenAccess("isAuthenticated()"); &#125; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; clients.inMemory() .withClient("clientId") .secret("secretId") .authorizedGrantTypes("authorization_code", "client_credentials") .scopes("app"); &#125; &#125; @Configuration protected static class RestSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.anonymous().disable() .sessionManagement() .sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() .exceptionHandling()// .accessDeniedHandler(accessDeniedHandler()) // handle access denied in general (for example comming from @PreAuthorization// .authenticationEntryPoint(entryPointBean()) // handle authentication exceptions for unauthorized calls. .and() .authorizeRequests()// .antMatchers("/hystrix.stream/**", "/info", "/error").permitAll() .anyRequest().authenticated().and().csrf().disable(); &#125; // @Bean// @Autowired// AccessDeniedHandler accessDeniedHandler() &#123;// return new AccessDeniedExceptionHandler();// &#125;//// @Bean// @Autowired// AuthenticationEntryPoint entryPointBean() &#123;// return new UnauthorizedEntryPoint();// &#125; // 不需要权限控制的路径 @Override public void configure(WebSecurity web) throws Exception &#123; web.ignoring().antMatchers("/hystrix.stream/**", "/info", "/error"); &#125; &#125;&#125; 然后在API里同样加入依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt;&lt;/dependency&gt; 在application.yml中加入以下配置: 12345678910security: oauth2: resource: token-info-uri: http://localhost:10000/oauth/check_token client: client-id: clientId client-secret: secretId user-authorization-uri: http://localhost:10000/oauth/authorize access-token-uri: http://localhost:10000/oauth/token grant-type: client_credentials Application.java中加上@EnableResourceServer 1234567891011121314151617@EnableResourceServerpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Configuration protected static class RestSecurity extends WebSecurityConfigurerAdapter &#123; //不需要权限控制的URL @Override public void configure(WebSecurity web) throws Exception &#123; web.ignoring().antMatchers("/info", "/error"); &#125; &#125;&#125; 配置完了，启动应用。获取access_token。 1234567891011curl -s clientId:secretId@localhost:10000/oauth/token \ -d grant_type=client_credentials \ -d scope=app &#123; "access_token": "8265eee1-1309-4481-a734-24a2a4f19299", "token_type": "bearer", "expires_in": 43189, "scope": "app"&#125; 访问API的时候在Http Header中带上，Authorization: Bearer$access_token。即可…]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring cloud项目实践(三)]]></title>
    <url>%2F2016%2F03%2F22%2FSpring-cloud%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5-%E4%B8%89%2F</url>
    <content type="text"><![CDATA[持续集成配置jenkins构建项目，自动build出docker镜像，发布到docker私库中，或者从目标服务器中启动容器。 Maven的Docker插件http://www.cnblogs.com/skyblog/p/5163691.html 有讲到如何用Dockerfile构建，下面是采用Maven插件的方式构建，插件的文档在这里1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;docker.plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;!--绑定build命令到mvn package中--&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;!--绑定push命令到mvn deploy中--&gt; &lt;execution&gt; &lt;id&gt;push-image&lt;/id&gt; &lt;phase&gt;deploy&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;push&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;imageName&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;:$&#123;project.version&#125;&lt;/imageName&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;imageName&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/imageName&gt; &lt;forceTags&gt;true&lt;/forceTags&gt; &lt;imageTags&gt; &lt;!--&lt;imageTag&gt;$&#123;project.version&#125;&lt;/imageTag&gt;--&gt; &lt;imageTag&gt;latest&lt;/imageTag&gt; &lt;/imageTags&gt; &lt;dockerDirectory&gt;$&#123;project.basedir&#125;/src/main/docker&lt;/dockerDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Dockerfile文件在src/main/docker/Dockerfile内容如下 123456FROM java:8VOLUME /tmpADD pin-user-0.1.0.jar app.jarRUN bash -c &apos;touch /app.jar&apos;EXPOSE 9000ENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] https://spring.io/guides/gs/spring-boot-docker/ 特别解释一下ENTRYPOINT指令,docker run命令中的参数都会传递给ENTRYPOINT指令。执行docker run -d pin-user --config.host=localhost启动容器。会把–config.host加在ENTRYPOINT后面,相当于执行了java -Djava.security.egd=file:/dev/./urandom -jar /app.jar --config.host=localhost。有了这个参数后我们就能很方便的控制环境和配置文件了。 配置jenkinsjenkins中新建一个项目，配置好git后，执行目标服务器的一个shell脚本来启动容器。 然后开始构建项目，因为在pom.xml配置中绑定了docker:build到package命令中，所以会自动执行docker:build，这里遇到了一个docker命令的权限问题，jenkins抛出一个错误。1java.io.IOException: Permission denied 在jenkins所在的服务器上执行以下命令查看jenkins用户组 1id jenkins 将jenkins用户加入到docker组中。 1usermod -a -G docker jenkins 再构建，权限问题没有了，但是我又得到另外一个错误 [ERROR] Failed to execute goal com.spotify:docker-maven-plugin:0.3.258:build (default) on project pin-user: Exception caught: Error getting container 1e509efd653d0a3a942bf5ef34601305b7301d64378381614b55d3f5f88c7166 from driver devicemapper: open /dev/mapper/docker-202:33-5767218-1e509efd653d0a3a942bf5ef34601305b7301d64378381614b55d3f5f88c7166: no such file or directory 说是因为docker在centos下的存储驱动原因，我这里试试把devicemapper换成btrfs。在centos下只能选择这2种方式。 需要把docker使用的分区的文件系统换掉。这一步会镜像会被全部清除掉，记得备份镜像，因为我是测试环境，所以镜像全部丢了也无所谓。教程如下。 https://wiki.centos.org/PhilipJensen/CentOS6ConvertToBTRFS#head-c0851e0e7c9205aa8ca5616b85179b96981b24a7 1umount /dev/xvdc1 提示divice busy。下面命令把相关进程kill掉再umount 1fuser -m -v -i -k /dev/xvdc1 再执行这个命令。 1btrfs-convert /dev/xvdc1 完事后再挂载回去 1mount /dev/xvdc1 /mnt2 还是提示busy，reboot重启下，再mount。然后在/etc/sysconfig/docker加上--storage-driver btrfs参数。 1other_args=&quot;--graph=/mnt2/apps/docker --storage-driver btrfs --insecure-registry=10.168.248.36:5000&quot; 重启docker，再执行docker info就看到docker的存储驱动已经变了 启动docker容器的时候报错了，把/var/lib/docker/linkgraph.db删了因为我在/etc/sysconfig/docker修改了docker的目录。所以我这里是目录是/mnt2/apps/docker/linkgraph.db。service restart docker重启下docker即可。现在再用jenkins构建和发布就没有错误了。 启动脚本1/mnt/web/scripts/docker_run.sh 10.168.248.36:5000/pin-user 9000 "--config.profile=dev --config.host=10.168.248.36" 下面我解释一下这个启动docker容器的脚本。脚本后面跟了3个参数，一个是镜像名称，一个是端口号，一个是启动容器加在ENTRYPOINT的项目配置。先找出之前镜像对应的containerId，把它删除掉，然后再用新的镜像启动容器。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#!/bin/bash#defined export JAVA_HOME=/usr/java/jdk1.8.0_40/IMAGE="$1"OPTIONS=""PORT="$2"#param validateif [ $# -lt 2 ]; then echo "you must use like this : ./deploy_run.sh &lt;image&gt; &lt;port&gt; [options]" exitfiif [ "$3" != "" ]; then OPTIONS="$3"fi#拿到容器ID后kill掉并删除。delete_container()&#123; echo "the container id is $1" if [ -n "$1" ]; then echo "delete container:" $1 docker stop $1 docker rm -f $1 fi&#125;echo "&gt;&gt;&gt; Get old image $IMAGE container id"CID=$(docker ps | grep "$&#123;IMAGE&#125;" | awk '&#123;print $1&#125;')#因为jenkins每次的build的时候，如果镜像的tag没有指定，那么新的镜像build成功后，之前的镜像名称就会变成none。#所以我们找出为名字为none的就是之前的镜像。if [ ! -n "$CID" ]; then echo "get old image id" OLD_IMAGE_IDS=$(docker images --no-trunc| grep none | awk '&#123;print $3&#125;') echo $OLD_IMAGE_IDS if [ -n "$OLD_IMAGE_IDS" ]; then if [ -n $&#123;OLD_IMAGE_IDS[1]&#125; ]; then for OLD_IMAGE_ID in $OLD_IMAGE_IDS do CID=$(docker ps | grep "$&#123;OLD_IMAGE_ID:0:12&#125;" | awk '&#123;print $1&#125;') delete_container $CID done else delete_container $OLD_IMAGE_IDS fi fielse delete_container $CIDfi#启动容器echo "docker run -d -v /mnt:/mnt -p $&#123;PORT&#125;:$&#123;PORT&#125; $IMAGE $OPTIONS"docker run -d -v /mnt:/mnt -p $&#123;PORT&#125;:$&#123;PORT&#125; $IMAGE $OPTIONSecho "clean docker images"#再次清理名称为none的docker镜像。docker images --no-trunc| grep none | awk '&#123;print $3&#125;' | xargs -r docker rmi -f#清理所有已经退出的容器#docker rm `docker ps -a | grep Exited | awk '&#123;print $1&#125;'`echo "finished" 可能最好的方式还是每次用不同的tag来build镜像，不过我这里就偷懒了，等到发布到生产环境的时候再指定吧。注意到&quot;--config.profile=dev --config.host=10.168.248.36&quot;这个参数配合ENTRYPOINT就可以针对生产环境和测试环境加载不同的配置文件了。项目中的配置文件: 1234567891011spring: application: name: user cloud: config: uri: http://$&#123;config.host:192.168.99.100&#125;:8888 profile: $&#123;config.profile:dev&#125; name: userencrypt: failOnError: false 到此我们的一个基本的spring-cloud项目实践就完成了，其他特性和功能自行选择后再添加就可以了。我接下来要继续加入的模块就是使用API网关构建微服务。概念如下 http://www.infoq.com/cn/articles/construct-micro-service-using-api-gateway/]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring cloud项目实践(二)]]></title>
    <url>%2F2016%2F03%2F21%2FSpring-cloud%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[配置中心和服务注册中心我们先把配置中心和服务注册中心跑起来，这个先照着之前的教程做，很简单没什么变动。就是git仓库需要密码的话加入下面的配置就好。 1234567891011121314151617181920server: port: 8888eureka: instance: hostname: configserver client: registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://$&#123;config.host:192.168.99.100&#125;:8761/eureka/spring: cloud: config: server: git: uri: yourgiturl password: **** username: **** 这里的${config.host:192.168.99.100}表示没有读到config.host就用192.168.99.100这个值。 1java -jar cloud-simple-service-1.0.0.jar --config.host=localhost 这个用法就很灵活了，后面配合Dockerfile可以根据不同的环境来启动不同的配置。 微服务应用Mybatishttp://www.cnblogs.com/skyblog/p/5129603.html这篇文章讲了如何配置一个使用myabtis的项目，我们照着他的做就可以了。 Mongodb我这里说一下配置mongodb遇到的问题。首先在pom.xml中加入mongodb的依赖。因为我是用的mongodb3，spring-boot-starter-data-mongodb依赖的驱动是2.0版本的，需要修改一下，加入3.0驱动的依赖。 123456789101112131415161718192021222324&lt;!--mongo驱动版本过低--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt;&lt;/dependency&gt;``` 不过还是认证会出错，解决方案参考下面的文章。 [http://zjumty.iteye.com/blog/2198432](http://zjumty.iteye.com/blog/2198432)照着这个文章做完依然还是有错误，所以我这里还有一些额外的改动，一共自建了3个类。首先在`Application.java`里新加上`MongoAutoConfiguration.class`，`MongoDataAutoConfiguration.class` @EnableAutoConfiguration(exclude = {DataSourceAutoConfiguration.class, MongoAutoConfiguration.class, MongoDataAutoConfiguration.class})123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185自定义的3个类如下 `MongoProperties.java` ```java@ConfigurationProperties(prefix = &quot;spring.data.mongodb&quot;)public class MongoProperties &#123; private static final int DEFAULT_PORT = 27017; /** * Mongo server host. */ private String host; /** * Mongo server port. */ private Integer port = null; /** * Mongo database URI. When set, host and port are ignored. */ private String uri = &quot;mongodb://localhost/test&quot;; /** * Database name. */ private String database; /** * Authentication database name. */ private String authenticationDatabase; /** * GridFS database name. */ private String gridFsDatabase; /** * Login user of the mongo server. */ private String username; /** * Login password of the mongo server. */ private char[] password; public String getHost() &#123; return this.host; &#125; public void setHost(String host) &#123; this.host = host; &#125; public String getDatabase() &#123; return this.database; &#125; public void setDatabase(String database) &#123; this.database = database; &#125; public String getAuthenticationDatabase() &#123; return this.authenticationDatabase; &#125; public void setAuthenticationDatabase(String authenticationDatabase) &#123; this.authenticationDatabase = authenticationDatabase; &#125; public String getUsername() &#123; return this.username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public char[] getPassword() &#123; return this.password; &#125; public void setPassword(char[] password) &#123; this.password = password; &#125; public void clearPassword() &#123; if (this.password == null) &#123; return; &#125; for (int i = 0; i &lt; this.password.length; i++) &#123; this.password[i] = 0; &#125; &#125; public String getUri() &#123; return this.uri; &#125; public void setUri(String uri) &#123; this.uri = uri; &#125; public Integer getPort() &#123; return this.port; &#125; public void setPort(Integer port) &#123; this.port = port; &#125; public String getGridFsDatabase() &#123; return this.gridFsDatabase; &#125; public void setGridFsDatabase(String gridFsDatabase) &#123; this.gridFsDatabase = gridFsDatabase; &#125; public String getMongoClientDatabase() &#123; if (this.database != null) &#123; return this.database; &#125; return new MongoClientURI(this.uri).getDatabase(); &#125; public MongoClient createMongoClient(MongoClientOptions options) throws UnknownHostException &#123; try &#123; if (hasCustomAddress() || hasCustomCredentials()) &#123; if (options == null) &#123; options = MongoClientOptions.builder().build(); &#125; List&lt;MongoCredential&gt; credentials = null; if (hasCustomCredentials()) &#123; String database = this.authenticationDatabase == null ? getMongoClientDatabase() : this.authenticationDatabase; credentials = Arrays.asList(MongoCredential.createScramSha1Credential( this.username, database, this.password)); &#125; String host = this.host == null ? &quot;localhost&quot; : this.host; int port = this.port == null ? DEFAULT_PORT : this.port; return new MongoClient(Arrays.asList(new ServerAddress(host, port)), credentials, options); &#125; // The options and credentials are in the URI return new MongoClient(new MongoClientURI(this.uri, builder(options))); &#125; finally &#123; clearPassword(); &#125; &#125; private boolean hasCustomAddress() &#123; return this.host != null || this.port != null; &#125; private boolean hasCustomCredentials() &#123; return this.username != null &amp;&amp; this.password != null; &#125; private MongoClientOptions.Builder builder(MongoClientOptions options) &#123; MongoClientOptions.Builder builder = MongoClientOptions.builder(); if (options != null) &#123; builder.alwaysUseMBeans(options.isAlwaysUseMBeans()); builder.connectionsPerHost(options.getConnectionsPerHost()); builder.connectTimeout(options.getConnectTimeout()); builder.cursorFinalizerEnabled(options.isCursorFinalizerEnabled()); builder.dbDecoderFactory(options.getDbDecoderFactory()); builder.dbEncoderFactory(options.getDbEncoderFactory()); builder.description(options.getDescription()); builder.maxWaitTime(options.getMaxWaitTime()); builder.readPreference(options.getReadPreference()); builder.socketFactory(options.getSocketFactory()); builder.socketKeepAlive(options.isSocketKeepAlive()); builder.socketTimeout(options.getSocketTimeout()); builder.threadsAllowedToBlockForConnectionMultiplier( options.getThreadsAllowedToBlockForConnectionMultiplier()); builder.writeConcern(options.getWriteConcern()); &#125; return builder; &#125;&#125; 这里跟上面的文章是一样的，就是MongoCredential.createScramSha1Credential这一句不一样而已。而且MongoCredential.createScramSha1Credential这个方法是在3.0的驱动里面才有的。 然后是MongoConfiguration.java 123456789101112131415161718192021222324@Configuration@EnableConfigurationProperties(MongoProperties.class)public class MongoConfiguration &#123; @Autowired private MongoProperties properties; @Autowired(required = false) private MongoClientOptions options; private Mongo mongo; @PreDestroy public void close() &#123; if (this.mongo != null) &#123; this.mongo.close(); &#125; &#125; @Bean public Mongo mongo() throws UnknownHostException &#123; this.mongo = this.properties.createMongoClient(this.options); return this.mongo; &#125;&#125; 这里就引用我们刚才自建的MongoProperties，这样spring在链接mngodb的时候就不会认证出错了。不过我还遇到了了另外一个问题，MongoDataAutoConfiguration引用的MongoProperties也得换成我们自己的，而且升级成3.0的驱动以后，MongoDataAutoConfiguration里面的代码还得修改一下才能正常运行。下面是我修改以后的: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149@Configuration@ConditionalOnClass(&#123;Mongo.class, MongoTemplate.class&#125;)@EnableConfigurationProperties(MongoProperties.class)@AutoConfigureAfter(MongoConfiguration.class)public class MongoDataConfiguration &#123; @Autowired private MongoProperties properties; @Autowired private Environment environment; @Autowired private ResourceLoader resourceLoader; @SuppressWarnings(&quot;deprecation&quot;) @Bean @ConditionalOnMissingBean public MongoDbFactory mongoDbFactory(Mongo mongo) throws Exception &#123; String database = this.properties.getMongoClientDatabase();// String authDatabase = this.properties.getAuthenticationDatabase();// if (StringUtils.hasLength(authDatabase)) &#123;// String username = this.properties.getUsername();// String password = new String(this.properties.getPassword());// UserCredentials credentials = new UserCredentials(username, password);// return new SimpleMongoDbFactory(mongo, database, credentials, authDatabase);// &#125; return new SimpleMongoDbFactory(mongo, database); &#125; @Bean @ConditionalOnMissingBean public MongoTemplate mongoTemplate(MongoDbFactory mongoDbFactory, MongoConverter converter) throws UnknownHostException &#123; return new MongoTemplate(mongoDbFactory, converter); &#125; @Bean @ConditionalOnMissingBean(MongoConverter.class) public MappingMongoConverter mappingMongoConverter(MongoDbFactory factory, MongoMappingContext context, BeanFactory beanFactory) &#123; DbRefResolver dbRefResolver = new DefaultDbRefResolver(factory); MappingMongoConverter mappingConverter = new MappingMongoConverter(dbRefResolver, context); try &#123; mappingConverter .setCustomConversions(beanFactory.getBean(CustomConversions.class)); &#125; catch (NoSuchBeanDefinitionException ex) &#123; // Ignore &#125; return mappingConverter; &#125; @Bean @ConditionalOnMissingBean public MongoMappingContext mongoMappingContext(BeanFactory beanFactory) throws ClassNotFoundException &#123; MongoMappingContext context = new MongoMappingContext(); context.setInitialEntitySet(getInitialEntitySet(beanFactory)); return context; &#125; private Set&lt;Class&lt;?&gt;&gt; getInitialEntitySet(BeanFactory beanFactory) throws ClassNotFoundException &#123; Set&lt;Class&lt;?&gt;&gt; entitySet = new HashSet&lt;Class&lt;?&gt;&gt;(); ClassPathScanningCandidateComponentProvider scanner = new ClassPathScanningCandidateComponentProvider( false); scanner.setEnvironment(this.environment); scanner.setResourceLoader(this.resourceLoader); scanner.addIncludeFilter(new AnnotationTypeFilter(Document.class)); scanner.addIncludeFilter(new AnnotationTypeFilter(Persistent.class)); for (String basePackage : getMappingBasePackages(beanFactory)) &#123; if (StringUtils.hasText(basePackage)) &#123; for (BeanDefinition candidate : scanner .findCandidateComponents(basePackage)) &#123; entitySet.add(ClassUtils.forName(candidate.getBeanClassName(), MongoDataConfiguration.class.getClassLoader())); &#125; &#125; &#125; return entitySet; &#125; private static Collection&lt;String&gt; getMappingBasePackages(BeanFactory beanFactory) &#123; try &#123; return AutoConfigurationPackages.get(beanFactory); &#125; catch (IllegalStateException ex) &#123; // no auto-configuration package registered yet return Collections.emptyList(); &#125; &#125; @Bean @ConditionalOnMissingBean public GridFsTemplate gridFsTemplate(MongoDbFactory mongoDbFactory, MongoTemplate mongoTemplate) &#123; return new GridFsTemplate( new GridFsMongoDbFactory(mongoDbFactory, this.properties), mongoTemplate.getConverter()); &#125; /** * &#123;@link MongoDbFactory&#125; decorator to respect * &#123;@link MongoProperties#getGridFsDatabase()&#125; if set. */ private static class GridFsMongoDbFactory implements MongoDbFactory &#123; private final MongoDbFactory mongoDbFactory; private final MongoProperties properties; public GridFsMongoDbFactory(MongoDbFactory mongoDbFactory, MongoProperties properties) &#123; Assert.notNull(mongoDbFactory, &quot;MongoDbFactory must not be null&quot;); Assert.notNull(properties, &quot;Properties must not be null&quot;); this.mongoDbFactory = mongoDbFactory; this.properties = properties; &#125; @Override public DB getDb() throws DataAccessException &#123; String gridFsDatabase = this.properties.getGridFsDatabase(); if (StringUtils.hasText(gridFsDatabase)) &#123; return this.mongoDbFactory.getDb(gridFsDatabase); &#125; return this.mongoDbFactory.getDb(); &#125; @Override public DB getDb(String dbName) throws DataAccessException &#123; return this.mongoDbFactory.getDb(dbName); &#125; @Override public PersistenceExceptionTranslator getExceptionTranslator() &#123; return this.mongoDbFactory.getExceptionTranslator(); &#125; &#125; @Bean public MongoTemplate syslogMongoTemplate(Mongo mongo) &#123; return new MongoTemplate(mongo, &quot;syslog&quot;); &#125;&#125; 我注释掉了一些代码，然后授权就正常了，估计3.0以后认证方式改了，这些API已经完全被弃用了，使用的话会直接抛异常。 12UserCredentials credentials = new UserCredentials(username, password);return new SimpleMongoDbFactory(mongo, database, credentials, authDatabase); 点进去看看源码 1234567891011121314/** * Create an instance of SimpleMongoDbFactory given the Mongo instance, database name, and username/password * * @param mongo Mongo instance, must not be &#123;@literal null&#125;. * @param databaseName Database name, must not be &#123;@literal null&#125; or empty. * @param credentials username and password. * @param authenticationDatabaseName the database name to use for authentication * @deprecated since 1.7. The credentials used should be provided by &#123;@link MongoClient#getCredentialsList()&#125;. */@Deprecatedpublic SimpleMongoDbFactory(Mongo mongo, String databaseName, UserCredentials credentials, String authenticationDatabaseName) &#123; this(mongo, databaseName, credentials, false, authenticationDatabaseName);&#125; 下面就是针对不同DB不同MongoTemplate的配置了，以后使用的话只需要在相应的类里注入就可以了。1234@Beanpublic MongoTemplate syslogMongoTemplate(Mongo mongo) &#123; return new MongoTemplate(mongo, &quot;syslog&quot;);&#125; 12@Autowiredprivate MongoTemplate syslogMongoTemplate; Mongodb配置信息可以看到在MongoProperties中有一个注解是@ConfigurationProperties(prefix = &quot;spring.data.mongodb&quot;)。spring-boot会默认读取这些配置，由于我们使用了配置中心。所以它也能从配置中心的配置文件中读取到，不需要配置在本地。(我把示例demo中的properties换成了yml的配置方式) 1234567891011121314mysqldb: datasource: url: jdbc\:mysql\://localhost\:3306/test?useUnicode\=true&amp;characterEncoding\=utf-8 username: csst password: csstspring: data: mongodb: host: 10.168.248.36 port: 27017 username: test password: test authenticationDatabase: admin 就这样加在之前配置文件中就好。如果在调试的过程中发现配置没有读取到，可以用下面的方式来查看配置中心是否配置正确并且已经开启服务。http://10.168.248.36:8888/user-dev.ymlhttp://10.168.248.36:8888/user-dev.properties在配置中心的后面加上配置文件的名字可以直接在浏览器中查看。在调试配置中心的时候也可以采取这样的操作，这样你能看到你的git地址和授权错误信息等。 spring-data-mongo提供了一个MongoRepository实现增删改查和复杂查询，在spring-boot中如何指定它使用哪个db呢？如果不配置他默认是使用的test。我测试了一下，加入下面的配置类就可以了。可以扫描一个包。 123456789101112131415161718@Configuration@EnableMongoRepositories(basePackageClasses = ApiLogRepository.class)@AutoConfigureAfter(MongoDataAutoConfiguration.class)public class SysLogDB extends AbstractMongoConfiguration &#123; @Autowired private Mongo mongo; @Override protected String getDatabaseName() &#123; return &quot;syslog&quot;; &#125; @Override public Mongo mongo() throws Exception &#123; return mongo; &#125;&#125; 到这里服务应用项目的mybatis和mongodb都配置好了，可以进行业务代码开发了。没有一个xml配置文件的感觉是不是很爽？下一篇文章讲解如何通过jenkins进行持续集成开发。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring cloud项目实践(一)]]></title>
    <url>%2F2016%2F03%2F21%2FSpring-cloud%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[基本概念和重要组件最近看了一篇文章，了解到微服务架构的组成部分和概念，以前有看过一点dubbo，文章里介绍到Netflix这么一个公司： Netflix是一家成功实践微服务架构的互联网公司，几年前，Netflix就把它的几乎整个微服务框架栈开源贡献给了社区，这些框架和组件包括 Eureka: 服务注册发现框架 Zuul: 服务网关 Karyon: 服务端框架 Ribbon: 客户端框架 Hystrix: 服务容错组件 Archaius: 服务配置组件 Servo: Metrics组件 Blitz4j: 日志组件 Netflix的开源框架组件已经在Netflix的大规模分布式微服务环境中经过多年的生产实战验证，正逐步被社区接受为构造微服务框架的标准组件。Pivotal去年推出的Spring Cloud开源产品，主要是基于对Netflix开源组件的进一步封装，方便Spring开发人员构建微服务基础框架。对于一些打算构建微服务框架体系的公司来说，充分利用或参考借鉴Netflix的开源微服务组件(或Spring Cloud)，在此基础上进行必要的企业定制，无疑是通向微服务架构的捷径。 Spring Cloud是微服务工具包，为开发者提供了在分布式系统的配置管理、服务发现、断路器、智能路由、微代理、控制总线等开发工具包。 觉得挺不错的，就找找资料尝试实战一下。在此记录和回顾一下过程中遇到的问题和实际项目中需要解决的一些问题。接下来我看了2个文章第一个是http://www.kennybastani.com/2015/07/spring-cloud-docker-microservices.html中文版的：http://www.chinacloud.cn/show.aspx?id=20968&amp;cid=12第二个是http://www.cnblogs.com/skyblog/category/774535.html 在继续往下看之前，可以先把上面2个文章看了。第一个是老外写的，用一个实例的demo演示了spring-cloud构建微服务架构的项目。 第二个文章则很好的介绍了spring-cloud各个子项目的作用。也用了一个demo演示了spring-cloud构建的微服务架构的项目。建议也看看动手试试，我这里就不再赘述了。后面的文章都是在基于看过这2篇文章后的基础上写的，很多东西没有再进行二次解释。这里贴一下spring-cloud的子项目。 目前来说spring主要集中于spring boot（用于开发微服务）和spring cloud相关框架的开发，spring cloud子项目包括： Spring Cloud Config：配置管理开发工具包，可以让你把配置放到远程服务器，目前支持本地存储、Git以及Subversion。 Spring Cloud Bus：事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。 Spring Cloud Netflix：针对多种Netflix组件提供的开发工具包，其中包括Eureka、Hystrix、Zuul、Archaius等。 Netflix Eureka：云端负载均衡，一个基于 REST 的服务，用于定位服务，以实现云端的负载均衡和中间层服务器的故障转移。 Netflix Hystrix：容错管理工具，旨在通过控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。 Netflix Zuul：边缘服务工具，是提供动态路由，监控，弹性，安全等的边缘服务。 Netflix Archaius：配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。 Spring Cloud for Cloud Foundry：通过Oauth2协议绑定服务到CloudFoundry，CloudFoundry是VMware推出的开源PaaS云平台。 Spring Cloud Sleuth：日志收集工具包，封装了Dapper,Zipkin和HTrace操作。 Spring Cloud Data Flow：大数据操作工具，通过命令行方式操作数据流。 Spring Cloud Security：安全工具包，为你的应用程序添加安全控制，主要是指OAuth2。 Spring Cloud Consul：封装了Consul操作，consul是一个服务发现与配置工具，与Docker容器可以无缝集成。 Spring Cloud Zookeeper：操作Zookeeper的工具包，用于使用zookeeper方式的服务注册和发现。 Spring Cloud Stream：数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。 Spring Cloud CLI：基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。 实践在了解了微服务的概念，以及spring-cloud各个子项目之间的关系后。接下来我们就自己动手构建一个项目，做一个能跑起来的项目实际上需要3个模块： Spring Cloud Config Spring Cloud Eureka 自己的项目 在配置好这几个项目后，我会用jenkins自动build项目，然后发布到docker中再启动容器。其中还可以针对生产和测试环境采用不同的配置。我的服务器环境是centos6.5。 我的服务应用目前做了以下2个配置。1.mongodb2.mybatis 下篇文章详细讲解。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins配合docker持续集成]]></title>
    <url>%2F2015%2F12%2F15%2Fjenkins%E9%85%8D%E5%90%88docker%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%2F</url>
    <content type="text"><![CDATA[我这里用docker来做持续集成的思路和jenkins和tomcat的持续集成是一样的。都是用jenkins拉取git的代码然后打war包，只不过重启tomcat步骤换成了docker的重启。直接展示一下docker的脚本吧。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#!/bin/bash#defined export JAVA_HOME=/usr/java/jdk1.8.0_40/REGISTRY_URL=localhost:5000WEB_DIR=&quot;$WEB_DIR/webapps&quot;PORT=&quot;8080&quot;IMAGE=&quot;$1&quot;PROJECT=&quot;$2&quot;#param validateif [ $# -lt 2 ]; then echo &quot;you must use like this : ./deploy_docker.sh &lt;image&gt; &lt;project&gt; [war dir] [port]&quot; exitfiif [ &quot;$3&quot; != &quot;&quot; ]; then WEB_DIR=&quot;$3&quot;fiif [ &quot;$4&quot; != &quot;&quot; ]; then PORT=&quot;$4&quot;fi#publish projectecho &quot;delete old $PROJECT.war&quot;rm -rf &quot;$WEB_DIR&quot;/webapps/$PROJECTecho &quot;copy new $PROJECT.war&quot;cp $WEB_DIR/$PROJECT.war &quot;$WEB_DIR&quot;/webapps/$PROJECT.war#bak projectBAK_DIR=$WEB_DIR/bak/$PROJECT/`date +%Y%m%d`mkdir -p &quot;$BAK_DIR&quot;cp &quot;$WEB_DIR&quot;/$PROJECT.war &quot;$BAK_DIR&quot;/&quot;$PROJECT&quot;_`date +%H%M%S`.war#remove tmprm -rf $WEB_DIR/$PROJECT.warecho &quot;build image:&quot; $IMAGEdocker build -t $REGISTRY_URL/$IMAGE $WEB_DIRecho &quot;push image:&quot; $IMAGEdocker push $REGISTRY_URL/$IMAGEecho &apos;&gt;&gt;&gt; Get old container id&apos;CID=`docker ps | grep &quot;dev&quot; | awk &apos;&#123;print $1&#125;&apos;`if [ -n &quot;$CID&quot; ]; then echo &quot;delete container:&quot; $CID docker stop $CID docker rm -f $CIDfiecho &quot;delete local image:&quot; $IMAGEdocker rmi -f $REGISTRY_URL/$IMAGEdocker run -d -p $PORT:8080 -v /mnt:/mnt --name dev $REGISTRY_URL/$IMAGEecho &quot;finished&quot; 上面是步骤是这样： jenkins将war包上传到$WEB_DIR目录下，然后执行脚本。 这个目录下有一个webapps目录，用来存放正在运行的项目war包，是从$WEB_DIR拷贝过去的。 然后build docker的镜像，并push到私库中。docker build -t $REGISTRY_URL/$IMAGE $WEB_DIR这里最后一个参数是Dockerfile的路径，我在这个目录下还写了一个docker的配置。 停止之前正在运行的Container。 删除本地的镜像。 启动镜像：-v /mnt:/mnt是要映射宿主机的的目录(保存错误日志到宿主机)。--name dev是容器的名字，这个自行修改。docker ps | grep &quot;dev&quot; | awk &#39;{print $1}&#39;这里的dev也需要更换。因为删除了本地的镜像，所以会去私库重新里面下载。 这个脚本有一点问题，local的image有时候删除不掉，不影响运行，但是随着编译的次数，存储空间会越用越多（因为我最终没有采用docker，所以没去深究了）。 还有Dockerfile如下： 12FROM tomcat:8.0.30-jre8ADD webapps /usr/local/tomcat/webapps 基于官方的tomcat8构建，并且把webapps下的文件拷贝到容器中的tomcat webapps下，这个webapps必须是Dockerfile同级目录下的，也就是$WEB_DIR这个目录下。 还有另外一种方式操作起来更加简单，就是映射宿主机的路径直接写成-v $WEB_DIR/webapps:/usr/local/tomcat/webapps，这样还省去了build时拷贝项目的过程。 但是我觉得都不是很方便，查看日志不太方便，而且docker编译和启动也比较耗时，相比直接kill掉tomcat重启要慢一些。目前没有找到一个更好的实践方式，需要慢慢探索一下，为了不影响开发，我又换回了之前直接kill tomcat的方式。]]></content>
      <categories>
        <category>CI</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins和tomcat的持续集成]]></title>
    <url>%2F2015%2F12%2F14%2Fjenkins%E5%92%8Ctomcat%E7%9A%84%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%2F</url>
    <content type="text"><![CDATA[今天来说一下如何用jenkins做持续集成。jenkins我这里就不介绍了，重点介绍一下如何用jenkins对tomcat进行持续集成。 思路流程是这样的： jenkins从git(or svn)拉取代码，进行构建。 将打出来的war包用jenkins的插件(Publish over SSH)传到你要部署的服务器。 执行一个shell脚本，将正在运行的tomcat进程kill掉，把war包拷到tomcat目录的webapps下。然后在运行${TOMCAT_DIR}/bin/startup.sh。 是不是很简单？这样我们在发布应用的时候就再也不需要每次先在本地打包，再手动去删除之前的项目，然后通过ftp工具上传到服务器上，最后再重启tomcat。如果一天要进行几十次类似的操作，其实还是非常耗时的。 实践配置jenkins拉取git代码首先配置在jenkins中配置好，然后再去gitlab里面配上hook触发点。我这里是配置的当gitlab中merge的时候触发构建操作。 用Maven打包应用这个很简单： 上传包到服务器并执行脚本这里需要在jenkins里安装Publish over SSH插件，并在全局配置中配置一下。 然后下面是项目中的配置 上图中的war其实是基于全局配置中的路径，举个例子： 全局配置中的Remote Directory是 /web，项目配置中的Remote directory是war，那么实际上war传送的路径是/web/war 最后就是最重要的脚本 脚本脚本的目的就是kill掉当前tomcat的进程，复制war包，启动tomcat。脚本如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bash#defined export JAVA_HOME=/usr/java/jdk1.8.0_40/TOMCAT_HOME="/mnt/web/apache-tomcat-7.0.54"TOMCAT_PORT=8080PROJECT="$1"#param validateif [ $# -lt 1 ]; then echo "you must use like this : ./deploy.sh &lt;projectname&gt; [tomcat port] [tomcat home dir]" exitfiif [ "$2" != "" ]; then TOMCAT_PORT=$2fiif [ "$3" != "" ]; then TOMCAT_HOME="$3"fi#shutdown tomcat#"$TOMCAT_HOME"/bin/shutdown.sh#echo "tomcat shutdown"#check tomcat processtomcat_pid=`/usr/sbin/lsof -n -P -t -i :$TOMCAT_PORT`echo "current :" $tomcat_pidwhile [ -n "$tomcat_pid" ]do sleep 5 tomcat_pid=`/usr/sbin/lsof -n -P -t -i :$TOMCAT_PORT` echo "scan tomcat pid :" $tomcat_pid if [ -n "$tomcat_pid" ]; then echo "kill tomcat :" $tomcat_pid kill -9 $tomcat_pid fidone#publish projectecho "scan no tomcat pid,$PROJECT publishing"rm -rf "$TOMCAT_HOME"/webapps/$PROJECTcp /yourwarpath/$PROJECT.war "$TOMCAT_HOME"/webapps/$PROJECT.war#bak projectBAK_DIR=/yourwarpath/bak/$PROJECT/`date +%Y%m%d`mkdir -p "$BAK_DIR"cp "$TOMCAT_HOME"/webapps/$PROJECT.war "$BAK_DIR"/"$PROJECT"_`date +%H%M%S`.war#remove tmprm -rf /yourwarpath/$PROJECT.war#start tomcat"$TOMCAT_HOME"/bin/startup.shecho "tomcat is starting,please try to access $PROJECT conslone url" PS还有一种方式是通过jenkins的deploy plugin来部署应用，我最早的时候用过一段时间这种方式。不知道是什么原因，部署多次之后会导致内存溢出，每部署一次服务器被占用的内存就多一点。最终导致服务器崩溃，ssh都连接不上去，所以最后换了shell脚本的方案。]]></content>
      <categories>
        <category>CI</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis 枚举存储]]></title>
    <url>%2F2015%2F12%2F11%2FMybatis-%E6%9E%9A%E4%B8%BE%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[枚举我们在写程序的时候会遇到这种需求。就是我的对象里面有一个属性是一个枚举值，但是mybatis默认是不支持的，官方提供了一个typeHandler可以用枚举的ordinal()来进行存和取的自动转换。把它配置在mybatis-configuration.xml里。 123&lt;typeHandlers&gt; &lt;typeHandler handler=&quot;org.apache.ibatis.type.EnumOrdinalTypeHandler&quot; javaType=&quot;com.xxx.user.UserType&quot;/&gt;&lt;/typeHandlers&gt; 问题但是这里有一些问题，必须如果数据库里面存在了别的数字，举个例，有以下枚举 123public enum UserType&#123; ADMIN, EDITOR&#125; 这个枚举在数据库中对应的数字应该是0和1，问题如下 枚举写的顺序不能变，否则数据库数据会错乱 枚举序数中间不能中断(0，2) 数据库里有除了0和1之外的数字，在查询数据的时候程序会得到一个异常(ArrayIndexOutOfBoundsException) 最怕的就是程序出异常了，这里ArrayIndexOutOfBoundsException的原因是因为EnumOrdinalTypeHandler的代码大致是下面这个的意思。 UserType.values()[i] 所以就出现了我们写代码其实并不经常会遇到的ArrayIndexOutOfBoundsException 解决方案为了避免出现这些情况，有个简单的办法就是重写一个EnumOrdinalTypeHandler，我这里贴一下我的解决方案。首先要为所有枚举写一个接口，为了获取枚举对应的intValue,代码如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public interface CommonEnum&lt;E&gt; &#123; int getValue(); /** * 获取枚举值对应的枚举 * * @param enumClass 枚举类 * @param enumValue 枚举值 * @return 枚举 */ static &lt;E extends CommonEnum&lt;E&gt;&gt; E getEnum(final Class&lt;E&gt; enumClass, final Integer enumValue) &#123; if (enumValue == null) &#123; return null; &#125; try &#123; return valueOf(enumClass, enumValue); &#125; catch (final IllegalArgumentException ex) &#123; return null; &#125; &#125; /** * 获取枚举值对应的枚举 * * @param enumClass 枚举类 * @param enumValue 枚举值 * @return 枚举 */ static &lt;E extends CommonEnum&gt; E valueOf(Class&lt;E&gt; enumClass, Integer enumValue) &#123; if (enumValue == null) throw new NullPointerException("EnumValue is null"); return getEnumMap(enumClass).get(enumValue); &#125; /** * 获取枚举键值对 * * @param enumClass 枚举类型 * @return 键值对 */ static &lt;E extends CommonEnum&gt; Map&lt;Integer, E&gt; getEnumMap(Class&lt;E&gt; enumClass) &#123; E[] enums = enumClass.getEnumConstants(); if (enums == null) throw new IllegalArgumentException(enumClass.getSimpleName() + " does not represent an enum type."); Map&lt;Integer, E&gt; map = new HashMap&lt;&gt;(2 * enums.length); for (E t : enums)&#123; map.put(t.getValue(), t); &#125; return map; &#125;&#125; 上面3个静态方法也可以提取到工具类中，我这里偷了一下懒，也因为我是用的JDK8。枚举实例如下： 123456789101112131415public enum UserType implements CommonEnum&lt;UserType&gt; &#123; ADMIN(0), EDITOR(2); private int value; UserType(int value) &#123; this.value = value; &#125; @Override public int getValue() &#123; return this.value; &#125;&#125; 这里就是一个很常见的枚举，重点在下面的typeHandler。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class CustomEnumTypeHandler&lt;E extends CommonEnum&lt;E&gt;&gt; extends BaseTypeHandler&lt;E&gt; &#123; private Class&lt;E&gt; type; public CustomEnumTypeHandler(Class&lt;E&gt; type) &#123; if (type == null) throw new IllegalArgumentException("Type argument cannot be null"); this.type = type; E[] enums = type.getEnumConstants(); if (enums == null) throw new IllegalArgumentException(type.getSimpleName() + " does not represent an enum type."); &#125; @Override public void setNonNullParameter(PreparedStatement ps, int i, E parameter, JdbcType jdbcType) throws SQLException &#123; ps.setInt(i, parameter.getValue()); &#125; @Override public E getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; int i = rs.getInt(columnName); if (rs.wasNull()) &#123; return null; &#125; else &#123; try &#123; return CommonEnum.getEnum(type, i); &#125; catch (Exception ex) &#123; throw new IllegalArgumentException("Cannot convert " + i + " to " + type.getSimpleName() + " by int value.", ex); &#125; &#125; &#125; @Override public E getNullableResult(ResultSet rs, int columnIndex) throws SQLException &#123; int i = rs.getInt(columnIndex); if (rs.wasNull()) &#123; return null; &#125; else &#123; try &#123; return CommonEnum.getEnum(type, i); &#125; catch (Exception ex) &#123; throw new IllegalArgumentException("Cannot convert " + i + " to " + type.getSimpleName() + " by int value.", ex); &#125; &#125; &#125; @Override public E getNullableResult(CallableStatement cs, int columnIndex) throws SQLException &#123; int i = cs.getInt(columnIndex); if (cs.wasNull()) &#123; return null; &#125; else &#123; try &#123; return CommonEnum.getEnum(type, i); &#125; catch (Exception ex) &#123; throw new IllegalArgumentException("Cannot convert " + i + " to " + type.getSimpleName() + " by int value.", ex); &#125; &#125; &#125;&#125; 这个类基本上也是拷的EnumOrdinalTypeHandler。重要的改动的代码如下： ps.setInt(i, parameter.getValue()); return CommonEnum.getEnum(type, i); 第一句是插入和更新的时候用到的，第二句是查询的时候用到的，最后把mybatis-configuration.xml里的改一下。 123&lt;typeHandlers&gt; &lt;typeHandler handler="xxx.CustomEnumTypeHandler" javaType="com.xxx.user.UserType"/&gt;&lt;/typeHandlers&gt;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos 6.5 安装docker]]></title>
    <url>%2F2015%2F12%2F08%2Fcentos-6-5-%E5%AE%89%E8%A3%85docker%2F</url>
    <content type="text"><![CDATA[先试试把开环境用docker来部署。 http://docs.docker.com/engine/installation/centos/直接按照这里安装就可以了，不过我在这里遇到了一点问题。我是centos6.5，内核在 3.8 以上通过以下命令查看您的 CentOS 内核： uname -r 如果执行以上命令后，输出的内核版本号低于 3.8，请参考下面的方法来来升级您的 Linux 内核。 对于 CentOS 6.5 而言，内核版本默认是 2.6。首先，可通过以下命令安装最新内核： 123rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -ivh http://www.elrepo.org/elrepo-release-6-5.el6.elrepo.noarch.rpmyum -y --enablerepo=elrepo-kernel install kernel-lt 随后，编辑以下配置文件： 1vi /etc/grub.conf 将default=1修改为default=0。最后，通过reboot命令重启操作系统。 重启后如果不出意外的话，再次查看内核，您的 CentOS 内核将会显示为 3.10。 如果到这里，您和我们所期望的结果是一致的。恭喜您！下面我们就一起来安装 Docker 了。 接下来按照官网文档的步骤安装 12345678910sudo yum updatesudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOFsudo yum install docker-engine 我在这里遇到了下面的一个问题： Error: docker-engine conflicts with docker-io-1.7.1-2.el6.x86_64 查了一会发现是因为docker-io改名为docker-engine，所以造成冲突了，我这台服务器还安装过以前版本的，执行以下命令来删除老的版本。 yum remove docker-io 然后再来安装 sudo yum install docker-engine 就可以安装成功了，继续下一步。我试着部署一个tomcat，执行以下命令下载一个centos镜像 docker pull centos 然后我发现每次命令都要用sudo，很不方便 原来默认安装完 docker 后，每次执行 docker 都需要运行 sudo 命令，非常浪费时间影响效率。如果不跟 sudo，直接执行 docker images 命令会有如下问题： FATA[0000] Get http:///var/run/docker.sock/v1.18/images/json: dial unix /var/run/docker.sock: permission denied. Are you trying to connect to a TLS-enabled daemon without TLS? 参考这里 http://bsaunder.github.io/2014/12/21/running-docker-without-sudo/执行以下命令来解决 sudo groupadd docker 将用户加入该 group 内。 sudo gpasswd -a ${USER} docker 重启docker sudo service docker restart 切换当前会话到新group (如果想立即生效此步不可少，因为 groups 命令获取到的是缓存的组信息，刚添加的组信息未能生效，所以 docker images 执行时同样有错。) newgrp - docker 还有就是docker的官方镜像下载实在是太慢了，找到一个国内提供加速服务的daocloud 注册登录后使用加速器功能就可以了 docker默认的images存放路径是/var/lib/docker我的服务器系统硬盘自带的容量很小所以我要修改他的存放路径，修改下面文件里的other_args参数，重启docker。 sudo vim /etc/sysconfig/docker other_args=&quot;--graph=yourpath&quot; 下面还有一个私库的问题，不用localhost访问出现了： 123unable to ping registry endpoint https://10.168.248.36:5000/v0/v2 ping attempt failed with error: Get https://10.168.248.36:5000/v2/: tls: oversized record received with length 20527 v1 ping attempt failed with error: Get https://10.168.248.36:5000/v1/_ping: tls: oversized record received with length 20527 依然是修改/etc/sysconfig/docker里面的other_args，加上部署私库的机器的IP。 --insecure-registry=yourip:5000 这样就可以正常的push和pull了]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab替换nginx服务]]></title>
    <url>%2F2015%2F10%2F29%2Fgitlab%E6%9B%BF%E6%8D%A2nginx%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[之前在自己服务器上搭建服务器，这是之前的文章gitlab安装需要注意的问题 后来在按照官网上替换自带服务器上的nginx的时候出现了一点问题。也是找了很久才找到这么一篇帖子，我在这里把nginx的部分转载过来一下。原文地址 其次，我替换自己的nginx服务器的时候，nginx官方提供的包并不带gitlab要求的passenger模块，所以不能直接用官方提供的方法。我是用gitlab-ctl reconfigure生成了nginx的配置以后复制到自己的nginx里去的。生成的配置在 /var/opt/gitlab/nginx/conf/gitlab-http.conf 还是nginx，我的nginx的启动账户不是gitlab的（默认是gitlab-www），所以会出现502错误。日志里内容是访问fastcgi权限不足。所以还要chmod 755 /var/opt/gitlab/gitlab-rails/sockets]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]Twitter的分布式自增ID算法Snowflake实现分析及其Java、Php和Python版]]></title>
    <url>%2F2015%2F04%2F22%2F-%E8%BD%AC-Twitter%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%87%AA%E5%A2%9EID%E7%AE%97%E6%B3%95Snowflake%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90%E5%8F%8A%E5%85%B6Java%E3%80%81Php%E5%92%8CPython%E7%89%88%2F</url>
    <content type="text"><![CDATA[转载：http://www.dengchuanhua.com/132.html 在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位+机器ID 10位+毫秒内序列12位。 该项目地址为：https://github.com/twitter/snowflake是用Scala实现的。 python版详见开源项目https://github.com/erans/pysnowflake。 核心代码为其IdWorker这个类实现，其原理结构如下，我分别用一个0表示一位，用—分割开部分的作用： 10---0000000000 0000000000 0000000000 0000000000 0 --- 00000 ---00000 ---0000000000 00 在上面的字符串中，第一位为未使用（实际上也可作为long的符号位），接下来的41位为毫秒级时间，然后5位datacenter标识位，5位机器ID（并不算标识符，实际是为线程标识），然后12位该毫秒内的当前毫秒内的计数，加起来刚好64位，为一个Long型。 这样的好处是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和机器ID作区分），并且效率较高，经测试，snowflake每秒能够产生26万ID左右，完全满足需要。且看其核心代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&lt;/pre&gt;/** Copyright 2010-2012 Twitter, Inc.*/package com.twitter.service.snowflakeimport com.twitter.ostrich.stats.Statsimport com.twitter.service.snowflake.gen._import java.util.Randomimport com.twitter.logging.Logger/** * An object that generates IDs. * This is broken into a separate class in case * we ever want to support multiple worker threads * per process */class IdWorker(val workerId: Long, val datacenterId: Long, private val reporter: Reporter, var sequence: Long = 0L)extends Snowflake.Iface &#123; private[this] def genCounter(agent: String) = &#123; Stats.incr("ids_generated") Stats.incr("ids_generated_%s".format(agent)) &#125; private[this] val exceptionCounter = Stats.getCounter("exceptions") private[this] val log = Logger.get private[this] val rand = new Random val twepoch = 1288834974657L //机器标识位数 private[this] val workerIdBits = 5L//数据中心标识位数 private[this] val datacenterIdBits = 5L//机器ID最大值 private[this] val maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits)//数据中心ID最大值 private[this] val maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits)//毫秒内自增位 private[this] val sequenceBits = 12L//机器ID偏左移12位 private[this] val workerIdShift = sequenceBits//数据中心ID左移17位 private[this] val datacenterIdShift = sequenceBits + workerIdBits//时间毫秒左移22位 private[this] val timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits private[this] val sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits) private[this] var lastTimestamp = -1L // sanity check for workerId if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; exceptionCounter.incr(1) throw new IllegalArgumentException("worker Id can't be greater than %d or less than 0".format(maxWorkerId)) &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; exceptionCounter.incr(1) throw new IllegalArgumentException("datacenter Id can't be greater than %d or less than 0".format(maxDatacenterId)) &#125; log.info("worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d", timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId) def get_id(useragent: String): Long = &#123; if (!validUseragent(useragent)) &#123; exceptionCounter.incr(1) throw new InvalidUserAgentError &#125; val id = nextId() genCounter(useragent) reporter.report(new AuditLogEntry(id, useragent, rand.nextLong)) id &#125; def get_worker_id(): Long = workerId def get_datacenter_id(): Long = datacenterId def get_timestamp() = System.currentTimeMillis protected[snowflake] def nextId(): Long = synchronized &#123; var timestamp = timeGen() //时间错误 if (timestamp &lt; lastTimestamp) &#123; exceptionCounter.incr(1) log.error("clock is moving backwards. Rejecting requests until %d.", lastTimestamp); throw new InvalidSystemClock("Clock moved backwards. Refusing to generate id for %d milliseconds".format( lastTimestamp - timestamp)) &#125; if (lastTimestamp == timestamp) &#123;//当前毫秒内，则+1 sequence = (sequence + 1) &amp; sequenceMask if (sequence == 0) &#123;//当前毫秒内计数满了，则等待下一秒 timestamp = tilNextMillis(lastTimestamp) &#125; &#125; else &#123; sequence = 0 &#125; lastTimestamp = timestamp//ID偏移组合生成最终的ID，并返回ID ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | (datacenterId &lt;&lt; datacenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence &#125;//等待下一个毫秒的到来 protected def tilNextMillis(lastTimestamp: Long): Long = &#123; var timestamp = timeGen() while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen() &#125; timestamp &#125; protected def timeGen(): Long = System.currentTimeMillis() val AgentParser = """([a-zA-Z][a-zA-Z\-0-9]*)""".r def validUseragent(useragent: String): Boolean = useragent match &#123; case AgentParser(_) =&gt; true case _ =&gt; false &#125;&#125;&lt;pre&gt; 上述为twitter的实现，下面且看一个Java实现，貌似为淘宝的朋友写的。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class IdWorker &#123; private final long workerId; private final static long twepoch = 1361753741828L; private long sequence = 0L; private final static long workerIdBits = 4L; public final static long maxWorkerId = -1L ^ -1L &lt;&lt; workerIdBits; private final static long sequenceBits = 10L; private final static long workerIdShift = sequenceBits; private final static long timestampLeftShift = sequenceBits + workerIdBits; public final static long sequenceMask = -1L ^ -1L &lt;&lt; sequenceBits; private long lastTimestamp = -1L; public IdWorker(final long workerId) &#123; super(); if (workerId &gt; this.maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format( "worker Id can't be greater than %d or less than 0", this.maxWorkerId)); &#125; this.workerId = workerId; &#125; public synchronized long nextId() &#123; long timestamp = this.timeGen(); if (this.lastTimestamp == timestamp) &#123; this.sequence = (this.sequence + 1) &amp; this.sequenceMask; if (this.sequence == 0) &#123; System.out.println("###########" + sequenceMask); timestamp = this.tilNextMillis(this.lastTimestamp); &#125; &#125; else &#123; this.sequence = 0; &#125; if (timestamp &lt; this.lastTimestamp) &#123; try &#123; throw new Exception( String.format( "Clock moved backwards. Refusing to generate id for %d milliseconds", this.lastTimestamp - timestamp)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; this.lastTimestamp = timestamp; long nextId = ((timestamp - twepoch &lt;&lt; timestampLeftShift)) | (this.workerId &lt;&lt; this.workerIdShift) | (this.sequence);// System.out.println("timestamp:" + timestamp + ",timestampLeftShift:"// + timestampLeftShift + ",nextId:" + nextId + ",workerId:"// + workerId + ",sequence:" + sequence); return nextId; &#125; private long tilNextMillis(final long lastTimestamp) &#123; long timestamp = this.timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = this.timeGen(); &#125; return timestamp; &#125; private long timeGen() &#123; return System.currentTimeMillis(); &#125; public static void main(String[] args)&#123; IdWorker worker2 = new IdWorker(2); System.out.println(worker2.nextId()); &#125;&#125; 再来看一个php的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&lt;?phpclass Idwork&#123;const debug = 1;static $workerId;static $twepoch = 1361775855078;static $sequence = 0;const workerIdBits = 4;static $maxWorkerId = 15;const sequenceBits = 10;static $workerIdShift = 10;static $timestampLeftShift = 14;static $sequenceMask = 1023;private static $lastTimestamp = -1;function __construct($workId)&#123;if($workId &gt; self::$maxWorkerId || $workId&lt; 0 )&#123;throw new Exception("worker Id can't be greater than 15 or less than 0");&#125;self::$workerId=$workId;echo 'logdebug-&gt;__construct()-&gt;self::$workerId:'.self::$workerId;echo '&lt;/br&gt;';&#125;function timeGen()&#123;//获得当前时间戳$time = explode(' ', microtime());$time2= substr($time[0], 2, 3);$timestramp = $time[1].$time2;echo 'logdebug-&gt;timeGen()-&gt;$timestramp:'.$time[1].$time2;echo '&lt;/br&gt;';return $time[1].$time2;&#125;function tilNextMillis($lastTimestamp) &#123;$timestamp = $this-&gt;timeGen();while ($timestamp &lt;= $lastTimestamp) &#123;$timestamp = $this-&gt;timeGen();&#125;echo 'logdebug-&gt;tilNextMillis()-&gt;$timestamp:'.$timestamp;echo '&lt;/br&gt;';return $timestamp;&#125;function nextId()&#123;$timestamp=$this-&gt;timeGen();echo 'logdebug-&gt;nextId()-&gt;self::$lastTimestamp1:'.self::$lastTimestamp;echo '&lt;/br&gt;';if(self::$lastTimestamp == $timestamp) &#123;self::$sequence = (self::$sequence + 1) &amp; self::$sequenceMask;if (self::$sequence == 0) &#123; echo "###########".self::$sequenceMask; $timestamp = $this-&gt;tilNextMillis(self::$lastTimestamp); echo 'logdebug-&gt;nextId()-&gt;self::$lastTimestamp2:'.self::$lastTimestamp; echo '&lt;/br&gt;'; &#125;&#125; else &#123;self::$sequence = 0; echo 'logdebug-&gt;nextId()-&gt;self::$sequence:'.self::$sequence; echo '&lt;/br&gt;';&#125;if ($timestamp &lt; self::$lastTimestamp) &#123; throw new Excwption("Clock moved backwards. Refusing to generate id for ".(self::$lastTimestamp-$timestamp)." milliseconds"); &#125;self::$lastTimestamp = $timestamp;echo 'logdebug-&gt;nextId()-&gt;self::$lastTimestamp3:'.self::$lastTimestamp;echo '&lt;/br&gt;';echo 'logdebug-&gt;nextId()-&gt;(($timestamp - self::$twepoch &lt;&lt; self::$timestampLeftShift )):'.((sprintf('%.0f', $timestamp) - sprintf('%.0f', self::$twepoch) ));echo '&lt;/br&gt;';$nextId = ((sprintf('%.0f', $timestamp) - sprintf('%.0f', self::$twepoch) )) | ( self::$workerId &lt;&lt; self::$workerIdShift ) | self::$sequence;echo 'timestamp:'.$timestamp.'-----';echo 'twepoch:'.sprintf('%.0f', self::$twepoch).'-----';echo 'timestampLeftShift ='.self::$timestampLeftShift.'-----';echo 'nextId:'.$nextId.'----';echo 'workId:'.self::$workerId.'-----';echo 'workerIdShift:'.self::$workerIdShift.'-----';return $nextId;&#125;&#125;$Idwork = new Idwork(1);$a= $Idwork-&gt;nextId();$Idwork = new Idwork(2);$a= $Idwork-&gt;nextId();?&gt;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>分布式自增ID算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC 接口版本管理]]></title>
    <url>%2F2015%2F03%2F31%2FSpring-MVC-%E6%8E%A5%E5%8F%A3%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[随着我们的应用后台不断的发版，因为改动导致了数据结构的变化，这个时候就需要对HTTP API进行版本控制了。对原有的客户端进行兼容，搜索一番后找到一个方法。先看这个文章，提供了一个解决方案。http://www.cnblogs.com/jcli/p/springmvc_restful_version.html Spring MVC通过在方法上使用RequestMapping来确认应该使用哪个方法来响应相应的请求，而RequestMapping又通过各种RequestCondition的实现来完成各种过滤（比如：consumes，headers，methods，params，produces以及value等）。在Spring MVC框架中使用RequestConditionHolder和RequestMappingInfo这两个实现。 自定义RequestCondition 实现RequestCondition接口 12345678910package org.springframework.web.servlet.mvc.condition;import javax.servlet.http.HttpServletRequest;import org.springframework.web.bind.annotation.RequestMapping;public interface RequestCondition&lt;T&gt; &#123; T combine(T other); T getMatchingCondition(HttpServletRequest request); int compareTo(T other, HttpServletRequest request);&#125; 继承RequestMappingHandlerMapping getCustomTypeCondition方法根据对应的Handler类返回类级别的condition getCustomMethodCondition方法根据对应的Handler方法返回方法级别的condition 基本上我是照着他做的，不过我这里也是遇到不少的问题，因为数据是直接post的json，需要转换为实体对象，所以还需要一些额外的配置。文中提到： 最后，得让SpringMVC加载我们定义的CustomRequestMappingHandlerMapping以覆盖原先的RequestMappingHandlerMapping, 所以要去掉前面说的mvc:annotation-driven/这个配置，我们通过JavaConfig的方式注入 我是不太愿意去掉&lt;mvc:annotation-driven/&gt;的，不过试了半天也没有好的效果，因为&lt;mvc:annotation-driven/&gt;注册的东西太多了。我尝试直接写一个org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping 一样包名的类来覆盖掉spring的类，来改掉getCustomTypeCondition和getCustomMethodCondition的方法实现。现在来看应该是可行的，但我没有这样干是因为中途遇见一个问题一直没调试好，最终又换成了自定义的类。这个问题就是因为客户端目前的版本号全部是在post的json中传过来的，就不考虑在路径上做改动。所以我出现了这样的操作，在没找到问题之前我一直以为我重写的类这一种方式有问题，看代码 123456789101112public ApiVersionCondition getMatchingCondition(HttpServletRequest request) &#123; String device = null; try &#123; device = JSONUtil.parse(request.getInputStream()).getString("device"); &#125; catch (IOException e) &#123; log.error(e.getMessage(), e); &#125; int version = VersionUtil.getVersion(JSON.parseObject(device).getString("app_ver")); if (version &gt;= this.apiVersion) // 如果请求的版本号大于配置版本号， 则满足 return this; return null;&#125; 这个就是把post过来的json数据取出来，然后取出里面的version进行判断，不过我得到这样一个错误，看了很久也没看懂1234567891011121314org.springframework.http.converter.HttpMessageNotReadableException: Required request body content is missing: org.springframework.web.method.HandlerMethod$HandlerMethodParameter@bee0537e at org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor.handleEmptyBody(RequestResponseBodyMethodProcessor.java:189) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor.readWithMessageConverters(RequestResponseBodyMethodProcessor.java:170) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor.resolveArgument(RequestResponseBodyMethodProcessor.java:105) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.method.support.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:77) ~[spring-web-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:162) ~[spring-web-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:129) ~[spring-web-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877) ~[spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966) [spring-webmvc-4.1.3.RELEASE.jar:4.1.3.RELEASE] 然后才发现我在这里把inputStream读了以后，到controller那一层已经没有任何数据了。基本上是算得上自己作死加犯傻了。结果还是采取的在Http Header里面放一个版本号来进行判断。12345678public ApiVersionCondition getMatchingCondition(HttpServletRequest request) &#123; int version = VersionUtil.getVersion(request.getHeader(&quot;App-Version&quot;)); if (version &gt;= this.apiVersion) // 如果请求的版本号大于配置版本号， 则满足 return this; return null;&#125; 说一下配置的地方，我没有用WebConfig的配置方式，但还是去掉了&lt;mvc:annotation-driven&gt;换成了几个bean。下面贴上我的配置123456789101112131415161718192021222324252627282930&lt;!--RequestMapping解析器--&gt;&lt;bean class=&quot;com.xiaomaihd.xueshaqu.version.CustomRequestMappingHandlerMapping&quot;&gt; &lt;property name=&quot;order&quot; value=&quot;0&quot;/&gt; &lt;property name=&quot;interceptors&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;conversionServiceExposingInterceptor&quot;/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean class=&quot;org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter&quot;&gt; &lt;property name=&quot;webBindingInitializer&quot;&gt; &lt;bean class=&quot;org.springframework.web.bind.support.ConfigurableWebBindingInitializer&quot;&gt; &lt;property name=&quot;conversionService&quot; ref=&quot;conversionService&quot;/&gt; &lt;property name=&quot;validator&quot; ref=&quot;validator&quot;/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name=&quot;messageConverters&quot; ref=&quot;messageConverters&quot;&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;conversionService&quot; class=&quot;org.springframework.format.support.FormattingConversionServiceFactoryBean&quot;/&gt;&lt;bean id=&quot;conversionServiceExposingInterceptor&quot; class=&quot;org.springframework.web.servlet.handler.ConversionServiceExposingInterceptor&quot;&gt; &lt;constructor-arg ref=&quot;conversionService&quot;/&gt;&lt;/bean&gt; 大功告成，目前还没发现其他的问题]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Rest</tag>
        <tag>Spring Mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[发现tomcat的一个bug]]></title>
    <url>%2F2015%2F03%2F10%2F%E5%8F%91%E7%8E%B0tomcat%E7%9A%84%E4%B8%80%E4%B8%AAbug%2F</url>
    <content type="text"><![CDATA[在做项目的时候出现一个错误，看了半天没看出来是什么问题1234567891011121314151617181920212223242526272829root causejava.util.NoSuchElementException java.util.ArrayList$Itr.next(ArrayList.java:834) org.apache.jasper.compiler.Validator$ValidateVisitor.getJspAttribute(Validator.java:1385) org.apache.jasper.compiler.Validator$ValidateVisitor.visit(Validator.java:772) org.apache.jasper.compiler.Node$UninterpretedTag.accept(Node.java:1251) org.apache.jasper.compiler.Node$Nodes.visit(Node.java:2377) org.apache.jasper.compiler.Node$Visitor.visitBody(Node.java:2429) org.apache.jasper.compiler.Validator$ValidateVisitor.visit(Validator.java:779) org.apache.jasper.compiler.Node$UninterpretedTag.accept(Node.java:1251) org.apache.jasper.compiler.Node$Nodes.visit(Node.java:2377) org.apache.jasper.compiler.Node$Visitor.visitBody(Node.java:2429) org.apache.jasper.compiler.Validator$ValidateVisitor.visit(Validator.java:529) org.apache.jasper.compiler.Node$JspRoot.accept(Node.java:564) org.apache.jasper.compiler.Node$Nodes.visit(Node.java:2377) org.apache.jasper.compiler.Node$Visitor.visitBody(Node.java:2429) org.apache.jasper.compiler.Node$Visitor.visit(Node.java:2435) org.apache.jasper.compiler.Node$Root.accept(Node.java:474) org.apache.jasper.compiler.Node$Nodes.visit(Node.java:2377) org.apache.jasper.compiler.Validator.validateExDirectives(Validator.java:1841) org.apache.jasper.compiler.Compiler.generateJava(Compiler.java:217) org.apache.jasper.compiler.Compiler.compile(Compiler.java:373) org.apache.jasper.compiler.Compiler.compile(Compiler.java:353) org.apache.jasper.compiler.Compiler.compile(Compiler.java:340) org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:657) org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:357) org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:390) org.apache.jasper.servlet.JspServlet.service(JspServlet.java:334) javax.servlet.http.HttpServlet.service(HttpServlet.java:727) 找了半天才发现是tomcat的一个bug，换了一个tomcat版本就好了导致这个错误的原因如下1&lt;jsp:param name=&quot;test&quot; value=&quot;&quot; /&gt; 如果是value是空值，在某些tomcat版本下就会出现这个情况，如果你遇见了，不妨换个版本试试。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Mvc 3.1 之后如何配置messageConverters]]></title>
    <url>%2F2015%2F03%2F06%2FSpring-Mvc-3-1-%E4%B9%8B%E5%90%8E%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEmessageConverters%2F</url>
    <content type="text"><![CDATA[&lt;mvc:annotation-driven /&gt; 是一种简写形式，完全可以手动配置替代这种简写形式，简写形式可以让初学都快速应用默认配置方案。&lt;mvc:annotation-driven /&gt; 会自动注册DefaultAnnotationHandlerMapping与AnnotationMethodHandlerAdapter 两个bean,是spring MVC为@Controllers分发请求所必须的。 这句话我在很多帖子都看到过，我自己的项目本身使用的Spring MVC 3.2，实际上在3.1之后，&lt;mvc:annotation-driven /&gt;注册的类发生了变化 Spring Framework 3.1 introduces a new set of support classes for processing requests with annotated controllers: RequestMappingHandlerMappingRequestMappingHandlerAdapterExceptionHandlerExceptionResolverThese classes are a replacement for the existing: DefaultAnnotationHandlerMappingAnnotationMethodHandlerAdapterAnnotationMethodHandlerExceptionResolver 12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd"&gt; &lt;mvc:annotation-driven /&gt;&lt;/beans&gt; The above registers a RequestMappingHandlerMapping, a RequestMappingHandlerAdapter, and an ExceptionHandlerExceptionResolver (among others) in support of processing requests with annotated controller methods using annotations such as @RequestMapping , @ExceptionHandler, and others. It also enables the following: Spring 3 style type conversion through a ConversionService instance in addition to the JavaBeans PropertyEditors used for Data Binding. Support for formatting Number fields using the @NumberFormat annotation through the ConversionService. Support for formatting Date, Calendar, Long, and Joda Time fields using the @DateTimeFormat annotation. Support for validating @Controller inputs with @Valid, if a JSR-303 Provider is present on the classpath. HttpMessageConverter support for @RequestBody method parameters and @ResponseBody method return values from @RequestMapping or @ExceptionHandler methods.This is the complete list of HttpMessageConverters set up by mvc:annotation-driven: ByteArrayHttpMessageConverter converts byte arrays. StringHttpMessageConverter converts strings. ResourceHttpMessageConverter converts to/from org.springframework.core.io.Resource for all media types. SourceHttpMessageConverter converts to/from a javax.xml.transform.Source. FormHttpMessageConverter converts form data to/from a MultiValueMap&lt;String, String&gt;. Jaxb2RootElementHttpMessageConverter converts Java objects to/from XML — added if JAXB2 is present on the classpath. MappingJackson2HttpMessageConverter (or MappingJacksonHttpMessageConverter) converts to/from JSON — added if Jackson 2 (or Jackson) is present on the classpath. AtomFeedHttpMessageConverter converts Atom feeds — added if Rome is present on the classpath. RssChannelHttpMessageConverter converts RSS feeds — added if Rome is present on the classpath. 这是摘取的官方文档，可以看出，注册的类已经变成了RequestMappingHandlerMapping和 RequestMappingHandlerAdapter。我之前在不知道的时候，使用AnnotationMethodHandlerAdapter 进行配置，结果在有&lt;mvc:annotation-driven /&gt;存在的情况下，我自己配置的AnnotationMethodHandlerAdapter 怎么都不起作用，于是去掉了&lt;mvc:annotation-driven /&gt;标签，手动注册了AnnotationMethodHandlerAdapter ，和DefaultAnnotationHandlerMapping。结果引发了其他问题，比如文件无法上传的问题。 阅读文档发现Spring提供了基于&lt;mvc:annotation-driven /&gt;自定义messageConverters的方法，如下所示： 123456&lt;mvc:annotation-driven conversion-service="conversionService"&gt; &lt;mvc:message-converters&gt; &lt;bean class="org.example.MyHttpMessageConverter"/&gt; &lt;bean class="org.example.MyOtherHttpMessageConverter"/&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt; 下面展示我自己的配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:context="http://www.springframework.org/schema/context" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:jdbc="http://www.springframework.org/schema/jdbc" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:util="http://www.springframework.org/schema/util" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-3.2.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-3.2.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.2.xsd"&gt; &lt;!-- spring自动扫描注解的组件 --&gt; &lt;context:component-scan base-package="cn.xx.xx" use-default-filters="false"&gt; &lt;context:include-filter expression="org.springframework.stereotype.Controller" type="annotation" /&gt; &lt;/context:component-scan&gt; &lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;ref bean="stringHttpMessageConverter" /&gt; &lt;ref bean="fastJsonHttpMessageConverter" /&gt; &lt;ref bean="marshallingHttpMessageConverter" /&gt; &lt;/mvc:message-converters&gt; &lt;/mvc:annotation-driven&gt; &lt;bean id="stringHttpMessageConverter" class="org.springframework.http.converter.StringHttpMessageConverter"&gt; &lt;constructor-arg value="UTF-8" index="0"&gt;&lt;/constructor-arg&gt;&lt;!-- 避免出现乱码 --&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;text/plain;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="fastJsonHttpMessageConverter" class="com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter"&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;application/json;charset=UTF-8&lt;/value&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt;&lt;!-- 避免IE出现下载JSON文件的情况 --&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="features"&gt; &lt;util:list&gt; &lt;!-- &lt;value&gt;WriteMapNullValue&lt;/value&gt; --&gt; &lt;value&gt;QuoteFieldNames&lt;/value&gt; &lt;value&gt;WriteDateUseDateFormat&lt;/value&gt; &lt;/util:list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="marshallingHttpMessageConverter" class="org.springframework.http.converter.xml.MarshallingHttpMessageConverter"&gt; &lt;property name="marshaller" ref="castorMarshaller" /&gt; &lt;property name="unmarshaller" ref="castorMarshaller" /&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;text/xml;charset=UTF-8&lt;/value&gt; &lt;value&gt;application/xml;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 返回类型定义 --&gt; &lt;util:list id="messageConverters"&gt; &lt;ref bean="stringHttpMessageConverter" /&gt; &lt;ref bean="fastJsonHttpMessageConverter" /&gt; &lt;ref bean="marshallingHttpMessageConverter" /&gt; &lt;/util:list&gt; &lt;bean id="castorMarshaller" class="org.springframework.oxm.castor.CastorMarshaller" /&gt; &lt;!-- AOP自动注解功能 --&gt; &lt;aop:aspectj-autoproxy /&gt; &lt;!-- 不进行拦截的 --&gt; &lt;mvc:resources location="/" mapping="/**/*.html" order="0" /&gt; &lt;mvc:resources location="/images/" mapping="/images/**" /&gt; &lt;mvc:resources location="/img/" mapping="/img/**" /&gt; &lt;mvc:resources location="/download/" mapping="/download/**" /&gt; &lt;mvc:resources location="/js/" mapping="/js/**" /&gt; &lt;mvc:resources location="/css/" mapping="/css/**" /&gt; &lt;mvc:resources location="/plugin/" mapping="/plugin/**" /&gt; &lt;mvc:resources location="/WEB-INF/pages/" mapping="/pages/**" /&gt; &lt;bean id="messageSource" class="org.springframework.context.support.ResourceBundleMessageSource"&gt; &lt;property name="basename" value="messages"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/pages/" /&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt; &lt;!-- 支持上传文件 --&gt; &lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver" /&gt; &lt;!-- restTemplate --&gt; &lt;bean id="restTemplate" class="org.springframework.web.client.RestTemplate"&gt; &lt;property name="messageConverters" ref="messageConverters"&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring mvc</tag>
        <tag>messageConverters</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS Jenkins + Sonar + Nexus 环境搭建]]></title>
    <url>%2F2015%2F03%2F06%2FCentOS-Jenkins-Sonar-Nexus-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Nexus篇在Centos上安装 如果机器上是JDK1.6的话，nexus-2.5.1是最后一个支持JDK1.6的版本。 下载地址：http://www.sonatype.org/nexus/archived 我这里使用FTP工具上传到服务器上。执行命令的时候如果遇到没有权限的地方用chmod改变文件权限。 设置为系统自启动服务（使用root用户）12cd /etc/init.d/cp /usr/local/jdk/nexus-2.5.1-01/bin/jsw/linux-x86-64/nexus nexus 编辑/etc/init.d/nexus文件，添加以下变量定义：123NEXUS_HOME=/usr/local/jdk/nexus-2.5.1-01PLATFORM=linux-x86-64PLATFORM_DIR="$&#123;NEXUS_HOME&#125;/bin/jsw/$&#123;PLATFORM&#125;" 修改如下变量，设置启动用户为ycftp(这里用你自己的用户)1RUN_AS_USER=ycftp 执行命令添加nexus自启动服务12chkconfig –add nexuschkconfig –levels 345 nexus on 执行如下命令启动、停止nexus服务12service nexus startservice nexus stop 启动后可通过http://yourip:8081/nexus访问 用admin/admin123登陆登陆后点击左侧Repositories，将下图所示设置为true，就可以搜索了这个时候还搜索不到 需要再右击选项点击 Repair Index如下所示修复完成后便可以搜索了。 Jenkins篇添加Jenkins的源（repository）:12sudo wget -O/etc/yum.repos.d/jenkins.repo http://jenkins-ci.org/redhat/jenkins.reposudo rpm--import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key 不然你在启动jenkins服务的时候他会说你没有什么key啥的。 安装Jenkins：1sudo yum installjenkins 安装完成后，有如下相关目录： /usr/lib/jenkins/：jenkins安装目录，WAR包会放在这里。 注意修改端口号 /etc/sysconfig/jenkins：jenkins配置文件，“端口”，“JENKINS_HOME”等都可以在这里配置。内容如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798## Path: Development/Jenkins## Description: Configuration for theJenkins continuous build server## Type: string## Default: "/var/lib/jenkins"## ServiceRestart: jenkins## Directory where Jenkins store itsconfiguration and working# files (checkouts, build reports, artifacts,...).#JENKINS_HOME="/var/lib/jenkins" ## Type: string## Default: ""## ServiceRestart: jenkins## Java executable to run Jenkins# When left empty, we'll try to find thesuitable Java.#JENKINS_JAVA_CMD="" ## Type: string## Default: "jenkins"## ServiceRestart: jenkins## Unix user account that runs the Jenkinsdaemon# Be careful when you change this, as youneed to update# permissions of $JENKINS_HOME and/var/log/jenkins.#JENKINS_USER="jenkins" ## Type: string## Default: "-Djava.awt.headless=true"## ServiceRestart: jenkins## Options to pass to java when runningJenkins.#JENKINS_JAVA_OPTIONS="-Djava.awt.headless=true" ## Type: integer(0:65535)## Default: 8080## ServiceRestart: jenkins## Port Jenkins is listening on.# Set to -1 to disable#JENKINS_PORT="8085" ## Type: integer(0:65535)## Default: 8009## ServiceRestart: jenkins## Ajp13 Port Jenkins is listening on.# Set to -1 to disable#JENKINS_AJP_PORT="8019" ## Type: integer(1:9)## Default: 5## ServiceRestart: jenkins## Debug level for logs -- the higher thevalue, the more verbose.# 5 is INFO.#JENKINS_DEBUG_LEVEL="5" ## Type: yesno## Default: no## ServiceRestart: jenkins## Whether to enable access logging or not.#JENKINS_ENABLE_ACCESS_LOG="no" ## Type: integer## Default: 100## ServiceRestart: jenkins## Maximum number of HTTP worker threads.#JENKINS_HANDLER_MAX="100" ## Type: integer## Default: 20## ServiceRestart: jenkins## Maximum number of idle HTTP workerthreads.#JENKINS_HANDLER_IDLE="20" ## Type: string## Default: ""## ServiceRestart: jenkins## Pass arbitrary arguments to Jenkins.# Full option list: java -jar jenkins.war--help#JENKINS_ARGS="" 我这里修改端口号为8085，避免和本机tomcat冲突 启动Jenkins1sudo servicejenkins start 启动后用 http://yourip:8085/ 访问 Sonar篇首先下载http://www.sonarqube.org/downloads/ 我用的ftp工具上传到服务器。 编辑~/.bash_profile，添加环境变量 添加SONAR_RUNNER_HOME(就是sonar-runner-2.4的全路径名，比如/usr/local/sonar-runner-2.4)环境变量，并将SONAR_RUNNER_HOME/bin加入PATH变量中 记得使环境变量生效 source ~/.bash_profile 我这里使用mysql数据库，但不说mysql怎么安装了。 先配置Sonar要用的数据库环境： 创建数据库 在mysql中执行如下脚本创建数据库及mysql用户 1234CREATE DATABASE sonar CHARACTER SET utf8 COLLATE utf8_general_ci;CREATE USER 'sonar' IDENTIFIED BY 'sonar';GRANT ALL ON sonar.* TO 'sonar'@'%' IDENTIFIED BY 'sonar';GRANT ALL ON sonar.* TO 'sonar'@'localhost' IDENTIFIED BY 'sonar'; 编辑${SONAR_HOME}/conf/sonar.properties配置数据库: 12345678sonar.jdbc.username=sonarsonar.jdbc.password=sonarsonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8&amp;rewriteBatchedStatements=true# Optionalpropertiessonar.jdbc.driverClassName:com.mysql.jdbc.Driver 配置DB驱动包如果使用Oracle数据库，必须手动复制驱动类到${SONAR_HOME}/extensions/jdbc-driver/oracle/目录下。其它支持的数据库默认提供了驱动，http://docs.codehaus.org/display/SONAR/Analysis+Parameters 列举了一些常用的配置及默认值. 修改sonar配置文件编辑sonar所在的目录（比如/usr/local/sonar-4.4）中conf/sonar.properties文件，配置数据库设置，默认已经提供了各类数据库的支持，只要将注释去掉就可以。这里使用mysql，因此取消mysql模块的注释，并将sonar中原有的嵌入式的数据库的jdbc.url注释掉。 1234567891011121314151617#vi sonar.properties#需要注释下面这条语句#sonar.jdbc.url=jdbc:h2:tcp://localhost:9092/sonarsonar.jdbc.username=sonarsonar.jdbc.password=sonarsonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8&amp;rewriteBatchedStatements=true# Optional propertiessonar.jdbc.driverClassName=com.mysql.jdbc.Driver 修改sonar-runner的配置文件切换至sonar-runner的安装目录下，修改sonar-runner.properties根据实际使用数据库情况取消相应注释，这里需要和sonar.properties中保持一致。123456789101112131415161718192021#Configure here general information about the environment， such as SonarQube DB details for example#No information about specific project should appear here#----- Default SonarQube serversonar.host.url=http://localhost:9000#----- PostgreSQL#sonar.jdbc.url=jdbc:postgresql://localhost/sonar#----- MySQLsonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8sonar.jdbc.driverClassName=com.mysql.jdbc.Driver#----- Oracle#sonar.jdbc.url=jdbc:oracle:thin:@localhost/XE#----- Microsoft SQLServer#sonar.jdbc.url=jdbc:jtds:sqlserver://localhost/sonar;SelectMethod=Cursor#----- Global database settingssonar.jdbc.username=sonarsonar.jdbc.password=sonar#----- Default source code encodingsonar.sourceEncoding=UTF-8#----- Security (when 'sonar.forceAuthentication' is set to 'true')sonar.login=adminsonar.password=admin 运行如下命令启动sonar，其它操作系统sonar均提供了启动脚本1$&#123;SONAR_HOME&#125;/bin/linux-x86-64/sonar.sh start 如果遇到wrapper没有执行权限，用chmod命令赋予权限 如：1chmod 777 wrapper 在浏览器中访问: http://yourip:9000/ ，运行界面如下：配置为自启动服务 使用root账户或者开启sudo权限操作。 创建自启动脚本文件/etc/init.d/sonar 1vi/etc/init.d/sonar 添加如下内容123456789101112131415161718#!/bin/sh## rc file for SonarQube## chkconfig: 345 96 10# description: SonarQube system (www.sonarsource.org)#### BEGIN INIT INFO# Provides: sonar# Required-Start: $network# Required-Stop: $network# Default-Start: 3 4 5# Default-Stop: 0 1 2 6# Short-Description: SonarQube system (www.sonarsource.org)# Description: SonarQube system (www.sonarsource.org)### END INIT INFO/usr/bin/sonar $* 添加启动服务123ln -s $SONAR_HOME/bin/linux-x86-64/sonar.sh /usr/bin/sonarchmod 755 /etc/init.d/sonarchkconfig --add sonar 与Jenkins集成在jenkins的插件管理中选择安装sonar jenkins plugin，该插件可以使项目每次构建都调用sonar进行代码度量。具体配置方式不再叙述。]]></content>
      <categories>
        <category>CI</category>
      </categories>
      <tags>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos 6.5 安装gitlab安装需注意的问题]]></title>
    <url>%2F2015%2F02%2F06%2Fcentos-6-5-%E5%AE%89%E8%A3%85gitlab%E5%AE%89%E8%A3%85%E9%9C%80%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[安装gitlab首先按照官网进行操作，环境为centos 6.5https://about.gitlab.com/downloads/注意安装过程中执行命令1sudo gitlab-ctl reconfigure 出现错误，在/etc/gitlab/gitlab.rb中修改域名后再执行一次方可启动成功进页面提示502，用命令查看日志1sudo gitlab-ctl tail postgresql 提示内存不足，官方也是建议1g内存进行搭建，512m的话也可以安装但是要添加swap，不然启动后就会提示内存不足，我目前是512的，添加swap后访问成功，但是有点慢可能，还是升为1g最好。 上面的图是我目前的配置，注意这里修改了gitlab默认的仓库存储路径，教程官网有https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/README.md我这里贴出来给大家看看 Storing Git data in an alternative directory1234567891011121314# Prevent users from writing to the repositories while you move them.sudo gitlab-ctl stop# Only move 'repositories'; 'gitlab-satellites' will be recreated# automatically. Note there is _no_ slash behind 'repositories', but there _is_ a# slash behind 'git-data'.sudo rsync -av /var/opt/gitlab/git-data/repositories /mnt/nas/git-data/# Fix permissions if necessarysudo gitlab-ctl reconfigure# Double-check directory layout in /mnt/nas/git-data. Expected output:# gitlab-satellites repositoriessudo ls /mnt/nas/git-data/# Done! Start GitLab and verify that you can browse through the repositories in# the web interface.sudo gitlab-ctl start 可以看到这下面是安装完成后gitlab一些文件和数据的路径 注：看到网上很多资料都不对，可能是过时了，所以搞gitlab建议大家还是以官网为主，说不定你在看我这个文章的时候也过时了 Omnibus-gitlab uses four different directories. /opt/gitlab holds application code for GitLab and its dependencies. /var/opt/gitlab holds application data and configuration files that gitlab-ctl reconfigure writes to. /etc/gitlab holds configuration files for omnibus-gitlab. These are the only files that you should ever have to edit manually. /var/log/gitlab contains all log data generated by components of omnibus-gitlab. 有错误的时候尽量用1234567891011121314151617181920sudo gitlab-ctl tail``` 查看日志分析错误原因，我在配置邮件的时候也是试了很久才用上接下来我们来配置邮件，我这里使用SMTP，用腾讯企业邮箱还是修改`/etc/gitlab/gitlab.rb` 配置文件，再次**提醒**网上很多文章可以已经过时了附上官网地址https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/doc/settings/smtp.md```rubygitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = "smtp.exmail.qq.com" gitlab_rails['smtp_port'] = 25 gitlab_rails['smtp_user_name'] = "xx@xxx.com" gitlab_rails['smtp_password'] = "password" gitlab_rails['smtp_domain'] = "exmail.qq.com" gitlab_rails['smtp_authentication'] = "login" gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['gitlab_email_from'] = 'xx@xxx.com' ##修改gitlab配置的发信人 user["git_user_email"] = "xx@xxx.com" 保持发信人和登录邮箱一致,我配置这里的时候也出错了，原因是我在看腾讯官网时候写的是用的SSL，端口是465但是gitlab好像不是用的ssl，gitlab的教程里也没找到是否启用ssl的配置，我也没去深究了，于是把端口号改为25，邮件发送成功。大家在测试邮件的时候可以用忘记密码来发送邮件测试，然后用gitlab-ctl tail来查看，大概等1分钟就能看到发送邮件的日志，出错了再具体分析原因]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis-memcached框架配置]]></title>
    <url>%2F2015%2F02%2F06%2Fmybatis-memcached%E6%A1%86%E6%9E%B6%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[官方放出了mybatis和memcached的整合包，先附上官方文档地址http://mybatis.github.io/memcached-cache/文档很简洁，事实证明使用起来也很简单memcached的安装我这里就不再讲了，网上很容易找到在项目中引入12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.caches&lt;/groupId&gt; &lt;artifactId&gt;mybatis-memcached&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt; 然后在想使用的mapper中加入1234&lt;mapper namespace="org.acme.FooMapper"&gt; &lt;cache type="org.mybatis.caches.memcached.MemcachedCache" /&gt; ...&lt;/mapper&gt; 就可以用了 再建一个memcached.properties，对他进行配置我简单测试了一下发现它可以配置多个服务器，用逗号分隔，经测试如果某一台挂掉，他会选择正常的那台如果2台都挂掉，就会报错,估计我们还是希望在memcached服务器挂掉后从数据库读取数据，不知道大家有什么好的实现方式或者思路吗123org.mybatis.caches.memcached.servers=172.29.33.201:11211,localhost:11211org.mybatis.caches.memcached.expiration=30 org.mybatis.caches.memcached.asyncget=false]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
        <tag>memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos6.5使用yum安装mysql]]></title>
    <url>%2F2015%2F01%2F31%2Fcentos6-5%E4%BD%BF%E7%94%A8yum%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[要使用yum 安装mysql，要使用mysql的yum仓库，先从官网下载适合你系统的仓库http://dev.mysql.com/downloads/repo/yum/centos 6.5 对应的是mysql-community-release-el6-5.noarch.rpm然后安装一下这个仓库列表1sudo yum localinstall mysql-community-release-el6-5.noarch.rpm 执行这个命令后就能看到可安装的mysql1yum repolist enabled | grep "mysql.*-community.*" 如果我们是要安装最新的版本，那么可以直接执行1sudo yum install mysql-community-server 如果我们要选择版本，可以先执行下面这个命令查看一下有哪些版本1yum repolist all | grep mysql 如果要选择版本的话，有两种方式，一种是使用命令来12shell&gt; sudo yum-config-manager --disable mysql56-communityshell&gt; sudo yum-config-manager --enable mysql57-community-dmr 这个命令就是在仓库中启用5.7版本的，禁用5.6版本子仓库或者编辑/etc/yum.repos.d/mysql-community.repo文件1234567# Enable to use MySQL 5.6[mysql56-community]name=MySQL 5.6 Community Serverbaseurl=//repo.mysql.com/yum/mysql-5.6-community/el/5/$basearch/enabled=1gpgcheck=1gpgkey=file:/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql enabled=0表示禁用比如要安装5.7版本的mysql，要确定5.6的enabled=0，5.7的enabled=1，一次保证只启用一个子仓库12345678# Note: MySQL 5.7 is currently in development. For use at your own risk.# Please read with sub pages: https://dev.mysql.com/doc/relnotes/mysql/5.7/en/[mysql57-community-dmr]name=MySQL 5.7 Community Server Development Milestone Releasebaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/6/$basearch/enabled=1gpgcheck=1gpgkey=file:/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 然后我们就可以愉快的安装mysql了1sudo yum install mysql-community-server 安装完成后我们启动mysql1sudo service mysqld start 查看mysql状态1sudo service mysqld status]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring mvc @ResponseBody 返回枚举类型]]></title>
    <url>%2F2015%2F01%2F31%2FSpring-mvc-ResponseBody-%E8%BF%94%E5%9B%9E%E6%9E%9A%E4%B8%BE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[我们在用@ResponseBody返回实体对象可以用spring mvc自动帮我们转化成json串但是当实体中包含了枚举类型的属性的时候怎么办，我这里使用的是fastjson，他默认是转换成了字符串。根据我上一篇博文的解决方案，我们这里自定义一个FastJsonHttpMessageConverter1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class FastJsonHttpMessageConverter extends AbstractHttpMessageConverter&lt;Object&gt; &#123; public final static Charset UTF8 = Charset.forName("UTF-8"); private Charset charset = UTF8; private SerializerFeature[] features = new SerializerFeature[0]; public FastJsonHttpMessageConverter()&#123; super(new MediaType("application", "json", UTF8), new MediaType("application", "*+json", UTF8)); &#125; @Override protected boolean supports(Class&lt;?&gt; clazz) &#123; return true; &#125; public Charset getCharset() &#123; return this.charset; &#125; public void setCharset(Charset charset) &#123; this.charset = charset; &#125; public SerializerFeature[] getFeatures() &#123; return features; &#125; public void setFeatures(SerializerFeature... features) &#123; this.features = features; &#125; @Override protected Object readInternal(Class&lt;? extends Object&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException &#123; ByteArrayOutputStream baos = new ByteArrayOutputStream(); InputStream in = inputMessage.getBody(); byte[] buf = new byte[1024]; for (;;) &#123; int len = in.read(buf); if (len == -1) &#123; break; &#125; if (len &gt; 0) &#123; baos.write(buf, 0, len); &#125; &#125; byte[] bytes = baos.toByteArray(); return JSON.parseObject(bytes, 0, bytes.length, charset.newDecoder(), clazz); &#125; @Override protected void writeInternal(Object obj, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException &#123; OutputStream out = outputMessage.getBody(); String text = JSONUtil.toJSONString(obj, features); byte[] bytes = text.getBytes(charset); out.write(bytes); &#125; &#125; 其实我就改了一句代码，如下所示，这样我们就可以返回想要的索引数字了。1String text = JSONUtil.toJSONString(obj, features);]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring mvc</tag>
        <tag>fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastjson序列化枚举属性]]></title>
    <url>%2F2015%2F01%2F30%2Ffastjson%E5%BA%8F%E5%88%97%E5%8C%96%E6%9E%9A%E4%B8%BE%E5%B1%9E%E6%80%A7%2F</url>
    <content type="text"><![CDATA[我的实体类里面有一个属性是枚举类型的，但是我在转换的时候我不希望取它的name，而是它的索引值0,1,2,3,搜索一番后发现这个回答 fastjson enum 枚举 反序列化为了方便大家查看，我把内容贴过来看fastjson源码，SerializeWriter1234567891011121314151617181920212223242526272829public void writeEnum( Enum &lt; ?&gt;value, char c )&#123; if ( value == null ) &#123; writeNull(); write( ',' ); return; &#125; if ( isEnabled( SerializerFeature.WriteEnumUsingToString ) ) &#123; if ( isEnabled( SerializerFeature.UseSingleQuotes ) ) &#123; write( '\'' ); write( value.name() ); write( '\'' ); write( c ); &#125; else &#123; write( '\"' ); write( value.name() ); write( '\"' ); write( c ); &#125; return; &#125; writeIntAndChar( value.ordinal(), c ); &#125; 可以看出SerializeWriter在初始化的时候，features不要设置SerializerFeature.WriteEnumUsingToString因为JSON.DEFAULT_PARSER_FEATURE是enable了SerializerFeature.WriteEnumUsingToString，也就是说是读枚举的value值而不是int值12345678910public static int DEFAULT_GENERATE_FEATURE; static &#123; int features = 0; features |= com.alibaba.fastjson.serializer.SerializerFeature.QuoteFieldNames.getMask(); features |= com.alibaba.fastjson.serializer.SerializerFeature.SkipTransientField.getMask(); features |= com.alibaba.fastjson.serializer.SerializerFeature.WriteEnumUsingToString.getMask(); features |= com.alibaba.fastjson.serializer.SerializerFeature.SortField.getMask(); // features |= com.alibaba.fastjson.serializer.SerializerFeature.WriteSlashAsSpecial.getMask(); DEFAULT_GENERATE_FEATURE = features; &#125; 所以，解决你这个问题的方法就是之前调用1JSONSerializer.config(SerializerFeature.WriteEnumUsingToString,false); 但是JSONSerializer.config不是一个静态方法，不能直接调用而且如果直接调用JSON.toJSON把实体类转为json，这里还有另外一句代码12345678910111213141516171819202122232425if (clazz.isEnum()) &#123; return ((Enum&lt;?&gt;) javaObject).name(); &#125;``` 如果是枚举类型，不管你怎么改配置都不会给你转成索引值的情况，所以我们这里就先想把实体转成`jsonString`，再把`jsonString`转成`JSONObject`。再继续看`fastjson`的源码在`JOSN.toJSONString`中```javapublic static final String toJSONString(Object object, SerializerFeature... features) &#123; SerializeWriter out = new SerializeWriter(); try &#123; JSONSerializer serializer = new JSONSerializer(out); for (com.alibaba.fastjson.serializer.SerializerFeature feature : features) &#123; serializer.config(feature, true); &#125; serializer.write(object); return out.toString(); &#125; finally &#123; out.close(); &#125; &#125; 他这里也是用的serializer.config来配置的，干脆我们自己写个工具方法吧，同时把WriteEnumUsingToString禁用掉1234567891011121314151617181920212223242526272829private static final SerializerFeature[] CONFIG = new SerializerFeature[]&#123; SerializerFeature.WriteNullBooleanAsFalse,//boolean为null时输出false SerializerFeature.WriteMapNullValue, //输出空置的字段 SerializerFeature.WriteNonStringKeyAsString,//如果key不为String 则转换为String 比如Map的key为Integer SerializerFeature.WriteNullListAsEmpty,//list为null时输出[] SerializerFeature.WriteNullNumberAsZero,//number为null时输出0 SerializerFeature.WriteNullStringAsEmpty//String为null时输出"" &#125;; public static JSONObject toJSON(Object javaObject) &#123; SerializeWriter out = new SerializeWriter(); String jsonStr; try &#123; JSONSerializer serializer = new JSONSerializer(out); for (com.alibaba.fastjson.serializer.SerializerFeature feature : CONFIG) &#123; serializer.config(feature, true); &#125; serializer.config(SerializerFeature.WriteEnumUsingToString, false); serializer.write(javaObject); jsonStr = out.toString(); &#125; finally &#123; out.close(); &#125; JSONObject jsonObject = JSON.parseObject(jsonStr); return jsonObject; &#125; 这样调用我们的工具类方法转换出来的结果，就是我们想要的数字了。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>fastjson</tag>
        <tag>枚举</tag>
      </tags>
  </entry>
</search>
